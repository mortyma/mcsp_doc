\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{cite}
%page boarders
\usepackage[vmargin=3cm, hmargin=3cm]{geometry}
\usepackage{float}
\floatstyle{plaintop}
\usepackage{paralist} %compactitem
\usepackage{fixltx2e} %some latex fixes
\usepackage{microtype} %Subliminal refinements towards typographical perfection
\setlength{\emergencystretch}{2em}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs} %Support use of the Raph Smithâ€™s Formal Script font in mathematics
\usepackage[sc]{mathpazo} %mathematical fonts
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage[margin=10pt, font=small, labelfont=bf]{caption}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{color}
\floatstyle{plaintop}
\usepackage{float}
\restylefloat{table}
\usepackage {tikz}
\usetikzlibrary{calc,shapes.multipart,chains,arrows, positioning,shapes}
\usepackage{pgfplots}
\usepackage{fancyvrb}
\usepackage{subfigure}

% a nice not implies
\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}
  
% todos  
\newcommand\todo[1]{\textcolor{red}{TODO: #1}}

% number equations s.t. they can be referenced easily
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% within an align* environment, number and label the equation. #1 is the label text to be appended to eqn:
\newcommand\neqn[1]{\numberthis\label{eq:#1}}

% theorem, lemma, ...
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}

% math stuff
% vectors
\newcommand{\vect}[1]{\vec{#1}}

% dominance relation
\newcommand{\dom}{\ensuremath{\prec}}
% less relation
\newcommand{\less}{\ensuremath{\prec}}
\newcommand{\lesseq}{\ensuremath{\preceq}}
\newcommand{\moreeq}{\ensuremath{\succeq}}

% total order relation
\newcommand{\tor}{\ensuremath{\leq_R}}
\newcommand{\osum}{\ensuremath{\leq_{\text{sum}}}}
\newcommand{\olex}{\ensuremath{\leq_{\text{lex}}}}

% sets
\newcommand{\sC}{\ensuremath{S_{\text{c}}}}
\newcommand{\sA}{\ensuremath{S_{\text{a}}}}
\newcommand{\sR}{\ensuremath{S_{\text{r}}}}

% concatenation of paths
\newcommand{\cP}[2]{\ensuremath{#1;#2}}

% style of listings
\definecolor{Gray}{gray}{0.5}
\definecolor{OliveGreen}{cmyk}{0.64,0,0.95,0.40}

   \lstset{
      language=C++,
      basicstyle=\ttfamily\footnotesize,
      keywordstyle=\color{blue},
      commentstyle=\color{OliveGreen},
      breaklines=true,
      breakatwhitespace=false,
      showspaces=false,
      showtabs=false,
      numbers=left,
      frame=single,
      captionpos=t,
  }

\lstnewenvironment{code}[1][]%
{
   \noindent
   \minipage{\linewidth} 
   \vspace{0.5\baselineskip}
   \lstset{
      #1
  }
  }
   %\lstset{basicstyle=\ttfamily\footnotesize,frame=single,#1}}
{\endminipage}

%\DeclareCaptionFormat{mylst}{\hrule#1#2#3}
%\captionsetup[code]{format=mylst,labelfont=bf,singlelinecheck=off,labelsep=space}

% 
\title{A multi-criteria priority queue for the Pheet task scheduling framework\\
\normalsize Technical report for \\``Project in Software Engineering \& Internet Computing''
}
\author{Martin Kalany, 0825673\\
Vienna University of Technology}

\begin{document}
\maketitle

%scale factor for graphs
\setkeys{Gin}{width=0.5\textwidth}

%R functions for plot generation
<<echo=FALSE, include=FALSE>>=
\SweaveInput{functions.Rnw}
@

\VerbatimFootnotes

%\pagebreak	

\begin{abstract}
\cite{Wimmer14} \todo{abstract}
\end{abstract}

\pagebreak
\tableofcontents
\pagebreak

\section{Introduction} \label{sec:intro}
% explain the idea of this work in a couple of sentences. This is for the informed reader.

% Introduce pheet
Pheet\footnote{\url{www.pheet.org}} is an open-source task scheduling framework for shared memory systems   
and is based on the task-parallel programming model, which allows a programmer to explicitly expose the parallelism of an application.
% The build-up for task priorities
Runtime systems based on the task-parallel model typically impose a non-adaptive, application independent execution order on the tasks where the scheduler is unaware of the preferred execution order of the tasks (The tasks are processed in, e.g., LIFO-order).
% The problem with Dijkstra's algorithm
While it has proven to be an efficient strategy for parallelization \cite{AroraBP01}, this approach has not been particularly useful for the parallelization of algorithms relying on priority queues (such as Dijkstra's famous algorithm for the single-source shortest path problem (SSSP) \cite{Dijkstra59}), since two work pools have to be maintained: 
One is used by the scheduling framework to maintain tasks that are yet to be executed; the other is required by the algorithm itself to determine the execution order of the tasks.

% Priority task scheduling
\emph{Priority task scheduling}, introduced by Wimmer et al.~\cite{WVTCT13}, addresses this problem by making the preferred execution order of tasks known to the scheduler. 
To achieve this, the task scheduler utilizes a concurrent priority queue which fulfills the ordering requirements imposed by the algorithm by assigning comparison-based priority values to the tasks.
Wimmer \cite{Wimmer14} further states that a scheduler based on the task priority scheduling model allows for an efficient parallel implementation of any algorithm relying on priority queues is possible.
They back up their claim by providing a parallel implementation of Dijkstra's algorithm on top of Pheet, which utilizes a scheduler based on the priority task scheduling model.

% Pareto priorities
In their dissertation \cite{Wimmer14}, they suggest to generalize the idea of priority task scheduling to multi-dimensional (or \emph{Pareto-}) priorities (as defined in Section \ref{sec:pareto_optima}), which requires an efficient Pareto-priority queue implementation to be added to Pheet. 
A Pareto-priority queue ensures that the next task to be executed is a Pareto optimum, i.e., a task for which the partial solution is not dominated by the partial solution of any other task.
Such a Pareto-priority queue would allow for an efficient parallel implementation of e.g., algorithms solving the multi-criteria shortest path problem (MSP)\footnote{Sometimes also called multi-objective shortest path problem, multi-objective optimization or multi-objective search.} as discussed in \cite{Martins84}.
They outline a different advantage of using multi-dimensional priorities for task scheduling, which is independent of the algorithm that utilizes the scheduler:
Multi-dimensional priorities establish only a partial ordering on the tasks, which gives the scheduler more flexibility in terms of which task to execute next. 

% Our work
In this work, we investigate a potential implementation of such a multi-dimensional priority queue for the Pheet task scheduling framework, which is then used in turn for a parallel algorithm solving the MSP. 
% MSP is not our focus
\footnote{We note here that the goal of our work is not to provide a better algorithm for the MSP; we investigate the potential of a multi-dimensional priority queue and use the MSP only as an example application.}
% first step: use linear combination
As a first step and to provide a reference point, we will solve the MSP via the SSSP problem by reducing the multi-dimensional priorities to a scalar value via the $L^1$ norm.
By doing so, the problem can be solved with Dijkstra's algorithm using one of Pheet's priority queue implementations. 
% total ordering not always possible
However, due to the general nature of a task's priority (defined in Section \ref{sec:pareto_optima}), to determine the highest-priority task the MCPQ cannot rely on a total ordering relation like the $L^1$ norm and has to make use of different techniques.

\todo{something about results?}

\subsubsection*{Outline} \label{sec:intro:outline}
We start with discussing related work in Section \ref{sec:related}, followed by a formal definition of Pareto-optimality (Section \ref{sec:pareto_optima}),and a high-level introduction of Pheet-related concepts that are relevant for this work (Section \ref{sec:pheet}).
A detailed presentation and analysis of our multi-criteria priority queue is given in Section \ref{sec:mcpq}).
We give a formal definition of the multi-criteria shortest path problem, which is used as an example application, in Section \ref{sec:shortest_path}, where we also discuss the hardness of the problem.
A detailed description of a Pheet-based algorithm solving the MSP will be given in Section \ref{sec:msp_algo}. 
This implementation is then used to compare the performance of our multi-criteria priority queue to other priority queue implementations already available in Pheet (Section \ref{sec:evaluation}).

\todo{Future work, conclusion,...}

\section{Related work} \label{sec:related}
%--------------------------------------------------------------------------------------------
%Should related work be covered near the beginning of the paper or near the end?
%   - Beginning, if it can be short yet detailed enough, or if it's critical to take a strong defensive stance about previous work right away. In this case Related Work can be either a subsection at the end of the Introduction, or its own Section 2.
%   	- End, if it can be summarized quickly early on (in the Introduction or Preliminaries), or if sufficient comparisons require the technical content of the paper. In this case Related Work should appear just before the Conclusions, possibly in a more general section "Discussion and Related Work".
%--------------------------------------------------------------------------------------------

% Ehrgott, Gandibleux: 
For an overview of related work for the MCSP without parallelization, we refer to the annotated bibliography of multiobjective combinatorial optimization \cite{EhrgottG02}, specifically Section ``6.1 Shortest path problems''.

% Bi-criteria

% Sonnier: approximate solutions for d \in {3,4}
Sonnier \cite{Sonnier06} were one of the first to publish parallel algorithms for solving MSP problems with 3 or 4 objectives. 
Their algorithms are based on a weighted sum approach and thus provide only an approximate solution, i.e., the solution set may not contain all Pareto-optimal paths.

% Sanders, Erb: first parallel MSP algo, but mostly for d=2
To the best of our knowledge, Sanders and Mandow \cite{SM13} published the first proposal for a parallel algorithm that provides an exact solution of the MSP.
Their approach extends Dijkstra's classical algorithm \cite{Dijkstra59} and relies on a so called \emph{Pareto queue}, a multi-dimensional generalization of a priority queue. 
While they give a high level description of the algorithm for arbitrary $d \geq 2$ and a detailed description of the bi-criteria case (which was further engineered by Erb \cite{Erb13}), they also state that efficient priority queues for $d\geq3$ are not yet known.

\section{Pareto optima} \label{sec:pareto_optima}
% Informally describe the problem
Given a set of vectors, the \emph{maximal vector problem} is to find the subset of vectors s.t.\ each vector of the subset is not dominated by any vector of the set. 
One vector dominates another if each of its components is equal to or smaller than (w.r.t.\ some partial ordering of the vectors) the corresponding component of the other vector, and strictly smaller in at least one component.
Such a maximal element is called \emph{Pareto-optimal} and the set of maxima is called the \emph{Pareto-set}. 

% Example
Figure \ref{fig:pareto_optima} illustrates the concept of Pareto-optimality. 
A formal definition is provided in the following.

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[xlabel=$w_1$, ylabel=$w_2$, enlargelimits=0.2, nodes near coords]
\addplot[mark=*, point meta=explicit symbolic, color=blue]
  table {
x	y
1	7.8
1.5	7.2
1.6	7
2.2	6.5
2.2	6
2.9	5.5
3.8	4.5
4	4.4
4.7	4.3
5	4
};

\addplot[mark=*, only marks, point meta=explicit symbolic, color=black]
  table {
x	y
1.3	7.8
2.4	7
2.8	6
3.2	7.1
4	5
4	7.7
4.3	6
4.6	7.4
5	5


};

\addplot[mark=*, point meta=explicit symbolic, color=blue] 
coordinates {
	(2.9, 5.5) [A]
};
\draw[blue,dashed] (axis cs:2.9,5.5) -- (axis cs:0,5.5);
\draw[blue,dashed] (axis cs:2.9,5.5) -- (axis cs:2.9,0);

\addplot[mark=*, point meta=explicit symbolic, color=blue] 
coordinates {
	(3.8, 4.5) [B]
};
\draw[blue,dashed] (axis cs:3.8, 4.5) -- (axis cs:0,4.5);
\draw[blue,dashed] (axis cs:3.8, 4.5) -- (axis cs:3.8,0);

\addplot[mark=*, point meta=explicit symbolic, color=black] 
coordinates {
	(3.2, 6.2) [C]
};
\draw[black,dashed] (axis cs:3.2, 6.2) -- (axis cs:0,6.2);
\draw[black,dashed] (axis cs:3.2, 6.2) -- (axis cs:3.2,0);

\addplot[mark=*, point meta=explicit symbolic, color=black] 
coordinates {
	(3.5, 5.2) [D]
};
\draw[black,dashed] (axis cs:3.4, 5.2) -- (axis cs:0,5.2);
\draw[black,dashed] (axis cs:3.4, 5.2) -- (axis cs:3.4,0);

\end{axis}
\end{tikzpicture}
\end{center}
\caption{Example of a Pareto-set in two-dimensional space: The set of Pareto-optimal vectors are in blue, others in black. Note that point C is dominated by A in both dimensions ($w_1(A) < w_1(C)$ and $w_2(A) < w_2(C)$). D is dominated by A in dimension 1 and by B in dimension 2; either condition on its own suffices for D not being Pareto-optimal.}
\label{fig:pareto_optima}
\end{figure}

\subsubsection*{Formal definition}
% Formal problem definition
Let $U_1, U_2,\dots , U_d$ be totally ordered sets, each with the binary relations $\less_i$  and $=_i$, (e.g., $\mathbb{N}$ and the ``smaller than'' and ``equals'' relations) and let $V$ be a set of $d$-dimensional vectors in $U_1 \times U_2 \times \dots \times U_d$.
For any $\vect{v} \in V$, we donate the $i$-th component of $\vect{v}$ by $w_i(\vect{v})$.

% Domination relation
Let $\vect{x}, \vect{y} \in V$. 
We say that $\vect{x}$ \emph{dominates} $\vect{y}$ ($\vect{x} \dom \vect{y}$) -- or equivalently, $\vect{y}$ \emph{is dominated by} $\vect{x}$ -- if and only if
\begin{align}
&\forall i \in \{1,\dots, d\}: w_i(x) \lesseq_i w_i(y) \quad \text{and}\\
&\exists j \in \{1,\dots, d\}: w_j(x) \less_i w_j(y) \quad .
\end{align}
Note that $\vect{x} \dom \vect{y} $ implies $\vect{x} \neq \vect{y}$ and that the binary relation $\moreeq_i$ can easily be defined by the other two relations.
For $d\geq 2$, the dominance relation ($\dom$) defines a partial order on $V$, but not a total order.
Thus, there exist vectors $\vect{x}, \vect{y} \in V$, $\vect{x} \neq \vect{y}$ for which neither $\vect{x} \dom \vect{y}$ nor $\vect{y} \dom \vect{x}$ holds.

% Pareto optima
A vector $\vect{x} \in V$ is a \emph{Pareto-optimum} if and only if
\begin{align}
\nexists \vect{v} \in V: \vect{v} \dom \vect{x} \quad ,
\end{align}
i.e., there is no vector $\vect{v}$ that dominates $\vect{x}$.
%Pareto set
A \emph{Pareto-set} $P \subseteq V$ is the set of all Pareto-optima of $V$.

\section{Pheet} \label{sec:pheet}
% what we will do in this section
In this section, we provide a brief introduction to Pheet-specific concepts that are relevant for this technical report. 
% reference Martin's dissertation
A detailed introduction to Pheet, the concepts it is built on and the research based on it can be found in the dissertation of Martin Wimmer \cite{Wimmer14}. 

\subsubsection*{Philosophy} \label{sec:pheet:philosophy}
% How is pheet implemented, what are its goals?
Pheet is a task-parallel programming library with the goal of providing a simple-to-use framework for the quick parallelization of algorithms.
Its flexible plug-in architecture based on C++ template meta-programming allows for any component in the task scheduling system to be replaced by an alternative implementation.
Additionally, Pheet provides a set of micro-benchmarks (that aim to evaluate specific aspects of the scheduling framework) and fine grained performance counters (that provide detailed insight into the scheduler and supporting data structures), which makes the framework a suitable platform for the implementation and testing of new components such as schedulers or priority queues. 

%\subsection{Concepts} \label{sec:pheet:concepts}

\subsubsection*{Places} \label{sec:pheet:places}
A \emph{place} denotes a single worker thread in the Pheet scheduling system that is pinned to specific processor, that is, a place will not migrate to another processing unit during execution.
Each processor utilized by Pheet is assigned exactly one place, implying that a processor is uniquely identified by it, which may be very useful for the implementation of parallel algorithms.
Furthermore, a place allows to implement processor-local data structures, e.g., parts of a distributed priority queue or application specific data structures and data accessible by the tasks that are executed on the associated processor.

\subsubsection*{Task parallelism} \label{sec:pheet:task_parallel}
% work pool
Pheet is based on the \emph{task-parallel model}, where a task is a small portion of work that is to be executed sequentially.
As soon as a processor is ready to execute work, it retrieves a task from the \emph{work pool}, which contains all the tasks that have yet to be executed.
A task may itself create additional tasks that are then stored in the work pool.

A famous algorithm based on the work pool pattern is Dijkstra's algorithm \cite{Dijkstra59} for the shortest path problem:
The algorithm requires a priority queue (that may be implemented as e.g., a simple binary search tree or a Fibonacci heap). This priority queue is a specialized work pool.

Note that in a task parallel program, the work pool is not necessarily a centralized data structure; each place may maintain its own. 

\subsubsection*{Task priorities and strategy scheduling}  \label{sec:pheet:task_priorites}
% What does the scheduler do?
The work pool exposes available work to the \emph{scheduler}, which is responsible for devising a schedule, i.e., a mapping of tasks to the processors.
% Global scheduling strategy
In standard task-parallel programming models, the scheduler typically employs a global policy that treats all tasks equally and thus independently of any task-specific properties.

% Task priorities 
Wimmer introduced the concept of \emph{task priorities} to make the scheduler aware of the preferred (relative) execution order of tasks. 
% Discrete priorities
One approach to handling priorities is to assign a discrete priority value (typically taken form a small set) to each task. 
% Comparison based prioritization
Another is a comparison based approach, where the programmer has to provide a comparator to the scheduler, that, given two tasks, decides which to execute first.

% Scheduling strategy
In Pheet, this comparator has to be implemented within the \emph{scheduling strategy} of a task.
A scheduling strategy is associated with a single task and may specify behavior that is depending on task specific criteria.
In contrast to a global scheduling policy, a scheduling strategy is associated with a single task and thus allows the programmer to influence the scheduler's behavior for a specific task.
% Dead tasks
An important application of this concept is the mechanism of \emph{dead tasks}, which allows to mark a spawned but not yet executed (or currently executing) task as obsolete. 
This mechanism is useful for speculative execution: A task may be spawned when it is likely that the work will of the task will have to be performed sometime in the future; when further calculations determine that the task need not be executed after all, the task can be marked dead via its associated strategy, thus instructing the scheduler not to execute the task.
Pheet handles dead tasks lazily, i.e., a dead task will not be dropped immediately but at a time it is convenient for the scheduler to do so, which allows for a more efficient implementation of the scheduler's data structures.


% Main part of the work
\section{Multi-criteria priority queue} \label{sec:mcpq}
% What is the MCPQ
The goal of the MCPQ is to provide a concurrent priority queue that can then be used by Pheet's strategy scheduler  to determine the execution order of tasks with multi-dimensional priorities of the form described in Section \ref{sec:pareto_optima}.


% Priorities and partial solutions
The priority of a task reflects the potential quality of the (partial) solutions it will produce upon execution.
By using multi-dimensional priority vectors, the quality of a solution may be measured by several, possibly conflicting metrics\footnote{We refer to our algorithm for the multi-criteria shortest path problem (definition see Section \ref{sec:shortest_path:defs:msp}; algorithm see Section \ref{sec:msp_algo}) for a concrete application of this concept.}. 
% Priority of task == desirability of execution
Obviously, it is desirable to execute the tasks that will produce (partial) solutions with the highest potential quality (i.e., tasks with highest priority) before other tasks.
% Pareto
Since each task is associated with a multi-dimensional priority vector, the tasks with highest priorities are the ones with a Pareto-optimal priority vector (w.r.t.\ the set of priority vectors of tasks \emph{currently} managed by the queue).

% abstract definition
The MCPQ is an \emph{ordered container} (as defined by Wimmer \cite{Wimmer14}), i.e., a bag-like data structure which orders tasks by their priorities.
It offers the following operations:
\begin{itemize}
\item \verb|push| adds a task to the container.
\item \verb|pop| returns a task that was previously added and removes it from the container. The order in which tasks are returned is determined by their priority.
\end{itemize}



% Outline
\begin{comment}
We start with a high-level discussion of the MCPQ, in particular of the requirements and major design choices (Section \ref{sec:mcpq:definition}).
Section \ref{sec:mcpq:description} provides a detailed description and analysis of the data structure, followed by a presentation of its implementation and integration into Pheet in Section \ref{sec:mcpq:pheet_integration}.

\subsection{Definition and requirements} \label{sec:mcpq:definition}
% Abstract datastructure description
\end{comment}

\subsubsection*{Ordering requirements} \label{sec:mcpq:ordering}
% ordering semantics  
The semantic definition of the \verb|pop| operation requires that \emph{all} threads agree on the set of tasks with Pareto-optimal priority, implying that all the \verb|push| and \verb|pop| operations executed by different threads need to appear to take affect in the same order for each thread, i.e., they need to be linearizable w.r.t.\ all the threads.
While these \emph{global ordering} semantics match the intuitive expectation, they also pose a (potentially big) performance bottleneck. 
\emph{Local ordering} semantics typically occur when each thread maintains its own, thread-local priority queue.
A task returned by \verb|pop| is required to have a Pareto-optimal priority only w.r.t.\ the tasks managed by the thread executing the operation; the priority queue of another thread might contain a task with strictly higher priority.
% pro and cons			
With local ordering semantics, a thread might execute unnecessary additional work (since the solutions created by the task will be of much lower quality than the ones created by a task with a dominating priority vector and thus will eventually be abandoned). 
However, this negative effect is countered by increased parallelism and less overhead due to synchronization.

\subsubsection*{Spying} \label{sec:mcpq:spying}
% Spying
When a thread does not have any more ready-tasks in its local priority queue, it will try to \emph{spy} some from the priority queue of another thread. 
A thread spies ready-tasks from another priority queue by copying references to the tasks to its own, thread-local priority queue.
Note that the tasks are then shared by multiple threads.
Once a thread pops a task it informs all the other threads that the task is already being processed and thus should not be executed by other threads anymore; this is done by marking a task as \emph{taken}.
The operations of the spying thread are linearized with the ones of all the other threads on the same priority queue.

\subsection{Description and analysis} \label{sec:mcpq:description}
% Purely local ordering semantics
The MCPQ observes only local ordering semantics and thus each thread maintains its own priority queue. 
% A general idea
% Concurrent and local part
The data structure is split into a part that may be accessed by several threads concurrently and a second part -- built on top of the first -- that may be accessed by the owning thread only.
% Virtual array
For the concurrent part, we assume a (potentially) infinite, thread-safe array-like data structure with linear (w.r.t.\ the capacity $n$ of data structure) access time for a whole sequence of $m\leq n$ items, called the \emph{Virtual Array}.

% That is, sequentially accessing $m$ items that are stored in a consecutive sequence is assumed to take time $O(n)$.
The virtual array is used to hold all the tasks currently managed by the thread-local priority queue and allows other threads to scan it for tasks awaiting execution.
An implementation is discussed in Section \ref{sec:virtual_array}, where we also prove the linear access time property for a sequence of elements.

% Thread-local part
The thread-local part is based on the idea of \emph{log-structured merge trees} (\cite{Wimmer14}, Section 5.7) and \emph{partitioning} to provide the operations \verb|push| and \verb|pop| as defined above.
To do so, the Virtual Array of size $n$ is subdivided into an ordered sequence of a logarithmic number of blocks.
Each block maintains a constant-size set of tasks whose priority is not dominated by any other task in that block
and provides the following two operations:
\begin{itemize}
\item \verb|peek| finds a highest priority task w.r.t.\ the tasks managed by the block\footnote{Note that \verb|peek| on a block is similar to \verb|pop| on the whole priority queue, except that it does not remove the returned task from the data structure.}.
\item \verb|take| removes a given task from the priority queue.
\end{itemize}

% Overview of pop
The \verb|pop| operation performs a \verb|peek| on each block to obtain a set of $O(\log n)$ tasks (one task from each block) that are not dominated w.r.t.\ to the tasks in their respective blocks.
This set is then scanned linearly to find a task that is not dominated by any other currently in the queue. 

% Blocks are just logical; offset
It is important to note that a block is only a logical construct which provides access to and manages a range of tasks stored in the virtual array. 
This range is identified by the offset $o$ of the block and its capacity.
Thus, any operation performed at position $i$ within a block is actually performed at position $o+i$ within the virtual array.

% Memory management: tricky, we use Pheet's capabilities.
Memory management for concurrent data structures can be hard, since a shared object may only be deleted if it can be guaranteed that the object is not accessed by any thread at the time of deletion.
Pheet therefore provides a wait-free memory reuse scheme (see \cite{Wimmer14}, Section 4.3).
We will not concern ourselves with memory management in this work, but make the following assumptions, justified by Lemmas 4.3.2 and 4.3.3 in \cite{Wimmer14}:

\begin{assumption}
Deleting objects savely requires $O(1)$ time.
\end{assumption}

\begin{assumption}
When employing a memory manager to manage memory of size $n$, the size of the allocated memory is $O(n)$.  
\end{assumption}

% keys
As usual in the literature, we will denote the items managed by a priority queue as \emph{keys}.
% outline
On overview of the structure of the MCPQ is depicted in Fig.\ \ref{fig:structure}. The concepts of log-structured merge-lists and partitioning are discussed in sections \ref{sec:mcpq:description:lsm} and \ref{sec:mcpq:description:partitioning}, respectively. 
Section \ref{sec:mcpq:description:managing_blocks} shows how the logarithmic number of blocks is maintained.
Operations \verb|peek| (Section \ref{sec:mcpq:description:peek}), \verb|take| (Section \ref{sec:mcpq:description:take}), \verb|push| (Section \ref{sec:mcpq:description:push}) and \verb|pop| (Section \ref{sec:mcpq:description:pop}) as well as \verb|spy| (Section \ref{sec:mcpq:description:spying}) are discussed as soon as all relevant concepts and structures have been introduced.


\newcommand{\itemWidth}{7mm}
% draw a block
% #1	id of each block element will be b\i#1
% #2	nr of block elements
% #3	Items will be numbered k_#3,...,k_#3+#2
% #4	xshift of first element

\newcommand{\block}[4]{
\pgfmathsetmacro\to{#2 - 1}
\foreach \i in {0,...,\to}{
    \pgfmathsetmacro\idx{\i + #3}
    \node[minimum size=\itemWidth, rectangle, draw, xshift=#4+(\i-1)*\itemWidth](b#1i\i) {\tiny $k_{\pgfmathprintnumber{\idx}}$};
}
}

\newcommand{\blockto}[5]{
\pgfmathsetmacro\to{#2 - 1}
\foreach \i in {0,...,\to}{
    \node[minimum size=\itemWidth, rectangle, draw, xshift=#4+(\i-1)*\itemWidth](b#1i\i) { };
}
\pgfmathsetmacro\tok{#5 - 1}
\foreach \i in {0,...,\tok}{
    \pgfmathsetmacro\idx{\i + #3}
    \node[minimum size=\itemWidth, rectangle, xshift=#4+(\i-1)*\itemWidth] {\tiny $k_{\pgfmathprintnumber{\idx}}$};
}
\node[anchor=north west] at (b#1i0.south west) {\tiny Block $b_{#1}$};
}


% #1 	number of keys, 0 <= #1 <= 2
% #2  	Items will be numbered k_#2,...,k_#2+#1
% #3 	xshift of first element
\newcommand{\insertblock}[3]{
\foreach \i in {0,...,1}{
    \node[minimum size=\itemWidth, rectangle, draw, xshift=#3+(\i-1)*\itemWidth](bai\i) {};
}
\ifnum #1 > 0
    \foreach \i in {1,...,#1}{
	\pgfmathsetmacro\idx{\i - 1 + #2}
	\node[minimum size=\itemWidth, rectangle, xshift=#3+(\i-2)*\itemWidth] {\tiny $k_{\pgfmathprintnumber{\idx}}$};
    }
\fi
\node[anchor=north west] at (bai0.south west) {\tiny Block $b_a$};
}

\begin{figure}
\begin{center}
\begin{tikzpicture}[>=stealth']
\foreach \i in {0,...,12}{
   \node[minimum size=\itemWidth, rectangle, draw, yshift=2cm, xshift=\i*\itemWidth](va\i) {\tiny $k_{\i}$};
}
\node[anchor=south west] at (va0.north west) {\tiny Virtual Array};

\block{0}{8}{0}{\itemWidth*0};
\block{1}{4}{8}{\itemWidth*9};
\insertblock{1}{12}{\itemWidth*15};

 
\draw[dotted] (b0i0.north west) -- (va0.south west);
\draw[dotted] (b0i7.north east) -- (va7.south east);

\draw[dotted] (b1i0.north west) -- (va8.south west);
\draw[dotted] (b1i3.north east) -- (va11.south east);

\draw[dotted] (bai0.north west) -- (va12.south west);
\draw[dotted] (bai1.north east) -- (va12.south east);

\end{tikzpicture}
\end{center}
\caption{Overview of the MCPQ structure: The Virtual Array provides an array-like data structure of (potentially) infinite capacity and can be accessed concurrently in a lock- and wait-free manner by multiple threads. A log-structured merge-list operating on the keys stored by the Virtual Array is used by the owning thread to find Pareto-optimal keys. }
\label{fig:structure}
\end{figure}

\todo{Remove the ugly overview figure?}

\subsubsection{Log-structured merge-lists (LSML)} \label{sec:mcpq:description:lsm}
\begin{figure}
\begin{center}
\subfigure [Structure of an LSM-L containing 7 keys.] {
    \begin{tikzpicture}[>=stealth']
    \block{0}{4}{0}{\itemWidth*0};
    \block{1}{2}{4}{\itemWidth*5};
    \insertblock{1}{6}{\itemWidth*8};

    \path[->, bend left=20] (b0i3) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i3);
    \end{tikzpicture}
}

\subfigure [After adding an additional key, block $b_a$ is full.] {
    \begin{tikzpicture}[>=stealth']
    \block{0}{4}{0}{\itemWidth*0};
    \block{1}{2}{4}{\itemWidth*5};
    \insertblock{2}{6}{\itemWidth*8};
    
    \path[->, bend left=20] (b0i3) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i3);
    \end{tikzpicture}
}

\subfigure [After the merge of blocks $b_1$ and $b_a$. A new, empty block $b_a$ is added. ] {
    \begin{tikzpicture}[>=stealth']
    \block{0}{4}{0}{\itemWidth*0};
    \block{1}{4}{4}{\itemWidth*5};
    \insertblock{0}{8}{\itemWidth*10};
    \path[->, bend left=20] (b0i3) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i3);
    \end{tikzpicture}
}

\subfigure [After the merge of blocks $b_0$ and $b_1$.] {
    \begin{tikzpicture}[>=stealth']
    \block{0}{8}{0}{\itemWidth*0};
    \insertblock{0}{8}{\itemWidth*9};
    \end{tikzpicture}
}
\end{center}
\caption{Inserting a key into an LSML.}
\label{fig:lsml_merge}
\end{figure}
% what is it; blocks; size
An LSML is a doubly-linked list of a logarithmic number of sorted arrays, called \emph{blocks}. A block has a capacity of $c 2^l$, where $c$ is the capacity of the smallest block and $l$ is the level of a block.
% nr keys in blocks, insert block b_0
A block of level $l$ contains exactly $c 2^l$ keys and at most one block of level $l$ is allowed.
Given $n$ keys, $n' = c \lfloor \frac{n}{c} \rfloor $ keys are stored in such blocks, while the remaining $n-n'$ keys are stored in an additional block with capacity $c$, called $b_a$, which thus contains 0 to $c$ keys.  
% size of blocks
The levels of the blocks can easily be determined by the binary representation of $n'$:
The level of the $i$-th block is equal to the index of the $i$-th non-zero bit in the binary representation.
% ordering of blocks
The blocks $b_i$ are ordered s.t.\ a block $b_{i}$ is of a strictly greater level than its successor $b_{i+1}$.
Let $b_l$ be the last block in this sequence.
Block $b_a$ is assigned level $0$ and made the successor of $b_l$ only once it is full.
% Merging
In case block $b_l$ is of level $0$, blocks $b_l$ and $b_a$ are \emph{merged} into a single block of level 1.
To maintain the logarithmic number of blocks, this merging operation is applied recursively, that is, two blocks of the same level $l$ are always merged to a single block of level $l+1$.
Note that a new, empty block $b_a$ is created after the old one was merged with another block.
Fig.\ \ref{fig:lsml_merge} illustrates the structure of an LSM-L as well as the merging operation triggered by the insertion of a new key.

\begin{figure}
\begin{center}
\subfigure [Structure of an LSML after keys $k_5$ to $k_7$ have been deleted.] {
    \begin{tikzpicture}[>=stealth']
    \blockto{0}{8}{0}{\itemWidth*0}{5};
    \block{1}{4}{8}{\itemWidth*9};
    \insertblock{1}{12}{\itemWidth*14};

    \path[->, bend left=20] (b0i7) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i7);
    \end{tikzpicture}
}

\subfigure [After removing another key, block $b_0$ is shrunk\dots] {
    \begin{tikzpicture}[>=stealth']
    \blockto{0}{4}{0}{\itemWidth*0}{4};
    \block{1}{4}{8}{\itemWidth*5};
    \insertblock{1}{12}{\itemWidth*10};
    
    \path[->, bend left=20] (b0i3) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i3);
    \end{tikzpicture}
}

\subfigure [\dots and merged with its successor.] {
    \begin{tikzpicture}[>=stealth']
    \blockto{0}{4}{0}{\itemWidth*0}{4};
    \foreach \i in {0,...,3}{
	\pgfmathsetmacro\idx{\i + 4}
	\node[minimum size=\itemWidth, rectangle, draw, xshift=(\itemWidth*3)+(\i)*\itemWidth] {\tiny $k_{\pgfmathprintnumber{\idx}}$};
    }
    \insertblock{1}{12}{\itemWidth*9};
    \end{tikzpicture}
}

\end{center}
\caption{Deleting keys from an LSML.}
\label{fig:lsml_remove}
\end{figure}

% Removing keys
Deletion of keys is handled lazily: A key is at first merely marked as deleted. 
Once half of the keys of a block $b_i$ have been removed, it is shrunk to half its original capacity by decrementing its level. If necessary, the block is merged with its successor $b_{i+1}$ (blocks of level 0 will be deleted once empty; the block $b_a$ is never deleted or shrunk).
Note that at most one merge operation is necessary after shrinking a block.
Fig.\ \ref{fig:lsml_remove} illustrates this process.

\begin{corollary}
Merging two blocks requires constant time.
\end{corollary}
\begin{proof}
Since all operations are executed in-place on the Virtual Array, two blocks are merged by doubling the capacity (i.e., the range of keys maintained by the block) of the first and deleting the second block.
\end{proof}

% removing keys from virtual array
Note that when a block of level $l$ is shrunk, $2^{l-1}$ keys that have been marked for deletion have to be removed from the Virtual Array. 
To allow for a clean analysis, we make the following assumption:
\begin{assumption}
Removing a range of keys from the Virtual Array can be done in constant time.
\end{assumption}
\todo{tell where we will provide details for this}.


\begin{lemma}
An LSML requires at most $2 + \lfloor \log n \rfloor$ blocks to store $n$ keys.
\end{lemma}
\begin{proof}
If no keys are deleted, this follows directly from the definition of the LSML's structure via the binary representation of $n' = c \lfloor \frac{n}{c} \rfloor$. 

Assume that $\lfloor \log n \rfloor + 1$ blocks of level $l>0$ are required to store $n$ keys.
Furthermore, assume $c=1$.
Once half of the keys of a block are marked as deleted, the block is shrunk to half its original capacity, implying that in a block of level $l>0$, strictly less than half of the keys can be marked for deletion.
Thus, $\lfloor \log n \rfloor$ such blocks contain at least $n/2 + 1$ non-deleted keys.
An additional block of level $l>0$ must be of level at least $\lfloor \log n \rfloor + 1$ (at most one block of any level may exist) and thus contain at least $2^{\lfloor \log n \rfloor + 1}/2 \geq n/2 $ non-deleted keys; a contradiction, since less than $n/2$ keys are left.
In addition to the $\lfloor \log n \rfloor$ blocks of level $l>0$, the additional block $b_a$ is always allocated and one block of level 0 containing just 1 key may be present.

For $c>1$, the number of blocks does not increase since the capacity of each is multiplied by $c$.
\end{proof}

\begin{corollary}
An LSML uses at most $2(n + c)$ space to store $n$ keys.
\end{corollary}
\begin{proof}
This follows directly from the previous lemma.
\end{proof}

Note that so far we have not discussed how a block actually manages the keys it contains (this will be done in Section \ref{sec:mcpq:description:partitioning}. 
We will make the following assumption to analyze the operations \textit{push} and \textit{pop}:
\begin{assumption}
The \textit{push}, \textit{peek} and \textit{pop} operations on a \emph{single block} can be done in constant amortized time.
\end{assumption}

\begin{lemma}
The \textit{push} operation requires $O(\log n)$ amortized time.
\end{lemma}
\begin{proof}
\end{proof}

\begin{lemma}
The \textit{pop} operation requires $O(\log n)$ amortized time.
\end{lemma}
\begin{proof}
\end{proof}

\subsubsection{Partitioning} \label{sec:mcpq:description:partitioning}
%Idea
A block partitions the keys it holds by a pivot value, which is found by randomly\footnote{All random selections in this work are meant to be made \emph{uniformly and independently.}} selecting a key within the range to be partitioned and a random value $i$ in the range $[1,\dots,d]$ (where $d$ is the cardinality of the keys). 
The pivot element $x$ is then taken to be $x = k[i]$ and our procedure (Algorithm \ref{algo:partition})  for in-place partitioning multi-dimensional keys is similar to the well-known partitioning algorithm for scalar values\footnote{In principle, one could take the whole key as the pivot element. However, this might make the partitioning infeasible if the set of Pareto-optima is large.}.

%pivot generation
\begin{algorithm}
\caption{Pseudo-code for the generation of a pivot element}
\label{algo:partition}
\begin{algorithmic}[1]
\Function{Pivot}{Block $b$, $left$, $right$}
\State Randomly select a key $k$ in the range $b[left],\dots, b[right]$
\State $i \gets$ random number in $[1,\dots,d]$
\State $x \gets k[i]$ 
\State \Return{$x$}
\EndFunction
\end{algorithmic}
\end{algorithm}


% partitioning
\begin{algorithm}
\caption{Pseudo-code for partitioning a block}
\label{algo:partition}
\begin{algorithmic}[1]
\Require{Block $b$; $left,right \leq$ size of $b$; $left\leq right$}
\Require{$s_p$: desired maximum size of right-most partition}
\Function{Partition}{Block $b$, $left$, $right$}
\State $x \gets$ \Call{pivot}{$b$, $l$, $r$}
\While{$left < right$}
  \While{$left < right$ and (task $t = b[left]$, $w_k(t.p) \less_k x$)}
    \State $left = left + 1$
  \EndWhile
  \While{$left < right$ and (task $t = b[right]$, $w_k(t.p) \moreeq_k x$)}
    \State $right = right -1 $
  \EndWhile
  \If{$left < right$}
    \State Swap the tasks $b[left]$ and $b[right]$
    \State $left = left + 1$
    \State $right = right - 1$
  \EndIf
\EndWhile
\Statex \Comment{Check if task at $left$ belongs to left partition}
\If{$t = b[left]$, $w_k(t.p) \less_k x$} Pivot element generation should be its own subsection (
  $ left = left +1$
\EndIf
\Statex \Comment{Recursively partition the right part until it falls below the cut-off value}
\If{$left < right$ and $(right-left) > s_p$}
  \State \Call{Partition}{$b$,$left$,$right$}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}
Tasks with less priority (at dimension $k$) than the pivot element get sorted to the left, while tasks with higher or equal priority are sorted to the right.
We will call these two parts the left and the right partition and use a subscript to indicate which partitioning step created them: The two partitions created by the first partitioning step are thus partitions dubbed $p_{l1}$ and $p_{r1}$.

%Pareto optimality
Note that tasks that get sorted into the right partition are guaranteed to not be dominated by the ones in the left (since they have a higher priority value in at least one dimension).
Nevertheless, the left partition might contain Pareto-optimal tasks; this behavior is acceptable since the MCPQ is required to find but one Pareto-optimal task and not the whole set of Pareto-optimal tasks.
The right partition $p_{r1}$ is then partitioned recursively until it is smaller than a given constant cut-off value $s_p$, the maximum size that we are aiming for the right partition.
Note that the left partition $l_i$ of each partitioning step is not processed further and that the last right partition (for simplicity referred to as $p_r$) contains tasks that are not dominated by any task in any of the left partitions, 
% the problem with the right partition
but it may itself contain two tasks where one dominates the other. 
However, the \verb|peek| operation (Section \ref{sec:mcpq:description:peek} addresses this problem by linearly scanning the whole right partition $p_r$ to ensure that a non-dominated task is returned.
A different approach would be to flat out compute the Pareto-optima within $r_p$ by a linear expected-time algorithm as discussed in \cite{BentleyCL93, GodfreySG07}.

% figs and refs
Fig.\ \ref{fig:block_structure} the structure of a block after it was partitioned while Fig.\ \ref{fig:partition_with_nonactive}. provides an example for the partitioning process (both include some details that will be covered in the remainder of this section).

% problems we have here
In the algorithm outlined above, several important details still need to be addressed:

\medskip
\noindent
\textbf{Pivot generation}. As with other algorithms, e.g., Quicksort, it is crucial to select ``good'' pivot elements to avoid worst-case behavior. 
To get reasonable pivot values (at least on expectation) we sample multiple times and then choose the median as our pivot element.  

\medskip
\noindent
\textbf{Ensuring termination}. The algorithm as given above does not necessarily terminate.
Assume that all the tasks managed by a block have the same priority. 
Partitioning would sort all tasks into the right partition -- irregardless of which task is chosen as the pivot -- and then try to partition the very same elements again.
We call such an event a ``failed partitioning step''. 
Note that it may also occur if we choose the same pivot element as was used by the last partitioning step.
While in this case we can try to generate a new, different pivot element, the former case does not allow for further partitioning. 
We ensure termination by aborting the recursive procedure after a constant number of successive failed partitioning steps.
In this case, the size of the right-most partition will be larger than the size we are aiming for; nevertheless, it is guaranteed that the tasks it contains are not dominated by tasks that have been sorted to the left.

\medskip
\noindent
\textbf{Handling dead, taken and removed tasks}. Once a tasks is popped, it has to be removed from the queue. 
Furthermore, tasks that have been marked dead or taken also need to be removed from the MCPQ. 
% active, non-active tasks
We will subsume all those tasks as ``non-active'', whereas a task still awaiting execution is called an active task.
% How a block handles dead tasks
Speaking informally, each block moves its non-active tasks to the end of the block and keeps them there.
Once the block gets merged with another block, the non-active tasks of both blocks are moved to the end of the block generated by the merge.
Thus, the non-active tasks gradually migrate to the end of the sequence of blocks, i.e., to the end of the virtual array.
Once they reach the very last block, the non-active tasks may be dropped from the priority queue. 

% Only cover blocks here
In the following, we will describe in detail how a block handles non-active tasks. 
All aspects concerning the sequence of blocks -- like merging blocks and dropping the non-active tasks -- are discussed in Section \ref{sec:mcpq:description:managing_blocks}.

\newcommand{\partition}[4]{
  \node[minimum width=#4, minimum height=8mm, rectangle, draw, anchor=west] at (#2.east) (#1) {};
  \node[anchor=north west] at (#1.south west) {\tiny #3}
}

% structure of a block
\begin{figure}
\begin{center}
\begin{tikzpicture}
\node (n0) {};
\partition{n1}{n0}{$p_{l_1}$}{30mm};
\partition{n2}{n1}{$p_{l_2}$}{20mm};
\node[anchor=west,yshift=2.5mm] at (n2.east) (d1) {\tiny\dots};
\node[anchor=west, minimum width=6mm] at (n2.east) (d0) {};
\node[anchor=west,yshift=-2.5mm] at (n2.east) (d1) {\tiny\dots};
\partition{n3}{d0}{$p_{l_i}$}{15mm};
\partition{n4}{n3}{$p_{r}$}{15mm};
\partition{n5}{n4}{$p_{na}$}{25mm};
\end{tikzpicture}
\caption{Structure of a block after partitioning. From the left to the right, we have 1) several left partitions $p_{li}$, one created by each partitioning step; 2) the right partition $p_r$, containing tasks not dominated by any task in the left partitions; and 3) the section $p_{na}$ containing the non-active tasks of this block. 
}
\label{fig:block_structure}
\end{center}
\end{figure}

% Extended algorithm, structure of a blocks
% TODO: don't want to reference it again
% Fig.\ \ref{fig:block_structure} shows the structure of a block, after it was partitioned.

% extended algorithm
The partitioning procedure (Algorithm \ref{algo:partition}) is adapted as follows to handle non-active tasks:
\begin{itemize}
\item Only active tasks are sampled for the pivot element.
\item An additional partition, called $p_{na}$, is used for storing the non-active tasks.
Initially, we have $na = end$.
\item If a non-active task is encountered during the scan for the next task to swap, it is swapped out to the section holding the non-active tasks. 
\end{itemize}

The adopted procedure is shown in Algorithm \ref{algo:partition_with_nonactive}.
An example illustrating the procedure is shown in Fig. \ref{fig:partition_with_nonactive}.

\begin{algorithm}
\caption{Pseudo-code for partitioning with non-active tasks}
\label{algo:partition_with_nonactive}
\begin{algorithmic}[1]
\Function{Partition}{Block $b$, $left$, $right$}
\State $x \gets$ \Call{pivot}{$b$, $l$, $r$}
\State $non\_active \gets right$ 
\While{$left < right$}
  \While{$left < right$}
    \If{$left$ is an active task}
      \If{$left \less_k x$} \Comment{If the task at $left$ has...}
	\State $left = left + 1$  \Comment{...less priority than the pivot, it stays,...}
      \Else \State break; \Comment{...otherwise, it is moved to the right partition.}
      \EndIf
    \Else \Comment{$left$ is a non-active task}
      \State $non\_active = non\_active - 1$; \Comment{Now points to an active task}
      \State swap the tasks $b[left]$ and $b[non\_active]$
      \If{$non_active == right$} $ right = right - 1 $;
      \EndIf
    \EndIf
  \EndWhile
  \While{$left < right$}
    \If{$right$ is an active task}
      \If{$right \moreeq_k x$} \Comment{If the task at $right$ has more or equal...}
	\State $right = right - 1$  \Comment{...priority as the pivot, it stays,...}
      \Else \State break; \Comment{...otherwise, it is moved to the left partition.}
      \EndIf
    \Else \Comment{$right$ is a non-active task}
      \State $non\_active = non\_active - 1$
      \If {$right == non\_active$} $right = right - 1$
      \Else
	\State Swap the tasks $b[right]$ and $b[dead]$;
	\State $left = left + 1$
	\State $right = right - 1$
      \EndIf
    \EndIf
  \EndWhile
  \If{$left < right$}
    \State Swap the tasks $b[left]$ and $b[right]$;
  \EndIf
\EndWhile
\Statex \Comment{Check if tasks at $left$ belongs to left partition (as above).}
\Statex \Comment{Partition recursively, if necessary (as above).}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{figure}
\begin{center}
\begin{tikzpicture}
\end{tikzpicture}
\caption{Example of a partition run with non-active tasks}
\label{fig:partition_with_nonactive}
\end{center}
\end{figure}

\subsubsection*{Analysis} \label{sec:mcpq:description:partition:analysis}
\todo{this might be hard}

\subsubsection{Peek} \label{sec:mcpq:description:peek}
% peek
Once a block is partitioned, the \verb|peek| operation (Algorithm \ref{algo:peek} is relatively easy:
We simply need to scan the right partition. The first encountered active task is kept as a candidate and compared to all remaining tasks in the partition to ensure that it is not dominated by any of them; if it is, the dominating task is taken as the new candidate.
% falling back
In case no active task is found in the right partition, we simply fall back to the previous partition: since the right partition does not contain any active task, it is merged with the section holding non-active tasks; the left partition with highest index is then taken as the new right partition.
This partition might have to be partitioned further, since it can be arbitrarily large and thus exceed the size $s_p$ that we aim for the right partition. 
Once the top partition again falls below $s_p$, we can again scan for an active task.
% dead block
In case no active task can be found in the whole block (i.e., after falling back to the last existing partition), the whole block is marked as non-active, implying that no more peek operations have to be executed on it.

\begin{figure}
\begin{center}
\caption{Peek and falling back}
\label{fig:peek}
\begin{tikzpicture}
\end{tikzpicture}
\end{center}
\end{figure}

\begin{algorithm}
\caption{Pseudo-code for the peek operation}
\label{algo:peek}
\begin{algorithmic}[1]
\Require{Block $b$ is partitioned into partitions $p_{l1},\dots , p_{li}, p_r, p_{na}$} 
\Function{peek}{Block $b$}
\State Task $t = null$ 
\ForAll{Tasks $x$ in $p_r$} \Comment{Scan the right partition}
  \If{$t == null$ or $x \less t$} $t = x$
  \EndIf
\EndFor
\If{$t == null$} \Comment{No active task in the right partition}
  \State Extend $p_{na}$ to include $p_r$ \Comment{Add all the tasks in $p_r$ to the non-active tasks section}
  \If{$p_{l1} < p_{na}$}
    \State Set $p_r \gets p_{li}$
    \State \Call{Partition}{$b$, $p_r$, $p_{na}$} \Comment{Further partition $p_r$}
    \State \Call{peek}{$b$}	\Comment{Call peek on the new right partition again}
  \EndIf
\EndIf
\State \Return{$t$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Take} \label{sec:mcpq:description:take}
While the \verb|peek| operation finds a Pareto-optimal task without removing it from the queue, the \verb|take| operation removes a given task from the MCPQ by marking it as taken before returning it.

\subsubsection{Managing blocks} \label{sec:mcpq:description:managing_blocks}
% adding elements
As outlined above, all items that are added to the MCPQ are first stored in a special block of level 0, called $b_\text{insert}$. 
Once this block is full, it is merged with the block $b_0$ (also of level 0) to create a block of level 1. 
% merging
A merge of two adjacent blocks is performed by extending the first block to cover both its own and the other block's items.
% repartition
The resulting block then needs to be partitioned again.

% removing elements
When a task is to be removed from the queue (either by a \verb|pop| operation, or by being marked as dead or taken), it is not immediately deleted from the queue, since this would not allow us to maintain the logarithmic structure of the blocks.
Instead, non-active tasks are collected by each block and migrate towards the end of the queue with each merging operation. 
Once they reach the very last block, the non-active tasks can savely be removed from the priority queue.


% The problem
This implies that the MCPQ may contain a large number of non-active tasks and that their removal depends on the occurrence of merging operations. 

\todo{we need to bound the number of elements in the MCPQ}

\begin{algorithm}
\caption{Pseudo-code for merging several blocks}
\label{algo:merge_from}
\begin{algorithmic}[1]
\Function{merge\_from}{Block $b$}
\State let $b_p$ be the block predecessing $b$
\State $merged \gets false$
\While{$b_p$ exists and is either a non-active block or of same level as $b$ }
\State merge $b_p$ and $b$
\State $b \gets b_p$
\State $b_p \gets$ predecessor of $b$
\State $merged \gets true$
\EndWhile
\If{$merged$}
\Call{Partition}{$b$}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}


\subsubsection{Push} \label{sec:mcpq:description:push}
Adding new tasks to the MCPQ via the \verb|push| operation is quite simple, as shown in Algorithm \ref{algo:push}.
Once the insertion block is full, all the tasks it contains are moved to an new block at the end of the linked list and blocks are merged if necessary.

\begin{algorithm}
\caption{Pseudo-code for the push operation}
\label{algo:push}
\begin{algorithmic}[1]
\Require{Insert-block $b_\text{insert}$}
\Function{Push}{Task $t$}
\If{$b_\text{insert}$ is full}
  \State Create a new block $b$ at the end of the linked list
  \State Move all elements from $b_\text{insert}$ to $b$
  \State \Call{merge\_from}{$b$}
\EndIf
\State Insert $t$ in $b_\text{insert}$
\EndFunction
\end{algorithmic}
\end{algorithm}

\bigskip
\noindent
\textbf{Analysis.}
\todo{again, this might be hard}

\subsubsection{Pop} \label{sec:mcpq:description:pop}
\begin{algorithm}
\caption{Pseudo-code for the pop operation}
\label{algo:pop}
\begin{algorithmic}[1]
\Function{pop}{item $boundary$}
\State Item $item_{best} \gets$ \Call{peek}{insertion block}
\State Block $b_{best} \gets$ insertion block
\State Block $b \gets $ insertion block
\While{$b.next$ exists}
  \State $b \gets b.next$
  \If{$b$ is non-active}
    \State continue
  \EndIf
  \State Item $item \gets$ \Call{peek}{$b$}
  \If{$item == null$}
    \State mark $b$ as non-active block
  \Else
    \If{$!item_{best} || item \less item_{best}$}
      \State $item_{best} \gets item$
      \State  $b_{best} \gets b$ 
    \EndIf
    \State \Call{try\_reduce\_level}{$b$}
    \State \Call{merge\_from}{$b$}
  \EndIf
\EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}


\subsubsection{Spying} \label{sec:mcpq:description:spying}
\begin{algorithm}
\caption{Pseudo-code for the spy operation}
\label{algo:spy}
\begin{algorithmic}[1]
\Function{spy}{item $boundary$}
\State Place $other \gets item.owner$
\If{$item$ is taken}
  \State return $null$
\EndIf
\For{$i \gets other.begin$; $i < other.end$; $i++ $}
  \State Item $item \gets other[i]$
  \If{$item$ is active}
    \Call{Put}{$item$}
  \EndIf
\EndFor


\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Integration in Pheet} \label{sec:mcpq:pheet_integration}
% Strategy scheduler 2
Pheet's \verb|StrategyScheduler2|, a \emph{two-level concurrent ordered container} (described in detail in \cite{Wimmer14}, Section 5.5) allows for a quick integration of our MCPQ into the scheduling framework.
% Base classes provided; pheet terminology
It offers base classes for alternative \emph{task storage}\footnote{In Pheet terminology, a priority queue managing tasks is called a task storage.} implementations.
% The classes we us.
In particular, to integrate the MCPQ into the Pheet scheduling framework, we will be extending the following classes:
\begin{inparaenum}
\item \verb|Strategy2BaseTaskStorageBaseItem|,
\item \verb|Strategy2BaseTaskStoragePlace| and
\item \verb|Strategy2BaseTaskStorage|.
\end{inparaenum}
We will give an intuitive understanding of their functionality required by our implementation together with a detailed description of the classes extending those base classes in the following. 
For a detailed description of the base classes and correctness proofs, we refer the reader to \cite{Wimmer14}.


\subsection{Implementation} \label{sec:mcpq:impl_analysis}
\todo{say that we proceed in a bottom-up manner}
\subsubsection*{Other stuff}
\todo{This does not really fit here}
Furthermore, such a \emph{distributed} implementation of the MCPQ avoids performance bottlenecks inherent to centralized data structures.

% lock- and wait-free
 To provide good progress guarantees, the MCPQ is implemented as a lock- and wait-free data structure. 
\todo{More here? or just refer to diss. p. 49? This last sentence doesn't fit anyway}



\subsubsection{Interfaces, overview}
% PriorityVector
We map the mathematical concept to an object as shown in Listing \ref{lst:priority_vector}, with \verb|PriorityValue| representing a value from a domain $U_i$ for $1 \leq i \leq d$.
We use this representation of a priority vector when referencing it in pseudo-code. 
\begin{code}[label=lst:priority_vector, caption=PriorityVector]
/* To be implemented by the application. */
class PriorityVector {

  /* Get the priority value at dimension d */
  PriorityValue priority_at(int d);
  
  /* Return true if this priority vector at dimension d has a priority value smaller than v */
  bool less_priority(int d, PriorityValue v);
  
  /* Return true if this priority vector at dimension d has a priority value greater than v */
  bool greater_priority(int d; PriorityValue v);
}
\end{code}

% A general, high level overview and interfaces
The class \verb|ParetoItem| (Listing \ref{lst:PTSItem}) is used to represent tasks managed by scheduler. It references an object of type \verb|SchedulerTask| (Listing \ref{lst:SchedulerTask})(which contains the code to be executed, i.e., the work the task is to perform) and an object implementing the \verb|ParetoStrategy|-interface (Listing \ref{lst:ParetoStrategy}) (which handles the priority of the task).

The \verb|ParetoItem| class has to guarantee that its \verb|take| operation returns the referenced task exactly once, even if multiple threads execute it concurrently. 
Although an item is at first only visible to the thread creating it (also called the \emph{owning} place), this may change after another thread performs a \verb|steal| operation on the priority queue of the owning place.
The base class \verb|BaseItem| provides some functionality for memory re-usage and serves as an interface for \verb|StrategyScheduler2|, which is oblivious of the task storage implementation.	

% ParetoItem
\begin{code}[label=lst:PTSItem, caption=Interface of ParetoItem]
class ParetoItem : BaseItem {

  /* The task to execute. */
  SchedulerTask task;
  
  /* The strategy associated with the task. */
  ParetoStrategy strategy;
  
  /* If false, no thread is processing this item yet. */
  atomic<bool> taken;
  
  /* Get the task referenced by this item if it is not taken. A task may be taken only once. */
  SchedulerTask take();
}
\end{code}

The \verb|SchedulerTask| and \verb|ParetoStrategy| classes have to be implemented by the application employing the scheduler, i.e., the application programmer.
While the former contains the code to be executed upon execution of the task, the latter provides access to the priority vector associated with a task.
Furthermore, the function \verb|priorizite|, given another \verb|ParetoStrategy| object, decides which of the two has higher priority. 
The \verb|dead_task| function is used by the scheduler to query whether the associated task may be dropped without being executed\footnote{This feature, called ``speculative execution'', is described in Section \ref{sec:pheet:task_priorites}}.
Both functions will be used by our task storage implementation to order tasks w.r.t.\ their associated priorities.

% Task
\begin{code}[label=lst:SchedulerTask, caption=Interface of SchedulerTask]
/* To be implemented by the application. The code in execute() will be run when the task is executed. */
class SchedulerTask {

  void execute();
}
\end{code}

% Strategy
\begin{code}[label=lst:ParetoStrategy, caption=Interface of ParetoStrategy]
/* To be implemented by the application. */
class  {

  void initialize(PriorityVector v);
  
  PriorityVector priority_vector();
  
  /* Return true if the priority vector of this strategy is not dominated by the priority vector of other. */
  bool prioritize(ParetoStrategy other);
  
  /* Return true if the associated task is marked as dead; i.e., if it needn't be executed anymore. */
  bool dead_task();
}
\end{code}
The central -- or global -- part of the MCQP is implemented in \verb|ParetoTaskStorage| (Listing \ref{lst:ParetoTaskStorage}), which is mostly used for rerouting \verb|pop| and \verb|steal| operations to the place-local parts of the MCPQ, implemented in \verb|ParetoTaskStoragePlace| (Listing \ref{lst:ParetoTaskStoragePlace}). 
% boundary
Notice that the \verb|pop| and \verb|steal| operations of both classes have an additional parameter called the boundary item. 
The \verb|StrategyScheduler2| maintains its own LIFO priority queue\footnote{This is necessary for some advanced features the base data structures provide. We do not require them, they are not introduced in this report.} and the boundary item would be the next item selected by \verb|pop|/\verb|steal| according to this order.
However, this default behavior may be overridden within certain limits: 
\begin{itemize}
\item The \verb|pop|/\verb|steal| operation may select other items with a \emph{higher} priority than the boundary item, thereby violating the LIFO order. Since we are using multi-dimensional priority vector, this means that the priority associated with the selected item may not be dominated by the priority of the task referenced by the boundary item.
\item The \verb|pop| operation must not return a \verb|SchedulerTask| if the boundary item was already taken by another thread in the meantime.
\end{itemize}

% TaskStoragePlace
\begin{code}[label=lst:ParetoTaskStoragePlace, caption=Interface of ParetoTaskStoragePlace]
/* The place-local part of the MCPQ. */
class ParetoTaskStoragePlace : Strategy2BaseTaskStoragePlace {

  /* Add the given item to the task storage. */
  void push(ParetoItem item);
  
  /* Get an item with locally non-dominated priority and remove it from the task storage. */
  SchedulerTask pop(ParetoItem boundary);
  
  /* Get an item (with locally non-dominated priority) by stealing it from another place. */
  SchedulerTask steal(ParetoItem boundary, ParetoTastStoragePlace other_place);
}
\end{code}

% TaskStorage
\begin{code}[label=lst:ParetoTaskStorage, caption=Implementation of ParetoTaskStorage]
/* The global part of the MCPQ. Calls to pop and steal are simply forwarded to the responsible places. */
class ParetoTaskStorage : Strategy2BaseTaskStorage {

  ParetoTaskStoragePlace[] places;
  
  SchedulerTask pop(ParetoItem boundary, int place_id) {
    return places[place_id].pop(boundary);
  }
  
  SchedulerTask steal(ParetoItem boundary, int place_id) {
    return places[place_id].steal(boundary);
  }
}
\end{code}


% Virtual Array
\subsubsection{Virtual array} \label{sec:virtual_array}
% Motivation
The \verb|VirtualArray| class (Listing \ref{lst:VirtualArray}) provides a thread-safe array of potentially unlimited size. 
It is used to relief other classes from certain aspects of memory management which can be tricky for shared data.
An instance of \verb|ParetoTaskStoragePlace| will store all the items it manages in the place-local \verb|VirtualArray|, which allows other places (i.e., threads) to scan the \verb|VirtualArray| and steal some of the items it holds. 
Thus, the \verb|VirtualArray| serves as a clear separation of the data structure into a part that is accessed by multiple threads and a part accessed only by the owning thread.

% Interface
\begin{code}[label=lst:VirtualArray, caption=Virtual array]
class VirtualArray {
public:
  /* Directly access the item at position idx. Complexity: O(n) */
  ParetoItem operator[](int idx);
  
  /* Obtain an iterator to the item at position idx. Complexity: O(n) */
  VirtualArrayIterator iterator_to(int idx);
  
  /* Increase the capacity by value. */
  void increase_capacity(int value);
  
  /* Decrease the capacity by value. */
  void decrease_capacity(int value);
}
\end{code}

\begin{code}[label=lst:VirtualArrayIterator, caption=Virtual array iterator]
class VirtualArrayIterator {
public:
  /* Increment the iterator. Complexity: O(1) */
  void operator++();
  
  /* Decrement the iterator. Complexity: O(1) */
  void operator--();
  
  /* Atomically access the item referenced by this iterator. Complexity: O(1) */
  ParetoItem operator*();
}
\end{code}

% Implementation
It is implemented via a doubly-linked list of blocks (class \verb|VirtualArrayBlock|) of constant size $s_\text{v}$. 
This doubly-link list may be traversed in forward direction by all threads; but only the owning thread may traverse it backwards or alter its structure. 
To ensure thread-safety, references to the items in the array are stored as atomic pointers.
% Analysis

\bigskip
\noindent
\textbf{Analysis.}
Since the virtual array is implemented as a linked list, accessing any element takes $O(n)$ time.
To provide for constant time access -- at least when traversing a consecutive range of elements, which is done by many operations, e.g., spying and partitioning -- we provide an iterator (Listing \ref{lst:VirtualArrayIterator}) which allows constant time access to the item it currently references as well as to the next and previous item in the array.
Thus, accessing a consecutive sequence $m$ of elements can be done in $O(n)$ time by the use of iterators.

\bigskip
\noindent
\textbf{Capacity management.}
% Memory management & usage
The capacity of the virtual array may be increased and decreased dynamically.
Let $c$ be the current capacity of the virtual array.
After a call to \verb|increase_capacity| with parameter \verb|value| returns, the virtual array may be accessed in the range $[0, c + \verb|value|]$. 
Since we provide access to the elements of the virtual array via iterators, it might occur that one thread wants to access an element via an iterator, while another thread already decreased the capacity of the virtual array s.t.\ the element is not in the valid range anymore. \todo{there should be a footnote to the section that makes it clear why this can happen, i.e., ParetoTaskStoragePlace}. 
To avoid accessing an invalid memory location in this situation, the operation \verb|decrease_capacity| merely marks an amount of items equal to \verb|value| as reusable.
That is, given an initial capacity $c$, the range $r_d = [c - \verb|value|, c]$ may still be accessed after the operation returns. 
However, that range will be reused for storing items once the capacity of the virtual array is increased again.
Note that a subsequent increase of the capacity might lead to items in the range $r_d$ to be overwritten. 
Thus, care must be taken to ensure that, at the time \verb|decrease_capacity| is called, no item in the range $r_d$ has to be accessed by any thread via the array anymore.
We will describe in Section \todo{which?} how this is ensured.
\todo{the capacity is only ever decreased if there are no more active items in the reduced range, i.e., all items in that range are either null, dead or taken. Another thread still scanning that range thus would not steal such items; a subsequent increase and refill would thus overwrite only non-active items!}


\begin{comment}
\section{Bits and pieces}

% concurrent DS
A priority queue for Pheet needs to be able to handle \emph{concurrent} accesses by multiple threads since each thread will fetch tasks to execute from the queue and insert newly generated tasks into the queue.

%Node
\todo{do we really need this? or can we assume that it is known/intuitively understandable?}

\begin{minipage}[c]{\linewidth}
\begin{lstlisting}[label=lst:Node, caption=Basic structure and operations of Node.]
class Node {
  /* All nodes adjacent to an outgoing arc of this Node. */
  Set<Node> neighbors();
}
\end{lstlisting}
\end{algorithm}

% Path
\begin{minipage}[c]{\linewidth}
\begin{lstlisting}[label=lst:Path, caption=Basic structure and operations of Path.]
class Path {
  Path step(Arc a);
  Node tail();
  Node head();
  Weight_Vector weight();
  bool dominated;
}
\end{lstlisting}
\end{algorithm}

% pareto priority queue idea
%\begin{comment}
 \cite{Wimmer14}

% What does the scheduler do?
The task parallel model is independent of any machine specifics such as the number of processors and the memory hierarchy.
It merely exposes the available work to the \emph{scheduler}, which is responsible for devising a schedule , i.e., a mapping of tasks to the processors, thereby determining which processor executes a task as well as the execution order of the tasks. 
The scheduler 

% Online/offline scheduling; what is used in Pheet and why
The schedulers developed for Pheet employ \emph{online scheduling}, where -- in contrast to \emph{offline scheduling} -- the schedule cannot be precomputed statically, since not all the required information is available before the execution.
This is due to e.g., new tasks being created during the execution and/or the nondeterministic execution time of tasks.

% Explain that we don't explain scheduling
Task scheduling itself is a vast and active area of research with many different variations in the problem setting and models.
Since we do not deal with any complex scheduling in this work, we refrain from giving a more elaborate introduction and refer the reader to the Wimmer's dissertation\cite{Wimmer14}, which contains several pointers to the literature.
% task parallelism
Additionally, the programmer may define dependencies between the tasks which ensure that a task is not executed before all required prerequisites are met; if, however, two tasks do not depend on each other, they may be executed in parallel (or sequentially in arbitrary order).
\todo{the above is quite general and does not really relate to the rest of the work or pheet.}

Furthermore, once a place starts executing a task, the task will stay at this place until its execution is finished. 

%This is not relevant
By uniquely identifying processors, places simplify the implementation of parellel algorithms and data structures.

Furthermore, places are employed to deal with aspects related to the memory hierarchy as well as locality. 
However, as those concepts are not considered in our contribution, we refer the interested reader to the work of Wimmer\cite{Wimmer14}.
\end{comment}

\section{Shortest path problems} \label{sec:shortest_path}
% Informally introduce shortest path
The problem of finding a shortest path between two nodes in a graph is a classical problem in computer science with numerous practical applications, such as finding the quickest route from location A to location B.
% Introduce the problem of msp
While the \emph{shortest-path problem} minimizes for a single criteria, e.g., the travel time, in practical applications one often wants to optimize multiple -- and possibly conflicting -- objectives:
When searching for a route from A to B, we may want to minimize the travel time as well as toll costs.
In such a setting, we usually cannot give a single best solution anymore. 
Instead, we can give several reasonable solutions: 
The fastest route will usually use highways, thus increasing the toll costs, while a slightly slower route may reduce the toll costs by avoiding highways. 
Problems like this can be modeled as \emph{multi-criteria shortest path} (MSP) problems, where we are interested in all optimal alternatives, i.e., in routes that are \emph{Pareto-optimal}. 
A route from A to B is Pareto-optimal if there is no other route with less or equal cost for each of the $d$ objectives.

%subsection{Definitions and notation} \label{sec:shortest_path:defs}
In the following, we provide definitions for variations of the shortest path problems considered in this report. 
The classic text book by Cormen et al.~\cite{CLRS01} provides a more detailed introduction to basic shortest path problems and algorithms.

% Define single-source shortest path
\subsection{The classical shortest path problem} \label{sec:shortest_path:defs:sssp}
We are given a weighted directed graph $G=(V,A)$, where $V= \{v_1, v_2, \dots, v_n\}$ is a finite set of nodes, $A \subseteq V \times V$ is a finite set of arcs (directed edges) and $c: A \rightarrow \mathbb{N}$ is the cost or weight function. 
We define the set of neighbors of a node $v$ as the nodes adjacent to an outgoing arc of $v$, i.e., $\{w \in V \mid \exists a\in A, a = (v,w) \}$.
W.l.o.g., we assume that $G$ is a connected graph.

Let $p = \langle v_0, a_1, v_1, a_2, \dots,a_k, v_k \rangle$ be a path\footnote{We will also write $p = \langle v_0, v_1, \dots,v_k \rangle$, since an arc $a_i$ is well defined by its two adjacent nodes.} from $v_0$ to $v_k$ in $G$.
Furthermore, let $P$ be the set of all paths and $P_{u,v}$ be the set of all paths from node $u$ to node $v$ in $G$.
We define the weight $w(p)$ of a path $p$ as 
\begin{align*}
w: P &\rightarrow \mathbb{N} \\
p &\mapsto w(p) = \sum_{i=1}^{k}c(a_i) \quad , \neqn{sssp_weighted}
\end{align*}
i.e., the sum of costs all the arcs in the path $p$.

% Variations: Single source, all-pairs,... shortest path
In a \emph{single-source shortest path problem (SSSP)}, we want to compute a shortest path w.r.t.~some weight function $w$ from a given source node $s \in V$ to each node $v \in V$.
Different variations of the problem exist, such as the \emph{single-destination}, the \emph{single-pair} or the \emph{all-pairs} shortest path problem. 
Note that an optimal algorithm for the SSSP can easily be turned into an optimal algorithm for the first two variants \cite{CLRS01}.
Thus, we will only deal with the SSSP in this report.

\subsection{Multi-criteria shortest path (MSP)} \label{sec:shortest_path:defs:msp}
% MSP
Multi-criteria shortest path problems \cite{Martins84} generalize shortest path problems w.r.t.~the weight function $c$, which is extended to $d$-dimensional vectors, i.e., $\vect{c}: A \rightarrow \mathbb{N}^d$ and
\begin{align*}
\vect{w}: P &\rightarrow \mathbb{N}^d \\
p &\mapsto \vect{w}(p) = (w_1(p), w_2(p), \dots , w_d(p)) \quad , \neqn{msp_weighted}
\end{align*}
where $w_j(p) = \sum_{i=1}^{k}c_j(a_i)$ $\forall j \in \{1,\dots,d\}$.

% Pareto optima
A path $p$ from node $v_1$ to node $v_2$ is a \emph{Pareto-optimal} or \emph{non-dominated} path if there is no path $q \in P$ from $v_1$ to $v_2$ s.t.~$q \dom p$.
Informally, we also call a Pareto-optimal path $p$ \emph{shortest path}.
Thus, for the MSP, the solution consists of a set of Pareto-optimal paths for each considered pair of nodes.


\subsubsection*{Applications} \label{sec:shortest_path:applications} 
% Applications
Apart from the problem of finding an optimal route in a road map as outlined above, the MSP problem has numerous outer applications, such as routing in multimedia networks \cite{ClimacoCP03}, route guidance \cite{JahnMS00} and curve approximation \cite{MehlhornZ00}.

\subsubsection*{Hardness and practicality} \label{sec:shortest_path:hardness}
% Hardness of the problem, reason for parallelization
% Note that the graph instances resulting from modeling practical problems are potentially very large. 
The crucial parameter for the complexity of the MSP is the total number of Pareto optima for all visited nodes. 
Since this number is exponential in $n$ in the worst case \cite{Hansen80}, the MSP is in general NP-hard.
Even for $d=2$, the decision problem whether there exists a path between two nodes whose length is below a given threshold is NP-hard \cite{GareyJ79}.
However, it was observed that the problem is efficiently tractable from a practical viewpoint for many practically relevant instances. 
The input data of practical applications tends to have certain characteristics, which lead to the set of Pareto optima for a vertex to be polynomially bounded \cite{Muller-HannemannW06}. 
It was shown that in applied scenarios this number may even be bounded by a small constant \cite{Muller-HannemannW01}. 
Thus, MSP can be solved efficiently for small $d$ and not too large graphs adhering certain characteristics.

% reason for parallelization
However, due to the potentially very large graph instances resulting from modeling e.g., road or railway networks, the computational cost is significant even if a small number of criteria is to be optimized.
Guerriero and Musmanno \cite{GuerrieroM01} thus suggest that parallel computing might help to design efficient solution methods. 

\subsubsection*{Approximate solutions} \label{sec:shortest_path:approx}
% heuristics
Due to the hardness of the MSP, heuristic methods are sometimes employed (see, e.g., \cite{BastMS03, Sonnier06, EhrgottG02}).
% weighted sum approach
A common approach is to define a total order relation $\tor$ on $P$ that allows for a more efficient computation of the Pareto-optima.
Such a relation $\tor$ has to satisfy the following properties\footnote{We denote the concatenation of two paths $p_1 \in P_{u,v}$, $p_2 \in P_{v,w}$ as $\cP{p_1}{p_2}$}
\begin{align}
\forall p,q \in P_{u,v}: p \dom q \implies p \tor q  \quad \text{Dominance} \\
\forall p \in P_{u,v}, \forall (v,w) \in A: p \tor \cP{p}{w} \quad \text{Monotonic}
\end{align}
Martins et al.~\cite{MartinsPRS07} show that the following relation satisfies these requirements:
\begin{align}
p \osum q \iff \sum_{i=1}^{d} w_i(p) \leq  \sum_{i=1}^{d} w_i(q) 
\end{align}
% supported/non-supported solutions
However, as Sonnier \cite{Sonnier06} points out, a problem with algorithms based on such a relation is that 
\begin{align}
p \osum q  \notimplies p \dom q \quad , \neqn{osum_problem}
\end{align}
i.e., only solutions that lie on the convex hull of the feasible region are found, which are called the supported solutions. 
In other words, there exist Pareto-optima which may not be found because of the property given in Eq.~\ref{eq:osum_problem}.

% graph example from Sonnier
To see this, consider the following example from Sonnier \cite{Sonnier06}.
Assume we are interested in the Pareto-optimal paths from node 1 to node 5 in the graph depicted in Fig.~\ref{fig:example_graph}.

\begin{figure}
\begin {center}
\begin {tikzpicture}[-latex, auto, node distance = 3 cm, on grid, semithick, state/.style = {circle, top color = white, draw, minimum width = 1 cm}]
\node[state] (n1) {$1$};
\node[state] (n2) [below right of = n1] {$2$};
\node[state] (n3) [above right of = n2] {$3$};
\node[state] (n4) [below right of = n3] {$4$};
\node[state] (n5) [above right of = n4] {$5$};
\path (n1) edge node[above = 0.05 cm] {$(1,4)$} (n3);
\path (n3) edge node[above = 0.05 cm] {$(4,1)$} (n5);
\path (n1) edge [bend right = 30] node[below = 0.2 cm] {$(1,1)$} (n2);
\path (n2) edge [bend right = 30] node[below = 0.2 cm] {$(1,2)$} (n3);
\path (n3) edge [bend right = 30] node[below = 0.2 cm] {$(2,1)$} (n4);
\path (n4) edge [bend right = 30] node[below = 0.2 cm] {$(1,1)$} (n5);
%\path (n1) edge [bend left = 30, color=red] node[above = 0.2 cm, color=red] {$(3.8,6.8)$} (n5);
\end{tikzpicture}
\end{center}
\caption{A weighted ($d=2$) directed graph.}
\label{fig:example_graph}
\end{figure}

The solution set is shown in Table \ref{table:pareto_paths}; note that all the paths are supported solutions, since we have 
\begin{align}
\forall j,k \in \{1,\dots,4\}: \sum_{i=1}^{d} w_i(p_j) =  \sum_{i=1}^{d} w_i(p_k) \quad .
\end{align}

\begin{table*}
\begin{center}
    \begin{tabular}{| l | c |}
    \hline
    \textbf{Path} & \textbf{Weight vector} \\ \hline \hline
    $p_1 = \langle 1,2,3,4,5 \rangle$ & (5,5) \\ \hline
    $p_2 = \langle 1,3,4,5\rangle$  & (4,6) \\ \hline
    $p_3 = \langle 1,2,3,5\rangle$  & (3,7) \\ \hline
    $p_4 = \langle 1,3,5\rangle$ & (2,8) \\ \hline
    %$p_5$: 1-5 & (3.8,6.8) \\ \hline
    \end{tabular}
\end{center}
\caption{Pareto-optimal paths from node 1 to node 2 in the graph of Fig.~\ref{fig:example_graph}}
\label{table:pareto_paths}
\end{table*}

We extend the example by adding an arc $(1,5)$ with weight vector $(3.8,6.8)$.
Note that now, in addition to the previous solutions, the path $p_5 = \langle 1,5 \rangle $ is a Pareto-optimal path.
However, we have 
\begin{align}
\forall j \in \{1,\dots,4\}: \sum_{i=1}^{d} w_i(p_j) < \sum_{i=1}^{d} w_i(p_5) \quad ,
\end{align}
i.e., $p_5$ is not on the convex hull an thus called an \emph{unsupported non-dominated solution}, as depicted in Fig. \ref{fig:unsupported_solution}.
% approximate solution set
Thus, an \emph{approximate solution set} provides a ``reasonable'' set of non-dominated paths, but is not necessarily complete.

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[xlabel=$w_1$, ylabel=$w_2$, nodes near coords, enlargelimits=0.2]
	\addplot[color=blue, mark=*, point meta=explicit symbolic] 
	coordinates {
		(5,5) [$p_1$]
		(4,6) [$p_2$]
		(3,7) [$p_3$]
		(2,8) [$p_4$]
	};
	\addplot[color=red, mark=*, point meta=explicit symbolic] 
	coordinates {
		(3.8,6.8) [$p_5$]
	};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{An unsupported non-dominated solution: $p_5$}
\label{fig:unsupported_solution}
\end{figure}

\begin{comment}
\subsubsection*{Mutligraphs} \label{sec:shortest_path:defs:multigraph}
A multigraph is a graph that may contain parallel edges, that is, multiple edges may connect the same two nodes. 
Formally, a directed multigraph $G=(V,A)$ consists of a finite set of nodes $V = \{v_1, v_2,\dots,v_n \}$ and a finite multiset\footnote{In contrast to a set, a multiset may contain the same element multiple times.} of arcs and a function $f$, 
\begin{align}
f: A &\rightarrow (u,v) \text{ where } u,v \in V \quad .
\end{align}

The definition of weighted multigraphs is analogous to the definition of weighted graphs given in Eq.~\ref{eq:msp_weighted}. 

% our algorithm works on multigraphs too!
\todo{This should go somewhere else - it certainly does not belong to the formal definitions}
We note that in contrast to the algorithms mentioned in Section \ref{sec:related}, our algorithm works on multigraphs as well and provide a comparative performance evaluation in Section.
\end{comment}


\section{A Pheet-based MSP algorithm} \label{sec:msp_algo}
% what we do in this section
In this section, we present our algorithm for the MSP.
% Problem setting
We want to compute an exact solution of the MSP for directed graphs with multi-dimensional weight vectors (as defined in Section \ref{sec:shortest_path:defs:msp}). 
% Intro to the algorithm: generalization of Dijkstra
The algorithm follows the principles of Dijkstra's algorithm: we generate a set of candidates and expand the most promising first. The problem of finding intermediate paths that are Pareto-optimal is delegated to a priority queue. 
% we have a set of solutions for each node
Note that due to the multi-dimensional weight vectors, we have a set of optima for each node (see Section \ref{sec:pareto_optima}.
% The algorithm is label-correcting (no strict sequence)
In contrast to Dijkstra's algorithm, ours is a \emph{label correcting} algorithm.
The weight vectors are only partially ordered and thus there can be multiple Pareto-optimal candidates that may be expanded in arbitrary order. 
However, even though non of these candidates dominates any of the others, the partial solutions created by expanding them might do so.

% Pareto set
Each node $v$ maintains a \emph{Pareto set} to store the Pareto-optimal paths to the node $v$ that have been found so far. 
We denote the set of all Pareto sets by $S_p$ and a specific Pareto set attached to a node $v$ by $S_p[v]$.

% Pseudo code
A high level description of the algorithm is shown in Algorithm \ref{algo:abstractMSP}; note its similarity to Dijkstra's algorithm.
% weight vector <-> priority vector
As with Dijkstra's algorithm, paths with minimal weight should be expanded first, i.e., paths with minimal weight have the highest priority. 
The procedure for expanding a candidate path is described in detail in Algorithm \ref{algo:abstractExpand}. 

% Abstract description of the algorithm
\begin{algorithm}
\caption{Pseudo-code for our MSP algorithm}
\label{algo:abstractMSP}
\begin{algorithmic}[1]
\Require{Graph $G$, start node $s$}
\ForAll{$v \in G$}
  \State $S_p[v] \gets \{\}$ \Comment{Initialize the Pareto-set for each node}
\EndFor
\State $S_p[s] \gets \{ \vect{0} \} $ \Comment{Shortest path from $s$ to $s$ is the null-vector}
\State $U \gets \{\langle s \rangle\}$ \Comment{$U$ is a priority queue containing paths that need to be expanded further}
\While{$U$ is not empty}
  \State Take a highest-priority path $p$ from $U$ 
  \State \Call{expand}{$p$}
\EndWhile
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Pseudo-code for expanding a candidate path}
\label{algo:abstractExpand}
\begin{algorithmic}[1]
\Require{$S_p[v]$: Set of Pareto-optimal paths to node $v$ ($\forall v\in V$)}
\Require{$U$: Set of paths that need to be explored further}
\Require{$p=\langle s, \dots, v \rangle$ is a path in Graph $G$ from start node $s$ to $v$}
\Function{expand}{Path $p$}
\ForAll{$w \in p$.head.neighbors} 
  \State $p' \gets \cP{p}{w}$	\Comment{Generate candidate}
  \If{$\nexists x \in S_p[w] \text{ s.t.\ } x \dom p'$} 
  \Statex \Comment{Candidate is not dominated by any path in the Pareto-set}
    \ForAll{$y\in S_p[w]$} \Comment{Remove any path dominated by $p'$}
      \If{$p' \dom y$}
	\State $S_p[w] \gets S_p[w] \setminus y$
      \EndIf
    \EndFor
    \State $S_p[w] \gets S_p[w] \cup p'$ \Comment{Add $p'$ to the Pareto-set for node $w$}
    \State $U \gets U \cup p'$ \Comment{$p'$ needs to be explored further}
  \EndIf
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Implementation} \label{sec:msp_algo:implementation}
% Connection to Pheet
The implementation relies on the Pheet task scheduling framework for parallelization and, in particular, on Pheet's concept of \emph{priority task scheduling} (Section \ref{sec:pheet:task_priorites}), which allows to combine the work pool of the scheduler with the priority queue used by the algorithm into one data structure: a priority-aware task scheduler.

% Each path to expand is a task
The algorithm can be parallelized in an intuitive way by wrapping each candidate path into a task. 
Note that expanding several paths can be done independently, except for updates to the Pareto-set of a given node.



% Works with different schedulers/task storages
We designed the implementation s.t.~it may easily employ different task scheduler implementations without any changes to the algorithm itself. 
Within this framework, we only have to provide implementations of 
\begin{inparaenum}[(i)]
\item the task and 
\item a scheduling strategy.
\end{inparaenum}

% Pareto set
For the implementation, we extend the Pareto-set by the \verb|insert| operation to provide some of the functionality required in the procedure for expanding a path (Algorithm \ref{algo:abstractExpand}, in particular lines 4 - 10).
In particular, \verb|insert|, given a path $p = \langle s,\dots,v \rangle$, removes all paths at node $v$ which are dominated by $p$; if however, $p$ is dominated by some path $p'$ in the Pareto-set for node $v$, $p$ is not added to the Pareto-set but is instead marked as dominated itself. 
The operation returns true if and only if $p$ was actually added to the Pareto-set.
Note that for any path $p$, only one of the following two alternatives may occur:
\begin{itemize}
\item $p$ is added to the Pareto-set and some paths $p'$ that are dominated by $p$ are marked dominated and removed from the set.
\item $p$ itself is dominated by some path $p'$ from the Pareto-set and is thus marked dominated and not added to the set.
\end{itemize} 
In other words, it cannot be the case that $p$ is marked dominated, but another path $p'$ already in the Pareto-set was removed from it because it is dominated by $p$. 
To see this, assume that the Pareto-set contains two paths $p_1$ and $p_2$, where $p_1$ is dominated by $p$ and $p_2$ dominates $p$. 
The \verb|insert| operation would first remove $p_1$ since $p \less p_1$ and then mark $p$ dominated since $p_2 \less p$. 
However, due to transitivity of the dominance relation, this implies $p_2 \less p_1$. 


% Pareto set implementation
A naive implementation only needs to scan the Pareto-set either until it finds a path $p'$ that dominates $p$ or and remove all paths dominated by $p$ 


% Explain the pareto set: insert takes a single path instead of a whole set to simplify the explanation somewhat.
\begin{code}[label=lst:ParetoSet, caption=Pareto set operations.]
class ParetoSet {
  /* Insert p into the Pareto-set. Any path p' that is dominated by p is marked dominated and removed from the set. If p is dominated by a path p' already in the set, p is be marked dominated and is not inserted into the set. Returns true if p was inserted into the Pareto-set. */
  bool insert(Path p);
}
\end{code}


% Msp task
%\begin{minipage}[c]{\linewidth}
%\begin{algorithm}
%\label{lst:MSPTask}
\begin{code}[caption=MSP Task, label=MSP Task]
class MSPTask : SchedulerTask {
public:

  MSPTask(Path p, ParetoSets Sp) //Constructor. Save parameters.
      : p(p), Sp(Sp) { }
  
  void execute() {
    /* The path this task was spawned for might be obsolete. */
    if(p.dominated) return;	
    Set<Path> candidates;
    /* Generate new candidates */
    for (Arc a : p.head().outgoing_edges()) {
      Path q = p.step(e);
      candidates.insert(q);
    }
    /* Add the candidates to the global pareto sets */
    Set<Path> added;
    added = Sp.insert(candidates);
    for (Path p : added) {
      if(!p.dominated()) {
	/* Spawn a new task for each added path */
	spawn_task(p, Sp);
      }
    }
  }
private:
  Path p;
  ParetoSets Sp;
}
\end{code}
%\end{algorithm}

\begin{comment}
% Msp task
\begin{algorithm}
\caption{MSP Task}
\label{algo:msptask}
\begin{algorithmic}[1]
   \Function{initialize}{Graph $g$, Path $p$, Pareto\_Sets $s$} 
      \State $\sC \gets \{\}$ \Comment{Set of candidates a task generates}
      \State $\dots$  \Comment{Store $g$, $p$ and $s$ for this instance of Task}
   \EndFunction
  \Statex
  \Function{execute}{}
    \If{$p$ is dominated} 
      \State return	\Comment{This task instance is obsolete}
    \EndIf
    \State Node $h \gets p$.head	\Comment{The last node of path $p$}
    \ForAll{$v \in h$.neighbors} \Comment{All neighbors of $h$}
      \State $\sC = \sC \cup \cP{p}{v}$ \Comment{Generate candidates}
    \EndFor
    \State $\sA \gets s$.insert(\sC) \Comment{Inserts the generated candidates into global Pareto set}
    \Statex \Comment{$\sA$ contains candidates not dominated by any path in $s$}
    \ForAll{$p\in \sA$}
      spawn\_task($g$, $p$, $s$) \Comment{Spawn a new task for each candidate}
    \EndFor
  \EndFunction
\end{algorithmic}
\end{algorithm}
\end{comment}

The following implementation variants are provided:
\begin{itemize}
% \item \verb|SequentialMsp|:
% \item \verb|StrategyMsp|:
\item \verb|Strategy2MspLSM|: \todo{we need to introduce the lsm somewhere}
% \item \verb|Strategy2MspKLSM|:
\item \verb|Strategy2MspPareto|:
\end{itemize}

% sequential algorithm: uses a priority queue

% StrategyScheduler: 

% Strategy2Scheduler, variants:

\subsection{Analysis} \label{sec:msp_algo:analysis}



\section{Performance evaluation} \label{sec:evaluation}

% Mars hardware
For performance evaluation, we used a shared memory system nicknamed \emph{Mars}, an Intel Xeon based system with the properties listed in Table \ref{table:mars}.
\begin{table*}
\begin{center}
    \begin{tabular}{| l | l |}
    \hline
    CPU model & Intel Xeon E7-8850 \\ \hline
    Number of cores & 80 (8 nodes with 10 cores each) \\ \hline
    CPU clock & 2.00 GHz \\ \hline
    L1i & 32 KB \\ \hline
    L1d & 32 KB \\ \hline
    L2 & 256 KB \\ \hline
    L3 & 24576 KB \\ \hline
    Main memory & 1 TB \\ \hline    
    \end{tabular}
    \caption{Hardware configuration of Mars}
    \label{table:mars}
\end{center}
\end{table*}
% compiler 
For all experiments, Pheet was compiled using \verb|clang 3.4.2-7| with the the \verb|-O3| flag to allow standard compiler optimizations.

\subsection{Test instances} \label{sec:eval:test_instances}
% test graphs
All input instances were created by \verb|PHEET_HOME/test/msp/lib/Graph/Generator/main.h|, which generates a random connected digraph with the following arguments:
\begin{compactitem}
\item \verb|n|: Number of nodes.
\item \verb|m|: Number of edges.
\item \verb|d|: Degree of the weight vectors.
\item \verb|w|: Upper limit for all dimensions of the weight vectors.
\item \verb|r|: Random seed value.
\end{compactitem}
If the option \verb|-p| is given, a multigraph will be generated. 
Otherwise, the generated graph will contain no parallel edges.
\subsection{Methodology} \label{sec:eval:methodology}
% number of runs
% confidence intervals





\section{Conclusion} \label{sec:conclusion}
\todo{...}
\section{Acknowledgements} \label{sec:ack}
\todo{...}


%bibliography
\bibliographystyle{acm}
\bibliography{sources} 



%tmp plots
\section{Plots}
\subsection{28.08.}

\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/base/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/1/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/2/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/3/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/4/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Improvements on 28-08}
\end{figure}

\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/strategy.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2lsm.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2klsm.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{s2pareto vs.\ s2lsm vs.\ s2klsm vs.\ strategy}
\end{figure}

\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/strategy.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2lsm.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/KDSet/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/KDSet/strategy.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/KDSet/s2lsm.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Each strategy/task storage with NaiveSet and KDSet}
\end{figure}

\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/lincomb/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{s2pareto with lincomb and pareto strategy}
\end{figure}


\subsection{29.08.}
\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_29/1/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Set path dominated in NaiveSet if necessary}
\end{figure}

\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/mars/g_2000_13000_3_10000_42/08_29/s2pareto.dat",
"../benchmarks/mars/g_2000_13000_3_10000_42/08_29/s2lsm.dat",
"../benchmarks/mars/g_2000_13000_3_10000_42/08_29/s2klsm.dat",
"../benchmarks/mars/g_2000_13000_3_10000_42/08_29/strategy.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Different strategies/task storages on mars}
\end{figure}

\subsection{08.09.}
\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/09_08/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_08/before/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Relaxed memory order for all virtual array accesses}
\end{figure}

\subsection{12.09.}
\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/09_12/before/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_12/after/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_12/merging/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Improvements in PLTSBlock::put}
\end{figure}

\subsection{18.09.}
\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/09_18/before/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_18/after/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Improvements and fix in PLTSBlock::pop}
\end{figure}

\subsection{18.09.}
\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/09_23/before/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_23/after/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_23/dead/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_23/pq/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Improvements and clean up (partition pointers and iterator generation}
\end{figure}

\subsection{23.09.}
\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/09_23/pqbefore/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_23/pqafter/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Block-local pivot queues}
\end{figure}

\subsection{27.09.}
\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/09_27/before/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_27/iters/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_27/shrink/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Block-local pivot queues}
\end{figure}

\subsection{29.09.}
\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/09_29/before/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_29/after/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Block-local pivot queues}
\end{figure}

\subsection{Improvements since 01.08.}
\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/baseline/strategy2.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_29/after/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Current vs. base from 01.08.}
\end{figure}


\end{document}
