\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{cite}
%page boarders
\usepackage[vmargin=3cm, hmargin=3cm]{geometry}
\usepackage{float}
\floatstyle{plaintop}
\usepackage{paralist} %compactitem
\usepackage{fixltx2e} %some latex fixes
\usepackage{microtype} %Subliminal refinements towards typographical perfection
\setlength{\emergencystretch}{2em}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs} %Support use of the Raph Smithâ€™s Formal Script font in mathematics
\usepackage[sc]{mathpazo} %mathematical fonts
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage[margin=10pt, font=small, labelfont=bf]{caption}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{color}
\usepackage{xcolor}
\floatstyle{plaintop}
\usepackage{float}
\restylefloat{table}
\usepackage {tikz}
\usetikzlibrary{calc,shapes.multipart,chains,arrows, positioning,shapes}
\usepackage{pgfplots}
\usepackage{fancyvrb}
\usepackage{subfigure}
\usepackage{etoolbox}\AtBeginEnvironment{algorithmic}{\small}

% a nice not implies
\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}
  
% todos  
\newcommand\todo[1]{\textcolor{red}{TODO: #1}}

% number equations s.t. they can be referenced easily
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% within an align* environment, number and label the equation. #1 is the label text to be appended to eqn:
\newcommand\neqn[1]{\numberthis\label{eq:#1}}

% theorem, lemma, ...
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}

% math stuff
% vectors
\newcommand{\vect}[1]{\vec{#1}}

% less relation
\newcommand{\less}{\ensuremath{\prec}}
\newcommand{\lesseq}{\ensuremath{\preceq}}
\newcommand{\moreeq}{\ensuremath{\succeq}}
\newcommand{\more}{\ensuremath{\succ}}

% total order relation
\newcommand{\tor}{\ensuremath{\leq_R}}
\newcommand{\osum}{\ensuremath{\leq_{\text{sum}}}}
\newcommand{\olex}{\ensuremath{\leq_{\text{lex}}}}

% sets
\newcommand{\sC}{\ensuremath{S_{\text{c}}}}
\newcommand{\sA}{\ensuremath{S_{\text{a}}}}
\newcommand{\sR}{\ensuremath{S_{\text{r}}}}

% concatenation of paths
\newcommand{\cP}[2]{\ensuremath{#1;#2}}

% column vectors
\makeatletter
\newcommand{\Spvek}[2][r]{%
  \gdef\@VORNE{1}
  \left(\hskip-\arraycolsep%
    \begin{array}{#1}\vekSp@lten{#2}\end{array}%
  \hskip-\arraycolsep\right)}

\def\vekSp@lten#1{\xvekSp@lten#1;vekL@stLine;}
\def\vekL@stLine{vekL@stLine}
\def\xvekSp@lten#1;{\def\temp{#1}%
  \ifx\temp\vekL@stLine
  \else
    \ifnum\@VORNE=1\gdef\@VORNE{0}
    \else\@arraycr\fi%
    #1%
    \expandafter\xvekSp@lten
  \fi}
\makeatother

% style of listings
\definecolor{Gray}{gray}{0.5}
\definecolor{OliveGreen}{cmyk}{0.64,0,0.95,0.40}

   \lstset{
      language=C++,
      basicstyle=\ttfamily\footnotesize,
      keywordstyle=\color{blue},
      commentstyle=\color{OliveGreen},
      breaklines=true,
      breakatwhitespace=false,
      showspaces=false,
      showtabs=false,
      numbers=left,
      frame=single,
      captionpos=t,
  }

\lstnewenvironment{code}[1][]%
{
   \noindent
   \minipage{\linewidth} 
   \vspace{0.5\baselineskip}
   \lstset{
      #1
  }
  }
{\endminipage}

% \texttt in function names
\algrenewcommand\textproc{\texttt}


% 
\title{A multi-criteria priority queue for the Pheet task scheduling framework\\
\normalsize Technical report for \\``Project in Software Engineering \& Internet Computing''
}
\author{Martin Kalany, 0825673\\
Vienna University of Technology}

\begin{document}
\maketitle

%scale factor for graphs
\setkeys{Gin}{width=0.5\textwidth}

%R functions for plot generation
<<echo=FALSE, include=FALSE>>=
\SweaveInput{functions.Rnw}
@

\VerbatimFootnotes

%\pagebreak	

\begin{abstract}
\noindent
% data structure
A priority queue is an abstract data structure that stores a set of keys. 
% efficient access
It is a crucial component for numerous applications which require efficient access to the item with the minimal (or highest-priority) key, e.g., 
\begin{inparaenum}
\item the classical single-source shortest path algorithm by Dijkstra or 
\item schedulers for multi-processor systems.
\end{inparaenum}
% scalar values
Typically, those keys are scalar values and inserting a key (the \texttt{push} operation) as well as finding and deleting the minimal key (\texttt{pop}) can be done in sub-linear time.

% whats new
We present a new priority queue, called the \emph{Pareto priority queue (PPQ)}, which manages keys of a more general form. In particular, keys are vectors from some domain $V$ and both the \texttt{push} and \texttt{pop} operation have amortized time complexity $O(\log^2 n)$. 

% tie in with pheet
We use the PPQ to provide Pheet, a task scheduling framework for shared memory systems, with the means to schedule tasks with multi-dimensional priorities. 
We provide an example implementation of the multi-criteria shortest path problem based on Pheet to evaluate the PPQ's performance compared to existing task schedulers. 


\end{abstract}

\pagebreak
\tableofcontents
\pagebreak

\section{Introduction} \label{sec:intro}
% explain the idea of this work in a couple of sentences. This is for the informed reader.

% Introduce pheet
Pheet\footnote{\url{www.pheet.org}} is an open-source task scheduling framework for shared memory systems   
and is based on the task-parallel programming model, which allows a programmer to explicitly expose the parallelism of an application.
% The build-up for task priorities
Runtime systems based on the task-parallel model typically impose a non-adaptive, application independent execution order on the tasks where the scheduler is unaware of the preferred execution order (the tasks are processed in, e.g., LIFO-order).
% The problem with Dijkstra's algorithm
While it has proven to be an efficient strategy for parallelization \cite{AroraBP01}, this approach has not been particularly useful for the parallelization of algorithms relying on priority queues such as Dijkstra's famous algorithm for the single-source shortest path problem (SSSP) \cite{Dijkstra59}, since two work pools have to be maintained: 
One is used by the scheduling framework to keep track of the tasks that are yet to be executed; the other is required by the algorithm itself to determine the execution order of the tasks.

% Priority task scheduling
\emph{Priority task scheduling} (Wimmer et al.~\cite{WVTCT13}), addresses this problem by making the preferred execution order of tasks known to the scheduler. 
The task scheduler utilizes a concurrent priority queue which fulfills the ordering requirements imposed by the algorithm by assigning a comparison-based priority value to each tasks.
Wimmer \cite{Wimmer14} further states that a scheduler based on the task priority scheduling model allows for an efficient parallel implementation of any algorithm relying on priority queues.
Their claim is backed up a parallel implementation of Dijkstra's algorithm with the help of a scheduler based on the priority task scheduling model.

% Pareto priorities
They suggest to generalize the idea of priority task scheduling to multi-dimensional (or \emph{Pareto-}) priorities (as defined in Section \ref{sec:pareto_optima}), which requires an efficient Pareto priority queue (PPQ) implementation to be added to Pheet. 
A PPQ ensures that the next task to be executed is a Pareto optimum, i.e., a task for which the partial solution is not dominated by the partial solution of any other task.
It allows for an efficient parallel implementation of e.g., algorithms solving the multi-criteria shortest path problem (MSP)\footnote{Also called  multi-objective shortest path problem, multi-objective optimization or multi-objective search.} as discussed in \cite{Martins84}.
A different advantage of using multi-dimensional priorities for task scheduling, which is independent of the algorithm that utilizes the scheduler, is that multi-dimensional priorities establish only a partial ordering on the tasks, which gives the scheduler more flexibility in terms of which task to execute next. 

% Our work
In this work, we investigate a potential implementation of such a PPQ for the Pheet task scheduling framework, which is then used in turn for a parallel algorithm solving the MSP. 
% MSP is not our focus
\begin{comment}
\footnote{We note here that the goal of our work is not to provide a better algorithm for the MSP; we investigate the potential of a PPQ and use the MSP only as an example application and as a benchmark.}
\end{comment}
% first step: use linear combination
An intuitive solution for the MSP is to apply the $L^1$ to reduce the multi-dimensional priorities to scalar values, i.e., by defining a total ordering on the priority vector's domain.   
However, due to the general nature of a task's priority, such a total order relation does not necessarily exist, and the PPQ has to make use of different techniques to determine a highest-priority task.

%performance eval
For performance evaluation, we provide a Pheet-based algorithm solving the MSP, which, due to Pheet's highly generic nature, may employ different priority queues to schedule tasks.
One of them is our PPQ which directly considers the multi-dimensional priorities of the tasks, while the other is Pheet's LSM priority queue, which relies on the $L^1$ norm to schedule tasks.

% Results
A comparison of the performance of the MSP algorithm using the PPQ and LSM priority queues shows that they scale equally well; however, the PPQ variant is slower by roughly a factor of 2 when using integer priority vectors, which can be considered a reasonable trade-off for being able to schedule tasks with general multi-dimensional priorities. 

\subsubsection*{Outline} \label{sec:intro:outline}
We start with discussing related work in Section \ref{sec:related}, followed by a formal definition of Pareto-optimality (Section \ref{sec:pareto_optima}), and a high-level introduction of Pheet-related concepts that are relevant for this work (Section \ref{sec:pheet}).
A detailed presentation and analysis of our PPQ is given in Section \ref{sec:ppq}).
We give a formal definition of the MSP, which is used as an example application, in Section \ref{sec:shortest_path}, where we also discuss the hardness of the problem.
A detailed description of a Pheet-based algorithm solving the MSP will be given in Section \ref{sec:msp_algo}. 
This implementation is then used to compare the performance of our PPQ to another priority queue implementation already available in Pheet (Section \ref{sec:evaluation}).

\section{Related work} \label{sec:related}
%--------------------------------------------------------------------------------------------
%Should related work be covered near the beginning of the paper or near the end?
%   - Beginning, if it can be short yet detailed enough, or if it's critical to take a strong defensive stance about previous work right away. In this case Related Work can be either a subsection at the end of the Introduction, or its own Section 2.
%   	- End, if it can be summarized quickly early on (in the Introduction or Preliminaries), or if sufficient comparisons require the technical content of the paper. In this case Related Work should appear just before the Conclusions, possibly in a more general section "Discussion and Related Work".
%--------------------------------------------------------------------------------------------

% pareto optima computation
Computing teh maxima of a set of vectors (as discussed in \cite{KungLP75}) is a well understood problem that can be solved in linear expected time \cite{BentleyCL93}. 

% Ehrgott, Gandibleux: 
For an overview of related work for the MSP without parallelization, we refer to the annotated bibliography of multi-objective combinatorial optimization \cite{EhrgottG02}, specifically Section ``6.1 Shortest path problems''.
% Sonnier: approximate solutions for d \in {3,4}
Sonnier \cite{Sonnier06} were one of the first to publish parallel algorithms for solving MSP problems with 3 or 4 objectives. 
Their algorithms are based on a weighted sum approach and thus provide only an approximate solution, i.e., the solution set may not contain all Pareto-optimal paths.

% Sanders, Erb: first parallel MSP algo, but mostly for d=2
To the best of our knowledge, Sanders and Mandow \cite{SM13} published the first proposal for a parallel algorithm that provides an exact solution of the MSP.
Their approach extends Dijkstra's classical algorithm \cite{Dijkstra59} and relies on a so called \emph{Pareto queue}, a multi-dimensional generalization of a priority queue. 
While they give a high level description of the algorithm for arbitrary $d \geq 2$ and a detailed description of the bi-criteria case (which was further engineered by Erb \cite{Erb13}), they also state that efficient priority queues for $d\geq3$ are not yet known.

%task based priority scheduling, Wimmer
Wimmer et al.\ \cite{WVTCT13, WimmerKLSM} present several different priority queues for task-based priority scheduling and our work is based in parts on their $k$-LSM priority queue and dissertation \cite{Wimmer14}.


\section{Pareto optima} \label{sec:pareto_optima}
% Informally describe the problem
Given a set of vectors, the \emph{maximal vector problem} is to find the subset of vectors s.t.\ each vector of the subset is not dominated by any vector of the set. 
One vector dominates another if each of its components is equal to or smaller than (w.r.t.\ some total order on the respective domain) the corresponding component of the other vector, and strictly smaller in at least one component.
Such a maximal element is called \emph{Pareto-optimal} and the set of maxima is called the \emph{Pareto-set}. 
% Example
Fig.\ \ref{fig:pareto_optima} illustrates the concept of Pareto-optimality and a formal definition is provided in the remainder of this section.

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[xlabel=$w_1$, ylabel=$w_2$, enlargelimits=0.2, nodes near coords]
\addplot[mark=*, point meta=explicit symbolic, color=blue]
  table {
x	y
1	7.8
1.5	7.2
1.6	7
2.2	6.5
2.2	6
2.9	5.5
3.8	4.5
4	4.4
4.7	4.3
5	4
};

\addplot[mark=*, only marks, point meta=explicit symbolic, color=black]
  table {
x	y
1.3	7.8
2.4	7
2.8	6
3.2	7.1
4	5
4	7.7
4.3	6
4.6	7.4
5	5


};

\addplot[mark=*, point meta=explicit symbolic, color=blue] 
coordinates {
	(2.9, 5.5) [A]
};
\draw[blue,dashed] (axis cs:2.9,5.5) -- (axis cs:0,5.5);
\draw[blue,dashed] (axis cs:2.9,5.5) -- (axis cs:2.9,0);

\addplot[mark=*, point meta=explicit symbolic, color=blue] 
coordinates {
	(3.8, 4.5) [B]
};
\draw[blue,dashed] (axis cs:3.8, 4.5) -- (axis cs:0,4.5);
\draw[blue,dashed] (axis cs:3.8, 4.5) -- (axis cs:3.8,0);

\addplot[mark=*, point meta=explicit symbolic, color=black] 
coordinates {
	(3.2, 6.2) [C]
};
\draw[black,dashed] (axis cs:3.2, 6.2) -- (axis cs:0,6.2);
\draw[black,dashed] (axis cs:3.2, 6.2) -- (axis cs:3.2,0);

\addplot[mark=*, point meta=explicit symbolic, color=black] 
coordinates {
	(3.5, 5.2) [D]
};
\draw[black,dashed] (axis cs:3.4, 5.2) -- (axis cs:0,5.2);
\draw[black,dashed] (axis cs:3.4, 5.2) -- (axis cs:3.4,0);

\end{axis}
\end{tikzpicture}
\end{center}
\caption{Example of a Pareto-set in two-dimensional space: The set of Pareto-optimal vectors are in blue, others in black. Note that point C is dominated by A in both dimensions ($w_1(A) < w_1(C)$ and $w_2(A) < w_2(C)$). D is dominated by A in dimension 1 and by B in dimension 2; either condition on its own suffices for D not being Pareto-optimal.}
\label{fig:pareto_optima}
\end{figure}

\subsubsection*{Formal definition}
% Formal problem definition
Let $U_1, U_2,\dots , U_d$ be totally ordered sets, each with the binary relations $\less_i$  and $=_i$, (e.g., $\mathbb{N}$ and the ``smaller than'' and ``equals'' relations) and let $V$ be a set of $d$-dimensional vectors in $U_1 \times U_2 \times \dots \times U_d$.
For any $\vect{v} \in V$, we donate the $i$-th component of $\vect{v}$ by $w_i(\vect{v})$.

% Domination relation
Let $\vect{x}, \vect{y} \in V$. 
We say that $\vect{x}$ \emph{dominates} $\vect{y}$ ($\vect{x} \less \vect{y}$) -- or equivalently, $\vect{y}$ \emph{is dominated by} $\vect{x}$ -- if and only if
\begin{align}
&\forall i \in \{1,\dots, d\}: w_i(x) \lesseq_i w_i(y) \quad \text{and}\\
&\exists j \in \{1,\dots, d\}: w_j(x) \less_j w_j(y) \quad .
\end{align}
Note that $\vect{x} \less \vect{y} $ implies $\vect{x} \neq \vect{y}$ and that the binary relation $\moreeq_i$ can easily be defined via the other two relations.
For $d\geq 2$, the dominance relation ($\less$) defines a partial order on $V$, but not a total order.
Thus, there exist vectors $\vect{x}, \vect{y} \in V$, $\vect{x} \neq \vect{y}$ for which neither $\vect{x} \less \vect{y}$ nor $\vect{y} \less \vect{x}$ holds.
% transitivity
The $\less$ relation is transitive, while $\not \less$ is not.
To see this, let $\vect{x} = \Spvek{1;2}$, $\vect{y} = \Spvek{4;1}$ and $\vect{z} = \Spvek{3;3}$ and let $\less_i$ and $=_i$ be the usual $<$ and $=$ relations on $\mathbb{N}$.
Note that $\vect{x} \not \less \vect{y}$ and $\vect{y} \not \less \vect{z}$, but $\vect{x} \less \vect{z}$.

% Pareto optima
\bigskip
A vector $\vect{x} \in V$ is a \emph{Pareto-optimum} if and only if
\begin{align}
\nexists \vect{v} \in V: \vect{v} \less \vect{x} \quad ,
\end{align}
i.e., there is no vector $\vect{v}$ that dominates $\vect{x}$.
%Pareto set
A \emph{Pareto-set} $P \subseteq V$ is the set of all Pareto-optima of $V$.

\section{Pheet} \label{sec:pheet}
% what we will do in this section
In this section, we provide a brief introduction to Pheet-specific concepts that are relevant for this technical report. 
% reference Martin's dissertation
A detailed introduction to Pheet, the concepts it is built on and the research based on it can be found in the dissertation of Martin Wimmer \cite{Wimmer14}. 

\subsubsection*{Philosophy} \label{sec:pheet:philosophy}
% How is pheet implemented, what are its goals?
Pheet is a task-parallel programming library with the goal of providing a simple-to-use framework for the quick parallelization of algorithms.
Its flexible plug-in architecture based on C++ template meta-programming allows for any component in the task scheduling system to be replaced by an alternative implementation.
Additionally, Pheet provides a set of micro-benchmarks (that aim to evaluate specific aspects of the scheduling framework) and fine grained performance counters (that provide detailed insight into the scheduler and supporting data structures), which makes the framework a suitable platform for the implementation and testing of new components such as schedulers or priority queues. 

%\subsection{Concepts} \label{sec:pheet:concepts}

\subsubsection*{Places} \label{sec:pheet:places}
A \emph{place} denotes a single worker thread in the Pheet scheduling system that is pinned to specific processor, that is, a place will not migrate to another processing unit during execution.
Each processor utilized by Pheet is assigned exactly one place, implying that a processor is uniquely identified by it, which may be very useful for the implementation of parallel algorithms.
Furthermore, a place allows to implement processor-local data structures, e.g., parts of a distributed priority queue or application specific data structures and data accessible by the tasks that are executed on the associated processor.

\subsubsection*{Task parallelism} \label{sec:pheet:task_parallel}
% work pool
Pheet is based on the \emph{task-parallel model}, where a task is a small portion of work that is to be executed sequentially.
As soon as a processor is ready to execute work, it retrieves a task from the \emph{work pool}, which contains all the tasks that have yet to be executed.
A task may itself create additional tasks that are then stored in the work pool.
A famous algorithm based on the work pool pattern is Dijkstra's algorithm \cite{Dijkstra59} for the shortest path problem:
The algorithm requires a priority queue (that may be implemented as e.g., a simple binary search tree or a Fibonacci heap). This priority queue is a specialized work pool.
Note that in a task parallel program, the work pool is not necessarily a centralized data structure; each place may maintain its own. 

\subsubsection*{Task priorities and strategy scheduling}  \label{sec:pheet:task_priorites}
% What does the scheduler do?
The work pool exposes available work to the \emph{scheduler}, which is responsible for devising a schedule, i.e., a mapping of tasks to the processors.
% Global scheduling strategy
In standard task-parallel programming models, the scheduler typically employs a global policy that treats all tasks equally and thus independently of any task-specific properties.
% Task priorities 
Wimmer introduced the concept of \emph{task priorities} to make the scheduler aware of the preferred (relative) execution order of tasks. 
% Discrete priorities
One approach to handling priorities is to assign a discrete priority value (typically taken form a small set) to each task. 
% Comparison based prioritization
Another is a comparison based approach, where the programmer has to provide a comparator to the scheduler, that, given two tasks, decides which to execute first.

% Scheduling strategy
In Pheet, this comparator has to be implemented within the \emph{scheduling strategy} of a task.
A scheduling strategy is associated with a single task and may specify behavior that is depending on task specific criteria.
In contrast to a global scheduling policy, a scheduling strategy is associated with a single task and thus allows the programmer to influence the scheduler's behavior for a specific task.
% Dead tasks
An important application of this concept is the mechanism of \emph{dead tasks}, which allows to mark a spawned but not yet executed (or currently executing) task as obsolete. 
This mechanism is useful for speculative execution: A task may be spawned when it is likely that the work will of the task will have to be performed sometime in the future; when further calculations determine that the task need not be executed after all, the task can be marked dead via its associated strategy, thus instructing the scheduler not to execute the task.
Pheet handles dead tasks lazily, i.e., a dead task will not be dropped immediately but at a time it is convenient for the scheduler to do so, which allows for a more efficient implementation of the scheduler's data structures.


% Main part of the work
\section{A Pareto priority queue (PPQ)} \label{sec:ppq}
% What is the PPQ
The goal of the PPQ is to provide a concurrent priority queue that can be used by Pheet's strategy scheduler to determine the execution order of tasks with multi-dimensional priorities of the form described in Section \ref{sec:pareto_optima}.

% Priorities and partial solutions
The priority of a task reflects the potential quality of the (partial) solutions it will produce upon execution.
By using multi-dimensional priority vectors, the quality of a solution may be measured by several, possibly conflicting metrics\footnote{We refer to our algorithm for the MSP (definition see Section \ref{sec:shortest_path:defs:msp}; algorithm see Section \ref{sec:msp_algo}) for a concrete application of this concept.}. 
% Priority of task == desirability of execution
Obviously, it is desirable to execute the tasks that will produce (partial) solutions with the highest potential quality (i.e., tasks with highest priority) before other tasks.
% Pareto
Since each task is associated with a multi-dimensional priority vector, the tasks with highest priorities are the ones with a Pareto-optimal priority vector w.r.t.\ the set of priority vectors of tasks \emph{currently} managed by the queue.


% abstract definition
The PPQ is an \emph{ordered container} (as defined by Wimmer \cite{Wimmer14}), i.e., a bag-like data structure which orders tasks by their priorities.
% keys
As usual in the literature, we will denote the items managed by a priority queue as \emph{keys}.
In the context of this work, a key is a priority vector associated with a task and a \emph{maximal} key is one with Pareto-optimal priority.
The PPQ offers the following operations:
\begin{itemize}
\item \texttt{push} adds a key to the container.
\item \texttt{pop} returns a key that was previously added and removes it from the container. The order in which keys are returned is determined by their priority.
\end{itemize}
The order of keys is defined by the application and \emph{not by the priority queue} itself.
Given two keys $k_1$, $k_2$ from some domain $V$, we use the following relation symbols to state their order w.r.t.\ each other:
\begin{itemize}
\item $k_1 \less_i k_2$: The priority of $k_1$ in dimension $i$ is higher\footnote{We use the $\less$ symbol here to indicate that $k_1$ is ordered before $k_2$} than that of $k_2$ .
\item $k_1 \more_i k_2$:  The priority of $k_1$ in dimension $i$ is less than that of $k_2$.
\item $k_1 \lesseq_i k_2$: The priority of $k_1$ in dimension $i$ is higher than or equal to that of $k_2$.
\item $k_1 \moreeq_i k_2$: The priority of $k_1$ in dimension $i$ is less than or equal to that of $k_2$.
\item $k_1 \less k_2$: $k_1$ dominates $k_2$.
\item $k_1 \not \less k_2$: $k_1$ does not dominate $k_2$.
\end{itemize}
For the MSP, partial solution consists of a path $p = \langle s,\dots,n \rangle$ form start node $s$ to node $n$ and the sum of the weight vectors of all arcs on this path, called the \emph{weight} of the path or solution.
Usually, partial solutions with minimal weight are to be expanded first, i.e., they have the highest priority.

% why we can't use algorithms of e.g. BentleyCL93
A trivial solution of the problem is to compute the whole set of Pareto-optimal keys for every \texttt{pop} operation, which can be done in linear expected-time \cite{BentleyCL93}  and implies constant insertion time (i.e., \texttt{pop}) and linear expected-time for adding a key (i.e., \texttt{push}). 
However, \texttt{push} is required to compute but one Pareto-optimum and for efficient priority queues one usually aims to have sub-linear time complexity for both operations.
The PPQ thus employs different mechanisms, which are discussed in this work in detail.

\subsubsection*{Ordering requirements} \label{sec:ppq:ordering}
% ordering semantics  
The semantic definition of the \texttt{pop} operation requires that \emph{all} threads agree on the set of Pareto-optimal keys, implying that all \texttt{push} and \texttt{pop} operations executed by different threads need to appear to take affect in the same order for each thread, i.e., they need to be linearized w.r.t.\ all the threads.
While these \emph{global ordering} semantics match the intuitive expectation, they also pose a (potentially big) performance bottleneck. 
\emph{Local ordering} semantics typically occur when each thread maintains its own, thread-local priority queue.
A key returned by \texttt{pop} is required to be Pareto-optimal only w.r.t.\ the keys managed by the thread executing the operation; the priority queue of another thread might contain a strictly higher priority key.
% pro and cons			
With local ordering semantics, a thread might execute unnecessary additional work (since the solutions created by the task associated with the key will be of much lower quality than the ones created by a task with a dominating priority vector and thus will eventually be abandoned). 
However, this negative effect is countered by increased parallelism and less overhead due to synchronization.

\subsubsection*{Spying} \label{sec:ppq:spying}
% Spying
When a thread does not have any more keys (and thus tasks) in its local priority queue, it will try to \emph{spy} some from the priority queue of another thread. 
A thread spies keys from another priority queue by copying references to them to its own, thread-local priority queue.
Note that the keys are then shared by multiple threads.
Once a thread pops a key it informs all the other threads that the associated task is already being processed and thus should not be executed by other threads anymore. This is done by marking a key as \emph{taken}.
The operations of the spying thread need to be linearized with the operations of all the other threads on the same priority queue.

\subsection{Description and analysis} \label{sec:ppq:description}
% Purely local ordering semantics
The PPQ observes only local ordering semantics.
% A general idea
% Concurrent and local part
Each thread maintains its own priority queue which is split into a part that may be accessed by several threads concurrently and a second part -- built on top of the first -- that may be accessed by the owning thread only.
% Virtual array
For the concurrent part, we assume a (potentially) infinite, thread-safe array-like data structure, called the \emph{Virtual Array}. 
The Virtual Array is used to hold all the keys currently managed by the PPQ and allows other threads to scan it for keys with associated tasks that are awaiting execution.

% Thread-local part
The owner-exclusive part is based on the idea of \emph{log-structured merge trees} (Wimmer \cite{Wimmer14}, Section 5.7) and \emph{partitioning} and provides operations \texttt{push} and \texttt{pop} as defined above.
The Virtual Array of size $n$ is subdivided into an ordered sequence of a logarithmic number of arrays, called \texttt{blocks}.
Each block maintains a constant-size set of keys that are not dominated by any other key in that block
and provides the following two operations:
\begin{itemize}
\item \texttt{peek} finds a Pareto-optimal key w.r.t.\ the keys managed by the block\footnote{Note that \texttt{peek} on a block is similar to \texttt{pop} on the whole priority queue, except that it does not remove the found key from the data structure.}.
\item \texttt{take} removes a given key from the priority queue.
\end{itemize}
% Overview of pop
The \texttt{pop} operation performs a \texttt{peek} on each block to obtain a set of\footnote{All logarithms are to base 2.} $O(\log n)$ keys (one key from each block) that are Pareto-optimal w.r.t.\ to the keys in their respective blocks.
This set is then scanned linearly to find a key that is not dominated by any other key currently in the queue. 
We say that a key is deleted if it was returned by a \texttt{pop} operation or its associated task was marked as taken or dead. 

% Blocks are just logical; offset
It is important to note that a block is only a logical construct which provides access to and manages a range of keys stored in the Virtual Array. 
This range is identified by the offset $o$ of the block and its capacity.
Thus, any operation performed at position $i$ within a block is actually performed at position $o+i$ within the Virtual Array.

% Memory management: tricky, we use Pheet's capabilities.
Memory management for concurrent data structures is usually far from trivial, since a shared object may only be deleted if it can be guaranteed that the object is not accessed by any thread at the time of deletion.
Pheet provides a wait-free memory reuse scheme and we will not concern ourselves with memory management in this work  but make the following assumptions, justified by Lemmas 4.3.2 and 4.3.3 in \cite{Wimmer14}:

\begin{assumption} \label{ass:deleteIsConstant}
Deleting objects savely requires $O(1)$ time.
\end{assumption}

\begin{assumption} \label{ass:allocatedMemory}
When employing a memory manager to manage memory of size $n$, the size of the allocated memory is $O(n)$.  
\end{assumption}

% analysis only for sequential case
For the analysis, we will limit ourselves to the sequential case, i.e., 
\begin{assumption} \label{ass:sequential}
We assume that no keys are spied or marked deleted by another thread.
\end{assumption}

% outline
The concepts of log-structured merge-lists and partitioning are discussed in sections \ref{sec:ppq:description:lsm} and \ref{sec:ppq:description:partitioning}, respectively. 
Section \ref{sec:ppq:description:merging_blocks} shows how the logarithmic number of blocks is maintained.
Operations \texttt{peek} (Section \ref{sec:ppq:description:peek}), \texttt{take} (Section \ref{sec:ppq:description:take}), \texttt{push} (Section \ref{sec:ppq:description:push}) and \texttt{pop} (Section \ref{sec:ppq:description:pop}) as well as \texttt{spy} (Section \ref{sec:ppq:description:spying}) are discussed as soon as all relevant concepts and structures have been introduced.

\begin{comment}
\begin{figure}
\begin{center}
\begin{tikzpicture}[>=stealth']
\foreach \i in {0,...,12}{
   \node[minimum size=\itemWidth, rectangle, draw, yshift=2cm, xshift=\i*\itemWidth](va\i) {\tiny $k_{\i}$};
}
\node[anchor=south west] at (va0.north west) {\tiny Virtual Array};

\block{0}{8}{0}{\itemWidth*0};
\block{1}{4}{8}{\itemWidth*9};
\insertblock{1}{12}{\itemWidth*15};

 
\draw[dotted] (b0i0.north west) -- (va0.south west);
\draw[dotted] (b0i7.north east) -- (va7.south east);

\draw[dotted] (b1i0.north west) -- (va8.south west);
\draw[dotted] (b1i3.north east) -- (va11.south east);

\draw[dotted] (bai0.north west) -- (va12.south west);
\draw[dotted] (bai1.north east) -- (va12.south east);

\end{tikzpicture}
\end{center}
\caption{Overview of the PPQ structure: The Virtual Array provides an array-like data structure of (potentially) infinite capacity and can be accessed concurrently in a lock- and wait-free manner by multiple threads. A log-structured merge-list operating on the keys stored by the Virtual Array is used by the owning thread to find Pareto-optimal keys. }
\label{fig:structure}
\end{figure}
\end{comment}


% Virtual Array
\subsubsection{Virtual array} \label{sec:virtual_array}
% Motivation
The Virtual Array is a thread-safe array-like data structure with iterator-based access.
% concurrency
It may be traversed in forward direction by all threads, but only the owning thread may traverse it backwards or alter its structure. 
To ensure thread-safety, references to the keys\footnote{We call the elements of the array \emph{keys} because it is part of the PPQ and thus deals with the same entities.} in the array need to be stored as atomic pointers.
% Operations
It provides the following operations:
\begin{itemize}
\item \texttt{begin} obtains an iterator to the first key in the Virtual Array.
\item \texttt{end} obtains an iterator to the past-the-end\footnote{This is the theoretical key \emph{after} the last key in the array and thus the end-iterator does not point to any key.} key in the Virtual Array.
\item \texttt{remove(iterator l, iterator r)} removes the range $]l, r[$ from the Virtual Array and frees the keys stored there. 
\end{itemize}
An iterator provides the usual operations:
\begin{itemize}
\item \texttt{++} (\texttt{---}) moves the iterator to the next (previous) key.
\item \texttt{advance(m)} applies the \texttt{++} (if $m$ is negative: the \texttt{---}) operator $m$ times.
\end{itemize}

% Implementation
The Virtual Array is implemented via a doubly-linked list of arrays of constant size $s_\text{v}$, called \emph{ArrayBlocks}. 
% Analysis
The operations \texttt{begin} and \texttt{end} of the Virtual Array as well as the \texttt{++} and \texttt{---} operators of an iterator trivially require $O(1)$ time, while the \texttt{advance} operation has time complexity\footnote{In practice, this operation can be speed-up to $m/s_v$ time.} $O(m)$.
% Access time
% Deletion
To ensure thread safety, the \texttt{delete} operation only removes whole ArrayBlocks from the Virtual Array. 
Given iterators $l$ amd $r$, it removes the range $[l', r']$, where $l' = c_1 s_v$ is the minimal $l'$ s.t.\ $l < l' $ and $r' = c_2 s_v$ is the maximal $r'$ s.t.\ $r' < r $, for constants $c_1, c_2 > 0$.
Note that keys in the range $]l, l'[$ and $]r', r[$ are not removed, i.e., after the \texttt{remove} operation the range $]l, r[$ is not necessarily empty and may contain up to $2 s_v$ elements. 
It is the user's responsibility to always apply the operation with the largest possible range\footnote{I.e., removing the range $]l,r[$ cannot be done by using the \texttt{remove} operation twice with the ranges $]l, k[$, $]k-1, r[$ for some $k$, $l<k<r$.} and to include keys that have not been removed although they were in the range of a previous \texttt{remove} operation\footnote{Section \ref{sec:ppq:description:partitioning} shows how this can be done.}.
The \texttt{delete} operation trivially requires $O(1)$ time.

\begin{lemma} \label{lemma:virutalArrayMemory}
To store $n$ keys, the Virtual Array allocates $O(n)$ memory.
\end{lemma}
\begin{proof}
Assuming that the \texttt{remove} operation is applied as described above, the worst case is 1 non-deleted key per ArrayBlock. If an ArrayBlock does not contain any non-deleted items, the closest preceding non-deleted key is in a preceding ArrayBlock, while the closest succeeding key is in a succeeding ArrayBlock, implying that the ArrayBlock was deleted by a \texttt{remove} operation. 
Since ArrayBlocks are of constant size, the Virtual Array is of size at most $s_v n$.
\end{proof}

\subsubsection{Log-structured merge-lists (LSML)} \label{sec:ppq:description:lsm}
\newcommand{\itemWidth}{7mm}
% draw a block
% #1	id of each block element will be b\i#1
% #2	nr of block elements
% #3	Items will be numbered k_#3,...,k_#3+#2
% #4	xshift of first element

\newcommand{\block}[4]{
\pgfmathsetmacro\to{#2 - 1}
\foreach \i in {0,...,\to}{
    \pgfmathsetmacro\idx{\i + #3}
    \node[minimum size=\itemWidth, rectangle, draw, xshift=#4+(\i-1)*\itemWidth](b#1i\i) {\tiny $k_{\pgfmathprintnumber{\idx}}$};
}
\node[anchor=north west] at (b#1i0.south west) {\tiny Block $b_{#1}$};
}

\newcommand{\blockto}[5]{
\pgfmathsetmacro\to{#2 - 1}
\foreach \i in {0,...,\to}{
    \node[minimum size=\itemWidth, rectangle, draw, xshift=#4+(\i-1)*\itemWidth](b#1i\i) { };
}
\pgfmathsetmacro\tok{#5 - 1}
\foreach \i in {0,...,\tok}{
    \pgfmathsetmacro\idx{\i + #3}
    \node[minimum size=\itemWidth, rectangle, xshift=#4+(\i-1)*\itemWidth] {\tiny $k_{\pgfmathprintnumber{\idx}}$};
}
\node[anchor=north west] at (b#1i0.south west) {\tiny Block $b_{#1}$};
}


% #1 	number of keys, 0 <= #1 <= 2
% #2  	Items will be numbered k_#2,...,k_#2+#1
% #3 	xshift of first element
\newcommand{\insertblock}[3]{
\foreach \i in {0,...,1}{
    \node[minimum size=\itemWidth, rectangle, draw, xshift=#3+(\i-1)*\itemWidth](bai\i) {};
}
\ifnum #1 > 0
    \foreach \i in {1,...,#1}{
	\pgfmathsetmacro\idx{\i - 1 + #2}
	\node[minimum size=\itemWidth, rectangle, xshift=#3+(\i-2)*\itemWidth] {\tiny $k_{\pgfmathprintnumber{\idx}}$};
    }
\fi
\node[anchor=north west] at (bai0.south west) {\tiny Block $b_a$};
}


\begin{figure}
\begin{center}
\subfigure [Structure of an LSML containing 7 keys.] {
    \begin{tikzpicture}[>=stealth']
    \block{0}{4}{0}{\itemWidth*0};
    \block{1}{2}{4}{\itemWidth*5};
    \insertblock{1}{6}{\itemWidth*8};

    \path[->, bend left=20] (b0i3) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i3);
    \end{tikzpicture}
}

\subfigure [After adding an additional key, ArrayBlock $b_a$ is full.] {
    \begin{tikzpicture}[>=stealth']
    \block{0}{4}{0}{\itemWidth*0};
    \block{1}{2}{4}{\itemWidth*5};
    \insertblock{2}{6}{\itemWidth*8};
    
    \path[->, bend left=20] (b0i3) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i3);
    \end{tikzpicture}
}

\subfigure [After the merge of blocks $b_1$ and $b_a$. A new, empty ArrayBlock $b_a$ is added. ] {
    \begin{tikzpicture}[>=stealth']
    \block{0}{4}{0}{\itemWidth*0};
    \block{1}{4}{4}{\itemWidth*5};
    \insertblock{0}{8}{\itemWidth*10};
    \path[->, bend left=20] (b0i3) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i3);
    \end{tikzpicture}
}

\subfigure [After the merge of ArrayBlocks $b_0$ and $b_1$.] {
    \begin{tikzpicture}[>=stealth']
    \block{0}{8}{0}{\itemWidth*0};
    \insertblock{0}{8}{\itemWidth*9};
    \end{tikzpicture}
}
\end{center}
\caption{Inserting a key into an LSML.}
\label{fig:lsml_merge}
\end{figure}
% what is it; blocks; size
An LSML is a doubly-linked list of a logarithmic number of sorted arrays, called \emph{blocks}\footnote{Note that the blocks of the LSML are different from the ArrayBlocks of the Virtual Array.} . A block has a capacity of $c 2^l$, where $c$ is the capacity of the smallest block and $l$, $l\geq 0$ is the level of a block.
% nr keys in blocks, insert block b_0
A block of level $l$ contains exactly $c 2^l$ keys and at most one block of each level is allowed.
Given $n$ keys, $n' = c \lfloor \frac{n}{c} \rfloor $ keys are stored in such blocks, while the remaining $n-n'$ keys are stored in an additional block with capacity $c$, called $b_a$, which contains 0 to $c$ keys.  
% size of blocks
The levels of the blocks can easily be determined by the binary representation of $n'$:
The level of the $i$-th block is equal to the index of the $i$-th non-zero bit in the binary representation.
% ordering of blocks
The blocks $b_i$ are ordered s.t.\ a block $b_{i}$ is of a strictly greater level than its successor $b_{i+1}$.
Let $b_l$ be the last block in this sequence.
Block $b_a$ is assigned level $0$ and made the successor of $b_l$ only once it is full.
% Merging
In case block $b_l$ is of level $0$, blocks $b_l$ and $b_a$ are \emph{merged} into a single block of level 1.
To maintain the logarithmic number of blocks, this merging operation is applied recursively, that is, two blocks of the same level $l$ are always merged to a single block of level $l+1$.
Note that in an LSML, two blocks of same level or always next to each other. 
A new, empty block $b_a$ is created after the old one was merged with another block.
Fig.\ \ref{fig:lsml_merge} illustrates the structure of an LSML as well as the merging operation triggered by the insertion of a new key.

\begin{figure}
\begin{center}
\subfigure [Structure of an LSML after keys $k_5$ to $k_7$ have been deleted.] {
    \begin{tikzpicture}[>=stealth']
    \blockto{0}{8}{0}{\itemWidth*0}{5};
    \block{1}{4}{8}{\itemWidth*9};
    \insertblock{1}{12}{\itemWidth*14};

    \path[->, bend left=20] (b0i7) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i7);
    \end{tikzpicture}
}

\subfigure [After removing another key, block $b_0$ is shrunk\dots] {
    \begin{tikzpicture}[>=stealth']
    \blockto{0}{4}{0}{\itemWidth*0}{4};
    \block{1}{4}{8}{\itemWidth*5};
    \insertblock{1}{12}{\itemWidth*10};
    
    \path[->, bend left=20] (b0i3) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i3);
    \end{tikzpicture}
}

\subfigure [\dots and merged with its successor.] {
    \begin{tikzpicture}[>=stealth']
    \blockto{0}{4}{0}{\itemWidth*0}{4};
    \foreach \i in {0,...,3}{
	\pgfmathsetmacro\idx{\i + 4}
	\node[minimum size=\itemWidth, rectangle, draw, xshift=(\itemWidth*3)+(\i)*\itemWidth] {\tiny $k_{\pgfmathprintnumber{\idx}}$};
    }
    \insertblock{1}{12}{\itemWidth*9};
    \end{tikzpicture}
}

\end{center}
\caption{Deleting keys from an LSML.}
\label{fig:lsml_remove}
\end{figure}

% Removing keys
Deletion of keys is handled lazily: A key is at first merely marked as deleted. 
Once half of the keys of a block $b_i$ have been removed, it is shrunk to half its original capacity and its level is decrementing. If necessary, the block is merged with its successor $b_{i+1}$ (blocks of level 0 will be deleted once empty; the block $b_a$ is never deleted or shrunk).
Note that at most one merge operation is necessary after shrinking a block.
Fig.\ \ref{fig:lsml_remove} illustrates this process.

\begin{corollary}
Merging two blocks requires constant time.
\end{corollary}
\begin{proof}
Since all operations are executed in-place on the Virtual Array, two blocks are merged by doubling the capacity (i.e., the range of keys maintained by the block) and increasing the level of the first block and deleting the second.
\end{proof}

\begin{lemma} \label{lemma:nrBlocks}
An LSML requires at most $2 + \lfloor \log n \rfloor$ blocks to store $n$ keys.
\end{lemma}
\begin{proof}
If no keys are deleted, this follows directly from the definition of the LSML's structure via the binary representation of $n' = c \lfloor \frac{n}{c} \rfloor$. 

Assume that $\lfloor \log n \rfloor + 1$ blocks of level $l>0$ are required to store $n$ keys.
Furthermore, assume $c=1$.
Once half of the keys of a block are marked as deleted, the block is shrunk to half its original capacity, implying that in a block of level $l>0$, strictly less than half of the keys can be marked for deletion.
Thus, $\lfloor \log n \rfloor$ such blocks contain at least $n/2 + 1$ non-deleted keys.
An additional block of level $l>0$ must be of level at least $\lfloor \log n \rfloor + 1$ (at most one block of any level may exist) and thus contain at least $2^{\lfloor \log n \rfloor + 1}/2 \geq n/2 $ non-deleted keys; a contradiction, since less than $n/2$ keys are left.
In addition to the $\lfloor \log n \rfloor$ blocks of level $l>0$, the additional block $b_a$ is always allocated and one block of level 0 containing just 1 key may be present.
For $c>1$, the number of blocks does not increase since the capacity of each block is multiplied by $c$.
\end{proof}

\subsubsection{LSML-Blocks} \label{sec:ppq:description:partitioning}
%pivot generation
\begin{algorithm}
\caption{Pseudo code for the generation of a pivot element}
\label{algo:pivot}
\begin{algorithmic}[1]
\Function{pivot}{Block $b$, $left$, $right$}
\State Randomly select a key $k$ in the range $b[left],\dots, b[right]$
\State $i \gets$ random number in $[1,\dots,d]$
\State $x \gets k[i]$ 
\State \Return{$(x,i)$}
\EndFunction
\end{algorithmic}
\end{algorithm}

%Idea
A block partitions the keys it holds by a pivot value, which is found by randomly\footnote{All random selections in this work are meant to be made \emph{uniformly and independently.}} selecting a key within the range to be partitioned and a random value $i \in [1,\dots,d]$ (where $d$ is the number of dimensions of a key). 
The pivot element $x$ is then taken to be $x = k[i]$ (Algorithm \ref{algo:pivot}). Our procedure (Algorithm \ref{algo:partition}) for in-place partitioning multi-dimensional keys is similar to the well-known partitioning algorithm for scalar values\footnote{In principle, one could take the whole key as the pivot element. However, this might make the partitioning infeasible if the set of Pareto-optima is large.}.

% partitioning
\begin{algorithm}[t]
\caption{Pseudo code for partitioning a block}
\label{algo:partition}
\begin{algorithmic}[1]
\Require{Block $b$; $left,right \leq$ size of $b$; $left\leq right$}
\Require{$s_p$: desired maximum size of right partition $p_r$}
\Function{partition}{Block $b$, $left$, $right$}
\State $(x,i) \gets$ \Call{pivot}{$b$, $left$, $right$}
\While{$left < right$}
  \While{$left < right$ and (key $k = b[left]$,  $k \more_i x$)}
    \State $left = left + 1$
  \EndWhile
  \While{$left < right$ and (key $k = b[right]$, $k \lesseq_i x$)}
    \State $right = right -1 $
  \EndWhile
  \If{$left < right$}
    \State Swap the keys $b[left]$ and $b[right]$
    \State $left = left + 1$
    \State $right = right - 1$
  \EndIf
\EndWhile
\Statex \Comment{Check if key at $left$ belongs to left partition}
\If{$k = b[left]$, $k \more_i x$}
  $ left = left + 1$
\EndIf
\Statex \Comment{Recursively partition the right part until it falls below the cut-off value}
\If{$left < right$ and $(right-left) > s_p$}
  \State \Call{partition}{$b$,$left$,$right$}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

Keys with less priority (at dimension $i$) than the pivot element get sorted to the left, while keys with higher or equal priority are sorted to the right.
This implies that the key used to generate the pivot element is sorted to the right as well.
We will call these two parts the left and the right partition and use a subscript to indicate which partitioning step created them: The two partitions created by the first partitioning step are dubbed $p_{l1}$ and $p_{r1}$.

%Pareto optimality
Keys sorted into the right partition are guaranteed to not be dominated by the ones in the left (since they have a higher priority value in at least one dimension).
Nevertheless, the left partition might contain Pareto-optimal keys; this behavior is acceptable since the PPQ is required to find but one Pareto-optimal key and not the whole set of Pareto-optima.
The right partition $p_{r1}$ is partitioned recursively until it is smaller than a given constant cut-off value $s_p$, the maximum size that we are aiming for in the right partition.
The left partition $l_i$ of each partitioning step is not processed further.
The right partition created by the last step (for simplicity referred to as $p_r$) contains keys that are not dominated by any key in any of the left partitions, 
% the problem with the right partition
though it may itself contain two keys where one dominates the other. 
The \texttt{peek} operation (Section \ref{sec:ppq:description:peek}) addresses this problem by linearly scanning the right partition $p_r$ to ensure that a non-dominated key is returned.
A different approach would be to flat out compute the Pareto-optima within $r_p$ by a linear expected-time algorithm as presented in \cite{BentleyCL93, GodfreySG07}.

\newcommand{\partition}[4]{
  \node[minimum width=#4, minimum height=8mm, rectangle, draw, anchor=west] at (#2.east) (#1) {};
  \node[anchor=north west] at (#1.south west) {\tiny #3}
}

% structure of a block
\begin{figure}
\begin{center}
\begin{tikzpicture}
\node (n0) {};
\partition{n1}{n0}{$p_{l_1}$}{30mm};
\partition{n2}{n1}{$p_{l_2}$}{20mm};
\node[anchor=west,yshift=2.5mm] at (n2.east) (d1) {\tiny\dots};
\node[anchor=west, minimum width=6mm] at (n2.east) (d0) {};
\node[anchor=west,yshift=-2.5mm] at (n2.east) (d1) {\tiny\dots};
\partition{n3}{d0}{$p_{l_i}$}{15mm};
\partition{n4}{n3}{$p_{r}$}{15mm};
\partition{n5}{n4}{$p_{d}$}{25mm};
\end{tikzpicture}
\caption{Structure of a block after partitioning. From the left to the right, we have 1) several left partitions $p_{li}$, one created by each partitioning step; 2) the right partition $p_r$, containing keys not dominated by any key in the left partitions; and 3) the section $p_{d}$ containing the keys marked for deletion. 
}
\label{fig:block_structure}
\end{center}
\end{figure}
% figs and refs
Fig.\ \ref{fig:block_structure} depictst the structure of a block after it was partitioned (The $p_d$ section contains deleted keys and is discussed in detail later on).

\medskip
% problems we have here
In the algorithm outlined above, several important details still need to be addressed:

\medskip
\noindent
\textbf{Pivot generation}. As with other randomized algorithms, e.g., Quicksort, it is crucial to select ``good'' pivot elements to avoid worst-case behavior. 
To get reasonable pivot values (at least on expectation) we sample multiple times and then choose the median as our pivot element.  
\begin{corollary}
Generating a pivot element takes $O(n)$ time. 
\end{corollary}
\begin{proof}
Accessing a random key within a block requires $O(n)$ times (due to the underlying Virtual Array). Accessing a constant number of keys and selecting the median of those does not increase the asymptotic complexity. 
\end{proof}

\medskip
\noindent
\textbf{Ensuring termination}. The algorithm as given above does not necessarily terminate.
Assuming that all the keys managed by a block are equal, partitioning would sort all the keys into the right partition -- irregardless of which key is chosen as the pivot -- and then try to partition the very same elements again.
We call such an event a ``failed partitioning step''.
It may also occur if we choose the same pivot element as was used by the last partitioning step.
While in this case we can try to generate a new, different pivot element, the former case does not allow for further partitioning and we ensure termination by aborting the recursive procedure after a constant number of successive failed partitioning steps.
In this case, the size of the right-most partition will be larger than the size we are aiming for; nevertheless, it is guaranteed that the keys it contains are not dominated by keys that have been sorted to the left by preceding partitioning steps.
We make the following assumption:
\begin{assumption} \label{ass:partition_succeeds}
After partitioning a block, its right partition $p_r$ is smaller than or equal to the constant cut-off value $s_p$. 
\end{assumption} 

\medskip
\noindent
\textbf{Handling dead, taken and removed tasks}. Once a task was returned by \texttt{pop}, its associated key has to be removed from the queue. 
Furthermore, keys associated with tasks that have been marked dead or taken also need to be removed. 
% active, non-active tasks
We subsume all those tasks as ``non-active'' and its associated keys as deleted, whereas a task still awaiting execution is called an active task.
% How a block handles dead tasks
Speaking informally, each block moves its deleted keys to the end of the block and keeps them there.
Once the block gets merged with another block, the deleted keys of both blocks are moved to the end of the block generated by the merge.
Thus, the deleted keys gradually accumulate in consecutive ranges and once the number of deleted keys is large enough, part of them can be removed via the \texttt{remove} operation of the Virtual Array. 
Smaller ranges will move to the last block of the LSML and accumulate there.

\begin{algorithm}[t]
\caption{Pseudo code for partitioning with deleted keys}
\label{algo:partition_with_nonactive}
\begin{algorithmic}[1]
\While{$left < right$}
\State key $k \gets b[left]$
\If{$k$ is a non-deleted key}
    \If{$k \more_i x$}
    \State $left = left + 1$  \Comment{If $k$ has less priority than the pivot, it stays}
    \Else
    { break;} \Comment{Otherwise, it is moved to the right partition}
    \EndIf
\Else \Comment{$k$ is a deleted key}
    \State $delete = delete - 1$; 
    \State swap keys $b[left]$ and $b[delete]$
    \If {$right == delete$} $right = right - 1$
    \EndIf
\EndIf
\EndWhile
\While{$left < right$}
\State key $k \gets b[right]$
\If{$k$ is a non-deleted key}
    \If{$k \lesseq_i x$}
    \State $right = right - 1$   \Comment{If the pivot has less priority than $k$, $k$ stays}
    \Else { break;} \Comment{Otherwise, $k$ is moved to the left partition.}
    \EndIf
\Else \Comment{$k$ is a deleted key}
    \State $delete = delete - 1$
    \If {$right == delete$} $right = right - 1$
    \Else
    \State Swap the keys $b[right]$ and $b[delete]$;
    \EndIf
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

% extended algorithm
The partitioning procedure is adapted as follows to handle deleted keys (Algorithm \ref{algo:partition_with_nonactive} replaces lines 4-9 in Algorithm \ref{algo:partition}):
\begin{itemize}
\item An additional partition, called $p_{d}$, is used for storing the deleted keys. The index $delete$ is used to keep track of it and is initially set to $right$, i.e., the end of the range to be partitioned.
\item If a deleted key is encountered during the scan for the next key to swap, it is swapped out to section $p_d$. 
\end{itemize}

\begin{lemma}
The partition operation has worst-case complexity $O(n^2)$.
\end{lemma}
\begin{proof}
The complexity of Algorithm \ref{algo:partition} is dominated by the outer while-loop (lines 3-15) and the recursive call (generating a pivot element requires $O(n)$, the checks in lines 16 and 18 require constant time). 
The loop is executed at most $n$ times, since either $left$ is incremented or $right$ is decremented by at least 1 in each execution. 
Each execution of one of the inner loops (lines 4-6 and 7-9, respectively) reduces the number of executions of the outer loop by 1. 

The same argument holds for the modification (given in Algorithm \ref{algo:partition_with_nonactive}), with one exception: 
If a deleted key is encountered while scanning from left (right), it is swapped to section $p_d$. 
If $delete$ and $right$ point to the same location, $right$ can be decremented since the key it points to is now a deleted key. 
If that is not the case, however, neither $left$ nor $right$ may be advanced, since the keys at their respective locations have not been checked yet. 
This can happen at most $n$ times for the whole block, since $right <= delete$ always holds.
Thus, the worst-case complexity does not change.

Due to Assumption \ref{ass:partition_succeeds}, we know that each successive call reduces the size of the range that remains to be partitioned by at least 1 key in a constant number of partitioning attempts. 
Thus, the recursion depth is bounded by $O(n)$ and partitioning requires $O(n^2)$ time in the worst case.
\end{proof}

\begin{conjecture}
\texttt{partition} has expected time complexity $O(n \log n)$.
\end{conjecture}
The conjecture is immediate if one assumes that each pivot element is taken from a key close to the median (when ordered keys by their values at dimension $i$ the pivot is taken from) of the set of keys to be partitioned, since each partitioning step would roughly half the size of the range that requires further partitioning.
Furthermore, the deleted keys are only touched once to move them to section $p_d$ and the number of non-deleted keys in a block never increases. This further indicates reasonable performance in practice.
However, to prove the conjecture formally, assumptions about the distribution of the keys would have to be made. 

\subsubsection*{Removing deleted keys}
After partitioning the whole block, its level is reduced as far as possible (Algorithm \ref{algo:reducellvl}) and, in case the section containing deleted keys is large enough, part of them are removed from the LSML via the \texttt{remove} operation of the Virtual Array.
The level $l$ of a block $b$ can be reduced if
\begin{itemize}
\item $l > 0$ (the minimum level of a block is $0$), and 
\item the level of the block succeeding $b$ is is smaller than $l$ (if such a block exists), and
\item all non-deleted keys are covered after reducing the level, i.e., the $delete$ index must remain inside the range of keys covered by the block, which is $s_p 2^l$.
\end{itemize}

\begin{algorithm}
\caption{Pseudo code for shrinking a block}
\label{algo:reducellvl}
\begin{algorithmic}[1]
\Function{shrink}{Block $b$}
\State $l \gets $ level of $b$ 
\State $l_s \gets $ level of the successor of $b$
\While{$l > 0$ and $l_s < l$ and $delete < s_p  2^{l-1}$}
\State $l = l-1$
\EndWhile
\State \Call{remove}{$delete$, end of block $b$} \Comment{Step 3: Remove deleted keys}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{corollary}
The \texttt{shrink} operation has time complexity $O(\log n)$.
\end{corollary}
\begin{proof}
The loop is executed at most $l$ times. Since $n = s_p 2^l$ (by definition of a block), the number of executions is bounded by $O(\log n)$. 
Removing a range of keys from the Virtual Array requires $O(1)$ time (see Section \ref{sec:virtual_array}).
\end{proof}

\begin{lemma} \label{lemma:partitionandshrink}
After partitioning and shrinking a block, it uses $O(n)$ memory to store $n$ keys. 
\end{lemma}
\begin{proof}
\texttt{partition} moves all deleted keys to section $p_d$ and \texttt{shrink} extends this section as far as possible. It follows directly from the call of the Virtual Array's \texttt{remove} operation at the end of the \texttt{shrink} procedure that at most a constant number of keys are stored in the block in addition to the $n$ non-deleted keys.
\end{proof}

\subsubsection{Peek} \label{sec:ppq:description:peek}
\begin{algorithm}
\caption{Pseudo code for the \texttt{peek} operation}
\label{algo:peek}
\begin{algorithmic}[1]
\Require{Block $b$ is partitioned into partitions $p_{l1},\dots , p_{li}, p_r, p_{d}$} 
\Function{peek}{Block $b$}
\State key $k = null$ 
\ForAll{Keys $x$ in $p_r$} \Comment{Scan the right partition}
  \If{$k == null$ or $x \less k$} $k = x$
  \EndIf
\EndFor
\If{$k == null$} \Comment{No non-deleted keys in the right partition}
  \State Extend $p_{d}$ to include $p_r$ \Comment{$p_d$ now covers $p_r$}
  \If{$p_{l1} < p_{d}$} \Comment{Check if some non-deleted keys exist in the block}
    \State Set $p_r \gets p_{li}$
    \State \Call{partition}{$b$, $p_r$, $p_{d}$} \Comment{Further partition $p_r$}
    \State \Call{shrink}{$b$} \Comment{Shrink the block as far as possible}
  \EndIf
  \State $k \gets $ \Call{peek}{$b$}	\Comment{Call peek again on the new right partition}
\EndIf
\State \Return{$k$}
\EndFunction
\end{algorithmic}
\end{algorithm}
% peek
Once a block is partitioned, the \texttt{peek} operation (Algorithm \ref{algo:peek}) is relatively easy:
We scan all the non-deleted keys in the right partition and keep the first encountered key as a candidate which is and compared to all remaining keys in the partition to ensure that it is not dominated by any of them.
If it is, the dominating key is taken as the new candidate.
% falling back
In case no such key is found in the right partition, we fall back to the previous partition: since the right partition does not contain any more non-deleted keys, it is merged with the section holding the deleted keys; the left partition with highest index is then taken as the new right partition.
This partition might have to be partitioned further, since its size is bounded from above only by $O(n)$ and may exceed $s_p$, the size that we aim for the right partition. 
Once the right partition falls below this value, we can again scan it for a non-dominated key.
% dead block
In case no non-deleted key can be found in the whole block (i.e., after falling back to the last existing partition) the whole block is marked as deleted, implying that no more peek operations have to be executed on it.
It will be merged with the section holding the deleted keys by the next \texttt{pop} or \texttt{push} operation that triggers a merge. 
When falling back to another partition, \texttt{peek} shrinks the block to ensure that it uses $O(n)$ memory (see Lemma \ref{lemma:partitionandshrink}) but it does not merge blocks in case the successor of block $b$ has the same level. 
It will be shown in the following how the \texttt{push} operation ensures the logarithmic number of blocks after while using \texttt{peek}.

\begin{lemma}
The \texttt{peek} operation has amortized time complexity $O(\log n)$.
\end{lemma}
\begin{proof}
The for-loop (lines 3-6) has constant complexity because the size of the right partition $p_r$ is bounded by the constant $s_p$, due to Assumption \ref{ass:partition_succeeds}.

We amortize the cost of the calls to \texttt{partition} and \texttt{peek} over the number of keys in a block:
All calls to \texttt{partition} are made with non-overlapping ranges of keys and thus for $n$ \texttt{peek} operations the whole block is partitioned only once in total, i.e., the partition operations caused by $n$ calls to \texttt{peek} require $O(n\log n)$ time in total.
% A block is partitioned into at most $n$ partitions, implying that $n$ \texttt{peek} operations cause at most $n$ fall backs. 
or each fall-back, \texttt{peek} is called once and thus $n$ \texttt{peek} operations cause at most $n$ recursive calls.

We conclude that the complexity of \texttt{peek} is dominated by \text{partition} and thus the operation has an amortized complexity of $O(\log n)$.
\end{proof}

\subsubsection{Take} \label{sec:ppq:description:take}
While the \texttt{peek} operation finds a Pareto-optimal key without removing it from the queue, the \texttt{take} operation removes a given key from the PPQ by marking it as deleted.
Subsequent \texttt{push} and \texttt{pop} operations will eventually remove the deleted key from the priority queue.
The \texttt{take} operation thus trivially has complexity $O(1)$.

\subsubsection{Merging blocks} \label{sec:ppq:description:merging_blocks}
\begin{algorithm}
\caption{Pseudo code for merging several blocks}
\label{algo:merge_from}
\begin{algorithmic}[1]
\Function{merge\_from}{Block $b$}
\State let $b_p$ be the block preceding $b$
\State $merged \gets false$
\While{$b_p$ exists and is either of same level as $b$ }
\State combine $b_p$ and $b$
\State $b \gets b_p$
\State $b_p \gets$ predecessor of $b$
\State $merged \gets true$
\EndWhile
\State \Return{$merged $}
\EndFunction
\end{algorithmic}
\end{algorithm}

As outlined above, all keys that are added to the PPQ are first stored in a special block of level 0, called $b_a$. 
Once this block is full, a new block of level 0 is created and all keys from the block $b_a$ are moved there.
If another block of level 0 already exists, the two blocks are merged to create a block of level 1. 
This operation is continued as long as there are two blocks of same level, as shown in Algorithm \ref{algo:merge_from} (due to the structure of the LSML, two blocks of same level are always neighbors).
The resulting block $b$ will have to be partitioned before the next \texttt{peek} operation. 

\begin{corollary}
The \texttt{merge\_from} operation has worst-case time complexity $O(\log n)$.
\end{corollary}
\begin{proof}
Since there are $O(\log n)$ blocks in an LSML, the while loop (lines 4-9) requires at most $O(\log n)$ time (in case all the blocks are combined) and the resulting block is  at most of size $n$.
\end{proof}

\subsubsection{Push} \label{sec:ppq:description:push}
Adding keys to the PPQ via the \texttt{push} operation is quite simple, as shown in Algorithm \ref{algo:push}. 
In case the block $b_a$ is full, a new block of level 0 is created and all keys from block $b_a$ are moved there. Blocks are then merged, if necessary.

\begin{algorithm}
\caption{Pseudo code for the push operation}
\label{algo:push}
\begin{algorithmic}[1]
\Require{Insert-block $b_\text{insert}$}
\Function{push}{Key $k$}
\If{$b_a$ is full}
  \State Create a new block $b$ of level 0 at the end of the linked list
  \State Move all elements from $b_a$ to $b$
  \If{\Call{merge\_from}{$b$}}
    \State \Call{partition}{$b$}
  \EndIf
\EndIf
\State Insert $k$ in $b_\text{insert}$
\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{table*}
\begin{center}
    \begin{tabular}{| r | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c |}
    \hline
    $i$: &  1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16\\
    \hline
    Processed keys:  & 0 & 2 & 0 & 4 & 0 & 2 & 0 & 8 & 0 & 2 & 0 & 4 & 0 & 2 & 0 & 16\\ 
    \hline
    \end{tabular}
\end{center}
\caption{Number of keys processed in a merge operation after inserting keys $i s_p$ keys.}
\label{table:nrMerges}
\end{table*}

\begin{lemma} \label{lemma:push}
The \texttt{push} operation has amortized time complexity $O(\log^2 n)$.
\end{lemma}
\begin{proof}
In case the block $b_a$ is not full, \texttt{push} trivially requires $O(1)$ time. 
Otherwise, creating a new block and moving all keys from block $b_a$ to the new block can be done in time $O(n)$, since obtaining an iterator to the end of the block requries $O(n)$ time (the number of keys in $b_a$ is equal to the constant $s_p$ and thus copying them requires only constant time).
Since the \texttt{merge\_from} operation requires $O(\log n$ time it follows that \texttt{partition} dominates the complexity of the operation. 

We amortize its cost over $n$ \texttt{push} operations: 
The first time block $b_a$ is full, no merge is required since no other block exists. 
The second time the new block needs to be merged with the other block of level 0, prompting a \texttt{partition} operation with $2s_p$ keys.
Once $4 s_p$ keys have been inserted, one additional merge of $4 s_p$ keys is required.
However, no additional merge of $2 s_p$ keys is done. 
This can be seen as follows: 
Inserting $3 s_p $ keys creates one block of level 1 and one block of level 0. 
After adding $s_p$ more keys, a new level 0 block is created and, since now two blocks of same level exist, \texttt{merge\_from} will combine them to a level 1 block. 
Now that two blocks of level 1 exists, they are combined into a level 2 block, which is then partitioned.
Table \ref{table:nrMerges} list the number of keys processed in the \texttt{merge\_from} and \texttt{partition} operation after a given number of keys have been added.

One merge operation is required every $2 s_p$ keys, i.e., $n/2s_p$ merge operations are required for $n$ keys. 
This is clearly less than one merge of $2 s_p$ keys every $2 s_p$ keys plus one merge of $4 s_p$ keys every $4 s_p$ keys and so on. 
The number of keys processed by the \texttt{partition} operation is thus at most  $m = n \log n$.
Pushing $n$ keys therefore requires at most 
$$
m \log m = n \log n \log (n \log n) = n \log^2 n + n \log n \log \log n
$$
time, implying \texttt{push} has amortized complexity $O(\log^2 n)$.
\end{proof}


\subsubsection{Pop} \label{sec:ppq:description:pop}
To find a non-dominated key, \texttt{peek} is used to generate a set of $O(\log n)$ keys (one per block), where each key is Pareto-optimal w.r.t.\ the keys stored in the block it was obtained from. 
Thus, this set needs to be scanned to ensure that a key that is not dominated by any other in the set is returned (to ensure this, each block needs to use the same pivot elements for partitioning its keys).
In case the number of non-deleted keys in a block after a \texttt{peek} operation drops below half the block's capacity, the block is shrunk. 
To maintain the logarithmic number of blocks, a merge of $b$ with its predecessor (in case the block was shrunk to the same level as $b$ in the previous iteration of the while loop), or with its successor (in case $b$ was shrunk to the same level as its successor and the successor is not shrunk in the next iteration) might be required. 
However, at most one of these situations can occur..

\begin{algorithm}
\caption{Pseudo code for the pop operation}
\label{algo:pop}
\begin{algorithmic}[1]
\Require{Block $b_a$}
\Function{pop}{}
\State Key $k_{best} \gets$ \Call{peek}{$b_a$}
\State Block $b_{best} \gets b_a$
\State Block $b \gets b_a $
\While{$b.next$ exists}
  \State $b \gets b.next$
  \State Key $k \gets$ \Call{peek}{$b$}
  \If{$k \neq null$}
    \If{$k_{best} == null$ or $k \less k_{best}$}
      \State $k_{best} \gets k$
      \State  $b_{best} \gets b$ 
    \EndIf
  \EndIf
  \If{\Call{merge\_from}{$b$}}
    \State mark block $b$ for partitioning
  \EndIf
\EndWhile
\ForAll{blocks $x$ marked for partitioning}
    \State \Call{partition}{$x$}
\EndFor
\State \Return{$k$}
\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{lemma}
The \texttt{pop} operation has amortized time complexity $O(\log^2 n)$.
\end{lemma}
\begin{proof}
The number of executions of the while loop is bounded by the number of blocks and is $O(\log n)$. 
\texttt{peek} and \texttt{merge\_from} each require $O(\log n)$ time and thus the while loop has complexity $O(\log n)$.
The cost of \texttt{partition} can be amortized over $n$ \texttt{pop} operations to $O(\log^2 n)$ with similar arguments as used for the analysis of the \texttt{push} operation (Lemma \ref{lemma:push}).
\end{proof}

So far, for the analysis of \texttt{push} (\texttt{pop}) we assumed that a sequence of $n$ operations was not interrupted by any \texttt{pop} (\texttt{push}) operations. 
Regarding general sequences of \texttt{push} and \texttt{pop}, we make the following claim (without proof): 
\begin{conjecture}
The amortized asymptotic complexity of $n$ \texttt{push} (\texttt{pop}) operations does not change if some keys are popped (pushed) in the meantime.
\end{conjecture}

\subsubsection{Spy} \label{sec:ppq:description:spying}
In case a thread does not have any keys in its own local priority queue, it may \texttt{spy} some from another thread. 
A thread spies keys from another thread, called the victim, by copying them into its own priority queue.
The key itself is not altered or deleted from the victims priority queue and it has to be ensured by appropriate synchronization mechanisms that two \texttt{pop} operations executed by different threads do not return the same key.
This can be achieved with the help of an atomic flag associated with each key: before \texttt{pop} returns a key, it has to check that the flag is not set before setting the flag itself, all in one atomic operation. 
This variable is called the \texttt{taken} flag and taken keys are dealt with in the same way as deleted keys.

\begin{algorithm}
\caption{Pseudo code for the spy operation}
\label{algo:spy}
\begin{algorithmic}[1]
\Function{spy}{}
\State Place $other \gets item.owner$
\For{$i \gets other.begin$; $i < other.end$; $i = i+1 $}
  \State Key $k \gets other[i]$
  \If{$k$ is a non-deleted key}
    \Call{push}{$k$}
  \EndIf
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

A \texttt{spy} operation is performed by scanned the keys of another thread's priority queue (via the Virtual Array) and adding each non-deleted key to the thread-local priority queue via \texttt{push} (Algorithm \ref{algo:spy}.

\begin{corollary}
The \texttt{spy} operation has complexity $O(n \log^2 n)$.
\end{corollary}
\begin{proof}
This follows immediately from \texttt{push} having complexity $O(\log^2 n)$.
\end{proof}

\subsubsection{Memory usage} \label{sec:ppq:description:memory}
\begin{corollary}
After a \texttt{pop} operation, an LSML uses $O(n)$ space to store $n$ keys.
\end{corollary}
\begin{proof}
This follows directly from lemmas  \ref{lemma:virutalArrayMemory}, \ref{lemma:nrBlocks} and \ref{lemma:partitionandshrink}.
\end{proof}


\subsection{Integration in Pheet} \label{sec:ppq:pheet_integration}
% Strategy scheduler 2
Pheet's \texttt{StrategyScheduler2}, a \emph{two-level concurrent ordered container} (described in detail in \cite{Wimmer14}, Section 5.5) allows for a quick integration of our PPQ into the scheduling framework.
% Base classes provided; pheet terminology
It offers base classes for alternative \emph{task storage}\footnote{In Pheet terminology, a priority queue managing tasks is called a task storage.} implementations.
% The classes we us.
In particular, to integrate the PPQ into Pheet, we have to extend the following classes:
\begin{inparaenum}
\item \texttt{Strategy2BaseTaskStorageBaseItem},
\item \texttt{Strategy2BaseTaskStoragePlace} and
\item \texttt{Strategy2BaseTaskStorage}.
\end{inparaenum}
We will give an intuitive understanding of their functionality as far as it is required for our implementation.
For a detailed description of the base classes and correctness proofs, we refer the reader to \cite{Wimmer14}.

\subsection{Implementation} \label{sec:ppq:impl_analysis}
% A general, high level overview and interfaces
The class \texttt{ParetoItem} (Listing \ref{lst:PTSItem}) is used to represent tasks managed by scheduler. It references an object of type \texttt{SchedulerTask} (Listing \ref{lst:SchedulerTask}), which contains the code to be executed, i.e., the work the task is to perform, and an object implementing the \texttt{ParetoStrategy} interface (Listing \ref{lst:ParetoStrategy}), which handles the priority of the task and allows to mark it as dead.

The \texttt{ParetoItem} class has to guarantee that its \texttt{take} operation returns the referenced task exactly once, even if multiple threads execute it concurrently. 
Although an item is at first only visible to the thread creating it (also called the \emph{owning} place), this may change after another thread performs a \texttt{spy} operation on the priority queue of the owning place.
The base class \texttt{BaseItem} provides some functionality for memory re-usage and serves as an interface for \texttt{StrategyScheduler2}, which is oblivious of the task storage implementation.	

% ParetoItem
\begin{code}[label=lst:PTSItem, caption=Interface of ParetoItem]
class ParetoItem : BaseItem {

  /* The task to execute. */
  SchedulerTask task;
  
  /* The strategy associated with the task. */
  ParetoStrategy strategy;
  
  /* If false, no thread is processing this item yet. */
  atomic<bool> taken;
  
  /* Get the task referenced by this item if it is not taken. A task may be taken only once. */
  SchedulerTask take();
}
\end{code}

The \texttt{SchedulerTask} and \texttt{ParetoStrategy} classes have to be implemented by the application employing the scheduler, i.e., the application programmer.
While the former contains the code to be executed upon execution of the task, the latter provides access to the priority vector associated with a task.
Furthermore, the function \texttt{priorizite}, given another \texttt{ParetoStrategy} object, decides which of the two has higher priority. 
The \texttt{dead\_task} function is used by the scheduler to query whether the associated task may be dropped without being executed\footnote{This feature, called ``speculative execution'', is described in Section \ref{sec:pheet:task_priorites}}.
Both functions will be used by our task storage implementation to order tasks w.r.t.\ their associated priorities.

% Task
\begin{code}[label=lst:SchedulerTask, caption=Interface of SchedulerTask]
/* To be implemented by the application. The code in execute() will be run when the task is executed. */
class SchedulerTask {

  void execute();
}
\end{code}

The central -- or global -- part of the MCQP is implemented in \texttt{ParetoTaskStorage} (Listing \ref{lst:ParetoTaskStorage}), which is mostly used for rerouting \texttt{pop} and \texttt{spy} operations to the place-local parts of the PPQ, implemented in \texttt{ParetoTaskStoragePlace} (Listing \ref{lst:ParetoTaskStoragePlace}). 
% boundary
Notice that the \texttt{pop} and \texttt{spy} operations of both classes have an additional parameter called the boundary item. 
The \texttt{StrategyScheduler2} maintains its own LIFO priority queue\footnote{This is necessary for some advanced features the base data structures provide. We do not require them, they are not introduced in this report.} and the boundary item would be the next item selected by \texttt{pop}/\texttt{spy} according to this order.
However, this default behavior may be overridden within certain limits: 
\begin{itemize}
\item The \texttt{pop}/\texttt{spy} operation may select other items with a \emph{higher} priority than the boundary item, thereby violating the LIFO order. Since we are using multi-dimensional priority vector, this means that the priority associated with the selected item may not be dominated by the priority of the task referenced by the boundary item.
\item The \texttt{pop} operation must not return a \texttt{SchedulerTask} if the boundary item was already taken by another thread in the meantime.
\end{itemize}

% TaskStoragePlace
\begin{code}[label=lst:ParetoTaskStoragePlace, caption=Interface of ParetoTaskStoragePlace]
/* The place-local part of the PPQ. */
class ParetoTaskStoragePlace : Strategy2BaseTaskStoragePlace {

  /* Add the given item to the task storage. */
  void push(ParetoItem item);
  
  /* Get an item with locally non-dominated priority and remove it from the task storage. */
  SchedulerTask pop(ParetoItem boundary);
  
  /* Get an item (with locally non-dominated priority) by stealing it from another place. */
  SchedulerTask steal(ParetoItem boundary, ParetoTastStoragePlace other_place);
}
\end{code}

% TaskStorage
\begin{code}[label=lst:ParetoTaskStorage, caption=Implementation of ParetoTaskStorage]
/* The global part of the PPQ. Calls to pop and steal are simply forwarded to the responsible places. */
class ParetoTaskStorage : Strategy2BaseTaskStorage {

  ParetoTaskStoragePlace[] places;
  
  SchedulerTask pop(ParetoItem boundary, int place_id) {
    return places[place_id].pop(boundary);
  }
  
  SchedulerTask steal(ParetoItem boundary, int place_id) {
    return places[place_id].steal(boundary);
  }
}
\end{code}


\section{Shortest path problems} \label{sec:shortest_path}
% Informally introduce shortest path
The problem of finding a shortest path between two nodes in a graph is a classical problem in computer science with numerous practical applications, such as finding the quickest route from location A to location B.
% Introduce the problem of msp
While the \emph{shortest-path problem} minimizes for a single criteria, e.g., the travel time, in practical applications one often wants to optimize multiple -- and possibly conflicting -- objectives:
When searching for a route from A to B, we may want to minimize the travel time as well as toll costs.
In such a setting, we usually cannot give a single best solution anymore. 
Instead, we can give several reasonable solutions: 
The fastest route will usually use highways, thus increasing the toll costs, while a slightly slower route may reduce the toll costs by avoiding highways. 
Problems like this can be modeled as \emph{multi-criteria shortest path} (MSP) problems, where we are interested in all optimal alternatives, i.e., in routes that are \emph{Pareto-optimal}. 
A route from A to B is Pareto-optimal if there is no other route with less or equal cost for each of the $d$ objectives.

%subsection{Definitions and notation} \label{sec:shortest_path:defs}
In the following, we provide definitions for variations of the shortest path problems considered in this report. 
The classic text book by Cormen et al.~\cite{CLRS01} provides a more detailed introduction to basic shortest path problems and algorithms.

% Define single-source shortest path
\subsection{The classical shortest path problem} \label{sec:shortest_path:defs:sssp}
We are given a weighted directed graph $G=(V,A)$, where $V= \{v_1, v_2, \dots, v_n\}$ is a finite set of nodes, $A \subseteq V \times V$ is a finite set of arcs (directed edges) and $c: A \rightarrow \mathbb{N}$ is the cost or weight function. 
We define the set of neighbors of a node $v$ as the nodes adjacent to an outgoing arc of $v$, i.e., $\{w \in V \mid \exists a\in A, a = (v,w) \}$.
W.l.o.g., we assume that $G$ is a connected graph.

Let $p = \langle v_0, a_1, v_1, a_2, \dots,a_k, v_k \rangle$ be a path\footnote{We will also write $p = \langle v_0, v_1, \dots,v_k \rangle$, since an arc $a_i$ is well defined by its two adjacent nodes.} from $v_0$ to $v_k$ in $G$.
Furthermore, let $P$ be the set of all paths and $P_{u,v}$ be the set of all paths from node $u$ to node $v$ in $G$.
We define the weight $w(p)$ of a path $p$ as 
\begin{align*}
w: P &\rightarrow \mathbb{N} \\
p &\mapsto w(p) = \sum_{i=1}^{k}c(a_i) \quad , \neqn{sssp_weighted}
\end{align*}
i.e., the sum of costs all the arcs in the path $p$.

% Variations: Single source, all-pairs,... shortest path
In a \emph{single-source shortest path problem (SSSP)}, we want to compute a shortest path w.r.t.~some weight function $w$ from a given source node $s \in V$ to each node $v \in V$.
Different variations of the problem exist, such as the \emph{single-destination}, the \emph{single-pair} or the \emph{all-pairs} shortest path problem. 
Note that an optimal algorithm for the SSSP can easily be turned into an optimal algorithm for the first two variants \cite{CLRS01}.
Thus, we will only deal with the SSSP in this report.

\subsection{Multi-criteria shortest path (MSP)} \label{sec:shortest_path:defs:msp}
% MSP
Multi-criteria shortest path problems \cite{Martins84} generalize shortest path problems w.r.t.~the weight function $c$, which is extended to $d$-dimensional vectors, i.e., $\vect{c}: A \rightarrow \mathbb{N}^d$ and
\begin{align*}
\vect{w}: P &\rightarrow \mathbb{N}^d \\
p &\mapsto \vect{w}(p) = (w_1(p), w_2(p), \dots , w_d(p)) \quad , \neqn{msp_weighted}
\end{align*}
where $w_j(p) = \sum_{i=1}^{k}c_j(a_i)$ $\forall j \in \{1,\dots,d\}$.

% Pareto optima
A path $p$ from node $v_1$ to node $v_2$ is a \emph{Pareto-optimal} or \emph{non-dominated} path if there is no path $q \in P$ from $v_1$ to $v_2$ s.t.~$q \less p$.
Informally, we also call a Pareto-optimal path $p$ \emph{shortest path}.
Thus, for the MSP, the solution consists of a set of Pareto-optimal paths for each considered pair of nodes.


\subsubsection*{Applications} \label{sec:shortest_path:applications} 
% Applications
Apart from the problem of finding an optimal route in a road map as outlined above, the MSP problem has numerous outer applications, such as routing in multimedia networks \cite{ClimacoCP03}, route guidance \cite{JahnMS00} and curve approximation \cite{MehlhornZ00}.

\subsubsection*{Hardness and practicality} \label{sec:shortest_path:hardness}
% Hardness of the problem, reason for parallelization
% Note that the graph instances resulting from modeling practical problems are potentially very large. 
The crucial parameter for the complexity of the MSP is the total number of Pareto optima for all visited nodes. 
Since this number is exponential in $n$ in the worst case \cite{Hansen80}, the MSP is in general NP-hard.
Even for $d=2$, the decision problem whether there exists a path between two nodes whose length is below a given threshold is NP-hard \cite{GareyJ79}.
However, it was observed that the problem is efficiently tractable from a practical viewpoint for many practically relevant instances. 
The input data of practical applications tends to have certain characteristics, which lead to the set of Pareto optima for a vertex to be polynomially bounded \cite{Muller-HannemannW06}. 
It was shown that in applied scenarios this number may even be bounded by a small constant \cite{Muller-HannemannW01}. 
Thus, MSP can be solved efficiently for small $d$ and not too large graphs adhering certain characteristics.

% reason for parallelization
However, due to the potentially very large graph instances resulting from modeling e.g., road or railway networks, the computational cost is significant even if a small number of criteria is to be optimized.
Guerriero and Musmanno \cite{GuerrieroM01} thus suggest that parallel computing might help to design efficient solution methods. 

\subsubsection*{Approximate solutions} \label{sec:shortest_path:approx}
% heuristics
Due to the hardness of the MSP, heuristic methods are sometimes employed (see, e.g., \cite{BastMS03, Sonnier06, EhrgottG02}).
% weighted sum approach
A common approach is to define a total order relation $\tor$ on $P$ that allows for a more efficient computation of the Pareto-optima.
Such a relation $\tor$ has to satisfy the following properties\footnote{We denote the concatenation of two paths $p_1 \in P_{u,v}$, $p_2 \in P_{v,w}$ as $\cP{p_1}{p_2}$}
\begin{align}
\forall p,q \in P_{u,v}: p \less q \implies p \tor q  \quad \text{Dominance} \\
\forall p \in P_{u,v}, \forall (v,w) \in A: p \tor \cP{p}{w} \quad \text{Monotonic}
\end{align}
Martins et al.~\cite{MartinsPRS07} show that the following relation satisfies these requirements:
\begin{align}
p \osum q \iff \sum_{i=1}^{d} w_i(p) \leq  \sum_{i=1}^{d} w_i(q) 
\end{align}
% supported/non-supported solutions
However, as Sonnier \cite{Sonnier06} points out, a problem with algorithms based on such a relation is that 
\begin{align}
p \osum q  \notimplies p \less q \quad , \neqn{osum_problem}
\end{align}
i.e., only solutions that lie on the convex hull of the feasible region are found, which are called the supported solutions. 
In other words, there exist Pareto-optima which may not be found because of the property given in Eq.~\ref{eq:osum_problem}.

% graph example from Sonnier
To see this, consider the following example from Sonnier \cite{Sonnier06}.
Assume we are interested in the Pareto-optimal paths from node 1 to node 5 in the graph depicted in Fig.~\ref{fig:example_graph}.

\begin{figure}
\begin {center}
\begin {tikzpicture}[-latex, auto, node distance = 3 cm, on grid, semithick, state/.style = {circle, top color = white, draw, minimum width = 1 cm}]
\node[state] (n1) {$1$};
\node[state] (n2) [below right of = n1] {$2$};
\node[state] (n3) [above right of = n2] {$3$};
\node[state] (n4) [below right of = n3] {$4$};
\node[state] (n5) [above right of = n4] {$5$};
\path (n1) edge node[above = 0.05 cm] {$(1,4)$} (n3);
\path (n3) edge node[above = 0.05 cm] {$(4,1)$} (n5);
\path (n1) edge [bend right = 30] node[below = 0.2 cm] {$(1,1)$} (n2);
\path (n2) edge [bend right = 30] node[below = 0.2 cm] {$(1,2)$} (n3);
\path (n3) edge [bend right = 30] node[below = 0.2 cm] {$(2,1)$} (n4);
\path (n4) edge [bend right = 30] node[below = 0.2 cm] {$(1,1)$} (n5);
%\path (n1) edge [bend left = 30, color=red] node[above = 0.2 cm, color=red] {$(3.8,6.8)$} (n5);
\end{tikzpicture}
\end{center}
\caption{A weighted ($d=2$) directed graph.}
\label{fig:example_graph}
\end{figure}

The solution set is shown in Table \ref{table:pareto_paths}; note that all the paths are supported solutions, since we have 
\begin{align}
\forall j,k \in \{1,\dots,4\}: \sum_{i=1}^{d} w_i(p_j) =  \sum_{i=1}^{d} w_i(p_k) \quad .
\end{align}

\begin{table*}
\begin{center}
    \begin{tabular}{| l | c |}
    \hline
    \textbf{Path} & \textbf{Weight vector} \\ \hline \hline
    $p_1 = \langle 1,2,3,4,5 \rangle$ & (5,5) \\ \hline
    $p_2 = \langle 1,3,4,5\rangle$  & (4,6) \\ \hline
    $p_3 = \langle 1,2,3,5\rangle$  & (3,7) \\ \hline
    $p_4 = \langle 1,3,5\rangle$ & (2,8) \\ \hline
    %$p_5$: 1-5 & (3.8,6.8) \\ \hline
    \end{tabular}
\end{center}
\caption{Pareto-optimal paths from node 1 to node 2 in the graph of Fig.~\ref{fig:example_graph}}
\label{table:pareto_paths}
\end{table*}

We extend the example by adding an arc $(1,5)$ with weight vector $(3.8,6.8)$.
Note that now, in addition to the previous solutions, the path $p_5 = \langle 1,5 \rangle $ is a Pareto-optimal path.
However, we have 
\begin{align}
\forall j \in \{1,\dots,4\}: \sum_{i=1}^{d} w_i(p_j) < \sum_{i=1}^{d} w_i(p_5) \quad ,
\end{align}
i.e., $p_5$ is not on the convex hull an thus called an \emph{unsupported non-dominated solution}, as depicted in Fig. \ref{fig:unsupported_solution}.
% approximate solution set
Thus, an \emph{approximate solution set} provides a ``reasonable'' set of non-dominated paths, but is not necessarily complete.

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[xlabel=$w_1$, ylabel=$w_2$, nodes near coords, enlargelimits=0.2]
	\addplot[color=blue, mark=*, point meta=explicit symbolic] 
	coordinates {
		(5,5) [$p_1$]
		(4,6) [$p_2$]
		(3,7) [$p_3$]
		(2,8) [$p_4$]
	};
	\addplot[color=red, mark=*, point meta=explicit symbolic] 
	coordinates {
		(3.8,6.8) [$p_5$]
	};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{An unsupported non-dominated solution: $p_5$}
\label{fig:unsupported_solution}
\end{figure}

\begin{comment}
\subsubsection*{Mutligraphs} \label{sec:shortest_path:defs:multigraph}
A multigraph is a graph that may contain parallel edges, that is, multiple edges may connect the same two nodes. 
Formally, a directed multigraph $G=(V,A)$ consists of a finite set of nodes $V = \{v_1, v_2,\dots,v_n \}$ and a finite multiset\footnote{In contrast to a set, a multiset may contain the same element multiple times.} of arcs and a function $f$, 
\begin{align}
f: A &\rightarrow (u,v) \text{ where } u,v \in V \quad .
\end{align}

The definition of weighted multigraphs is analogous to the definition of weighted graphs given in Eq.~\ref{eq:msp_weighted}. 

% our algorithm works on multigraphs too!
\todo{This should go somewhere else - it certainly does not belong to the formal definitions}
We note that in contrast to the algorithms mentioned in Section \ref{sec:related}, our algorithm works on multigraphs as well and provide a comparative performance evaluation in Section.
\end{comment}


\section{A Pheet-based MSP algorithm} \label{sec:msp_algo}
% what we do in this section
In this section, we present our algorithm for the MSP.
% Problem setting
We want to compute an exact solution of the MSP for directed graphs with multi-dimensional weight vectors (as defined in Section \ref{sec:shortest_path:defs:msp}). 
% Intro to the algorithm: generalization of Dijkstra
The algorithm follows the principles of Dijkstra's algorithm: we generate a set of candidates and expand the most promising first. The problem of finding intermediate paths that are Pareto-optimal is delegated to a priority queue. 
% we have a set of solutions for each node
Note that due to the multi-dimensional weight vectors, we have a set of optima for each node (see Section \ref{sec:pareto_optima}).
% The algorithm is label-correcting (no strict sequence)
In contrast to Dijkstra's algorithm, ours is a \emph{label correcting} algorithm.
The weight vectors are only partially ordered and thus there can be multiple Pareto-optimal candidates that may be expanded in arbitrary order. 
However, even though non of these candidates dominates any of the others, the partial solutions created by expanding them might do so.

% Pareto set
Each node $v$ maintains a \emph{Pareto set} to store the Pareto-optimal paths to the node $v$ that have been found so far. 
We denote the set of all Pareto sets by $S_p$ and a specific Pareto set attached to a node $v$ by $S_p[v]$.

% Pseudo code
A high level description of the algorithm is shown in Algorithm \ref{algo:abstractMSP}; note its similarity to Dijkstra's algorithm.
% weight vector <-> priority vector
As with Dijkstra's algorithm, paths with minimal weight should be expanded first, i.e., paths with minimal weight have the highest priority. 
The procedure for expanding a candidate path is described in detail in Algorithm \ref{algo:abstractExpand}. 

% Abstract description of the algorithm
\begin{algorithm}
\caption{Pseudo code for our MSP algorithm}
\label{algo:abstractMSP}
\begin{algorithmic}[1]
\Require{Graph $G$, start node $s$}
\ForAll{$v \in G$}
  \State $S_p[v] \gets \{\}$ \Comment{Initialize the Pareto-set for each node}
\EndFor
\State $S_p[s] \gets \{ \vect{0} \} $ \Comment{Shortest path from $s$ to $s$ is the null-vector}
\State $U \gets \{\langle s \rangle\}$ \Comment{$U$ is a priority queue containing paths that need to be expanded further}
\While{$U$ is not empty}
  \State Take a highest-priority path $p$ from $U$ 
  \State \Call{expand}{$p$}
\EndWhile
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Pseudo code for expanding a candidate path}
\label{algo:abstractExpand}
\begin{algorithmic}[1]
\Require{$S_p[v]$: Set of Pareto-optimal paths to node $v$ ($\forall v\in V$)}
\Require{$U$: Set of paths that need to be explored further}
\Require{$p=\langle s, \dots, v \rangle$ is a path in Graph $G$ from start node $s$ to $v$}
\Function{expand}{Path $p$}
\ForAll{$w \in p$.head.neighbors} 
  \State $p' \gets \cP{p}{w}$	\Comment{Generate candidate}
  \If{$\nexists x \in S_p[w] \text{ s.t.\ } x \less p'$} 
  \Statex \Comment{Candidate is not dominated by any path in the Pareto-set}
    \ForAll{$y\in S_p[w]$} \Comment{Remove any path dominated by $p'$}
      \If{$p' \less y$}
	\State $S_p[w] \gets S_p[w] \setminus y$
      \EndIf
    \EndFor
    \State $S_p[w] \gets S_p[w] \cup p'$ \Comment{Add $p'$ to the Pareto-set for node $w$}
    \State $U \gets U \cup p'$ \Comment{$p'$ needs to be explored further}
  \EndIf
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Implementation} \label{sec:msp_algo:implementation}
% Connection to Pheet
The implementation relies on the Pheet task scheduling framework for parallelization and, in particular, on Pheet's concept of \emph{priority task scheduling} (Section \ref{sec:pheet:task_priorites}), which allows to combine the work pool of the scheduler with the priority queue used by the algorithm into one data structure: a priority-aware task scheduler.
% Each path to expand is a task
The algorithm can be parallelized in an intuitive way by wrapping each candidate path into a task. 
Note that expanding several paths can be done independently, except for updates to the Pareto-set of a given node.

% Works with different schedulers/task storages
\begin{code}[label=lst:ParetoSet, caption=Pareto set operations., mathescape]
class ParetoSet {
  /* Insert $\color{OliveGreen} p = \langle s,\dots, v\rangle$ into the Pareto set at node $\color{OliveGreen} v$. Any path $\color{OliveGreen} p'$ in that set dominated by $\color{OliveGreen} p$ is marked dominated and removed from the set. If $\color{OliveGreen} p$ is dominated by a path $\color{OliveGreen} p'$ already in the set, $\color{OliveGreen} p$ is marked dominated and is not inserted into the set. Returns true if $\color{OliveGreen} p$ was inserted into the Pareto set. */ 
  bool insert(Path p);
}
\end{code}

We designed the implementation s.t.~it may easily employ different task scheduler implementations without any changes to the algorithm itself. 
Within this framework, we only have to provide implementations of 
\begin{inparaenum}[(i)]
\item the task and 
\item a scheduling strategy.
\end{inparaenum}
% Pareto set
For the implementation, we extend the Pareto-set (Lst.\ \ref{lst:ParetoSet}) by the \texttt{insert} operation to provide some of the functionality required in the procedure for expanding a path (Algorithm \ref{algo:abstractExpand}).
In particular, \texttt{insert}, given a path $p = \langle s,\dots,v \rangle$, removes all paths at node $v$ which are dominated by $p$; if however, $p$ is dominated by some path $p'$ in the Pareto-set for node $v$, $p$ is not added to the Pareto-set but is instead marked as dominated itself. 
The operation returns true if and only if $p$ was added to the Pareto-set.
Note that for any path $p$, only one of the following two alternatives may occur:
\begin{itemize}
\item $p$ is added to the Pareto-set and some paths $p'$ that are dominated by $p$ are marked dominated and removed from the set.
\item $p$ itself is dominated by some path $p'$ from the Pareto-set and is thus marked dominated and not added to the set.
\end{itemize} 
In other words, it cannot be the case that $p$ is marked dominated, but another path $p'$ already in the Pareto-set was removed from it because it is dominated by $p$. 
To see this, assume that the Pareto-set contains two paths $p_1$ and $p_2$, where $p_1$ is dominated by $p$ and $p_2$ dominates $p$. 
The \texttt{insert} operation would first remove $p_1$ since $p \less p_1$ and then mark $p$ dominated since $p_2 \less p$. 
However, due to transitivity of the dominance relation, this implies $p_2 \less p_1$. 

% Pareto set implementation
A naive implementation only needs to scan the Pareto-set either until it finds a path $p'$ that dominates $p$ or remove all paths dominated by $p$.

\begin{code}[caption=MSP Task, label=lst:msptask, mathescape]
class MSPTask : SchedulerTask {
public:

  MSPTask(Path p, ParetoSets Sp) //Constructor. Save parameters.
    : p(p), Sp(Sp) { }
  
  void execute() {
    /* The path this task was spawned for might be obsolete. */
    if(p.dominated) return;  
    /* Generate new candidates and add them to the global Pareto set */
    for (Arc a : p.head().outgoing_edges()) {
      Path candidate = p.step(e);
      if(Sp.insert(candidate)) {
        /* Spawn a new task for each candidate path that was added to the Pareto set */
        spawn_task(p, Sp, strategy);
      }
    }
  }
private:
  Path p;
  ParetoSets Sp;
}
\end{code}

An implementation of the Pheet-based MSP algorithm is given in Lst.\ \ref{lst:msptask}. 
The \texttt{spawn\_task} function (line 15) is generic and allows tasks to be spawned with different strategies.
The scheduler uses the strategy -- that is to be defined by the application -- to determine the order of tasks.
Furthermore, tasks may be marked dead via their associated strategy in case they were spawned speculatively and need not be executed anymore.
We provide two strategies that are based the generic \texttt{Strategy} class (Lst.\ \ref{lst:strategy})
\begin{itemize}
\item The \texttt{L1Strategy} (Lst.\ \ref{lst:LinearCombinationStrategy} assumes that a total order relation can be defined on the set of priority vectors. As Section \ref{sec:shortest_path:approx} shows, the $\leq_{sum}$ relation based on the $LÂ¹$ norm constitutes such a relation for integer vectors (which, however, is not applicable in general).
\item The \texttt{ParetoStrategy} (Lst.\ \ref{lst:ParetoStrategy}) allows for general (i.e., non-integer) priority vectors and does not rely on a total order relation.
Given two priority vectors, it offers function to check whether one dominates the other as well to compare them w.r.t.\ a given dimension.
\end{itemize}


\begin{code}[label=lst:strategy, caption=Interface of the Strategy class, mathescape]
class ParetoStrategy {

  void initialize(PriorityVector pv);
    
  /* Return true if the priority vector of this strategy has higher or equal priority than other. */
  bool prioritize(PriorityVector other);
    
  /* Return true if the associated task is marked as dead; i.e., if it need not be executed anymore. */
  bool dead_task();
}
\end{code}

\begin{code}[label=lst:LinearCombinationStrategy, caption=LinearCombinationStrategy, mathescape]
class ParetoStrategy {
    
  /* Return true if the priority vector of this strategy has higher or equal priority than other w.r.t. to the total order relation $\leq_{sum}$, i.e., the $L^1$ norm. */
  bool prioritize(PriorityVector other) {
    return sum(pv) <= sum(other);
  }
}
\end{code}


\begin{code}[label=lst:ParetoStrategy, caption=Interface of ParetoStrategy, mathescape]
class ParetoStrategy {
  /* Return true if the priority vector of this strategy has higher or equal priority than other w.r.t. the dominance relation, i.e., if other does not dominate the priority vector of this strategy. */
  bool prioritize(PriorityVector other);
  
  /* Return true if the priority vector of this strategy at dimension d has less priority than v. */
  bool less_priority(Dimension d, Value v);
  
 /* Return true if the priority vector of this strategy at dimension d has higher priority than v. */
  bool higher_priority(Dimension d, Value v);
}
\end{code}

When a new task is spawned with the \texttt{ParetoStrategy}, the \texttt{ParetoTaskStorage}, an implementation of our PPQ (Section \ref{sec:ppq}), is used by the scheduler. 
Pheet's \texttt{LSMLocalityTaskStorage} (see \cite{Wimmer14}) is used for tasks spawned with the \texttt{L1Strategy} strategy.
We note here that Pheet provides several more priority queue implementations (or task storages in Pheet terminology) that the scheduler can employ. 
However, the \texttt{LSMLocalityTaskStorage} priority queue is the most suitable for a comparison with our PPQ implementation since it too observes purely local ordering guarantees and is also based on log-structured merge-lists.

\section{Performance evaluation} \label{sec:evaluation}
We compare the performance of our PPQ to Pheet'S LSM priority queue via the MSP algorithm.
% Mars hardware
All experiments were run on a shared memory system nicknamed \emph{Mars}, an Intel Xeon based system with the properties listed in Table \ref{table:mars}.
\begin{table*}
\begin{center}
    \begin{tabular}{| l | l |}
    \hline
    CPU model & Intel Xeon E7-8850 \\ \hline
    Number of cores & 80 (8 nodes with 10 cores each) \\ \hline
    CPU clock & 2.00 GHz \\ \hline
    L1i & 32 KB \\ \hline
    L1d & 32 KB \\ \hline
    L2 & 256 KB \\ \hline
    L3 & 24576 KB \\ \hline
    Main memory & 1 TB \\ \hline    
    \end{tabular}
    \caption{Hardware configuration of Mars}
    \label{table:mars}
\end{center}
\end{table*}
% compiler 
Pheet was compiled using \texttt{gcc 4.9.1} with the the \texttt{-O3} flag to allow standard compiler optimizations.
All experiments were repeated 20 times and the graphs show the mean values.

\subsubsection*{Test instances} \label{sec:eval:test_instances}
% test graphs
Input instances were created by our own graph generator \footnote{\texttt{PHEET\_HOME/test/msp/lib/Graph/Generator/main.h}}, which generates a random connected digraph with the following arguments:
\begin{compactitem}
\item \verb|n|: Number of nodes.
\item \verb|m|: Number of arcs.
\item \verb|d|: Degree of the weight vectors.
\item \verb|w|: Upper limit for all dimensions of the weight vectors.
\item \verb|r|: Random seed value.
\end{compactitem}
If the option \verb|-p| is given, a multigraph will be generated. 
Otherwise, the generated graph will contain no parallel edges.

The generator first generates a random tree to ensure that the graph is connected and then inserts additional arcs. 
These are selected randomly from the set of $n(n-1)$ arcs of a complete graph minus the $(n-1)$ arcs chosen for the tree. 

\setkeys{Gin}{width=1\textwidth}
\subsection{Results} \label{sec:results}
We compare the execution time of the Pheet-based MSP algorithm with the two scheduler variants described in Section \ref{sec:msp_algo:implementation}.
Fig.\ \ref{fig:plot1} and Fig.\ \ref{fig:plot2} indicate that the problem scales scales reasonably for up to 80 processors when using our PPQ and it is slower than the LSM variant by roughly a factor of two.
However, the  LSM variant uses a total ordering relation (e.g., the $L^1$ norm) for ordering tasks, which is not applicable to priority vectors of a more general form, while the PPQ does not rely on such an ordering and we claim that it can be worthwhile trade-off.

\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE, width=9, height=4>>=
plotOverCpus(
"../benchmarks/mars/g_5000_30000_3_10000_42/s2pareto.dat",
"../benchmarks/mars/g_5000_30000_3_10000_42/s2lsm.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Execution time of MSP on a graph with 5000 nodes, 30000 edges and 3 objectives.}
\label{fig:plot1}
\end{figure}


\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE, width=9, height=4>>=
plotOverCpus(
"../benchmarks/mars/g_1000_3000_5_10000_42/s2pareto.dat",
"../benchmarks/mars/g_1000_3000_5_10000_42/s2lsm.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Execution time of MSP on a graph with 1000 nodes, 3000 edges and 5 objectives.}
\label{fig:plot2}
\end{figure}


\section{Conclusion} \label{sec:conclusion}
Task-based priority scheduling is an established mechanism for scheduling work for highly parallel shared memory systems.
We extended this concept by allowing multi-dimensional priority vectors for tasks and discussing how and under which circumstances multi-dimensional priorities may be reduced to scalar priority values, as well as the limitations of this approach. 
An efficient implementation of a concurrent priority queue that cam directly work with priority vectors (called Pareto priority queue, or PPQ) is required e.g., for the multi-criteria shortest path problem, which we used as an example application for the PPQ and as a benchmark. 
A detailed description and analysis of the PPQ was given, which shows that both the \texttt{push} and \texttt{pop} operation require $O(\log^2 n)$ amortized time. 
The performance of a Pheet-based implementation was compared to another priority queue (called LSM) already available in Pheet, which relies on a total ordering of the priority vectors (The $L^1$ norm is such a total order relation for integer vectors).
The PPQ scales well to as many as 80 processes but is slower by a factor of 2 when compared to the other priority queue.
In our option, this is a reasonable trade-off for being able to directly handle general multi-dimensional priority vectors, while to the LSM relies on the $L^1$ norm and is thus restricted to less general priority vectors, which it can handle only indirectly. 

For the analyis, some assumptions had to be made which we plan to rmeove in future work. 
Furthermore, a more rigid performance evaluation that does not depend on a specific example application is required.

\section{Acknowledgements} \label{sec:ack}
I sincerely want to thank my supervisor Martin Wimmer for providing this interesting and challenging topic as well as guidance and support throughout the duration of the project, as well as my co-supervisor Jesper Larsson Tr\"aff for valuable feedback.
Some work, which is mostly covered by Section \ref{sec:msp_algo}, was done jointly with Jakob Gruber and I highly appreciate his contributions.
Furthermore, I am especially grateful to all members of the ``Parallel Computing'' research group at the Vienna University of Technology for providing such a motivating and yet comfortable working environment.

%bibliography
\bibliographystyle{acm}
\bibliography{sources} 

\end{document}
