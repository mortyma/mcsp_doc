\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{cite}
%page boarders
\usepackage[vmargin=3cm, hmargin=3cm]{geometry}
\usepackage{float}
\floatstyle{plaintop}
\usepackage{paralist} %compactitem
\usepackage{fixltx2e} %some latex fixes
\usepackage{microtype} %Subliminal refinements towards typographical perfection
\setlength{\emergencystretch}{2em}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs} %Support use of the Raph Smithâ€™s Formal Script font in mathematics
\usepackage[sc]{mathpazo} %mathematical fonts
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage[margin=10pt, font=small, labelfont=bf]{caption}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{color}
\floatstyle{plaintop}
\usepackage{float}
\restylefloat{table}
\usepackage {tikz}
\usetikzlibrary{calc,shapes.multipart,chains,arrows, positioning}
\usepackage{pgfplots}

% a nice not implies
\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}
  
% todos  
\newcommand\todo[1]{\textcolor{red}{TODO: #1}}

% number equations s.t. they can be referenced easily
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% within an align* environment, number and label the equation. #1 is the label text to be appended to eqn:
\newcommand\neqn[1]{\numberthis\label{eq:#1}}

% theorem, lemma, ...
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}

% math stuff
% vectors
\newcommand{\vect}[1]{\vec{#1}}

% dominance relation
\newcommand{\dom}{\ensuremath{\prec}}
% less relation
\newcommand{\less}{\ensuremath{\prec}}
\newcommand{\lesseq}{\ensuremath{\preceq}}
\newcommand{\moreeq}{\ensuremath{\succeq}}

% total order relation
\newcommand{\tor}{\ensuremath{\leq_R}}
\newcommand{\osum}{\ensuremath{\leq_{\text{sum}}}}
\newcommand{\olex}{\ensuremath{\leq_{\text{lex}}}}

% sets
\newcommand{\sC}{\ensuremath{S_{\text{c}}}}
\newcommand{\sA}{\ensuremath{S_{\text{a}}}}
\newcommand{\sR}{\ensuremath{S_{\text{r}}}}

% concatenation of paths
\newcommand{\cP}[2]{\ensuremath{#1;#2}}

% style of listings
\definecolor{Gray}{gray}{0.5}
\definecolor{OliveGreen}{cmyk}{0.64,0,0.95,0.40}

   \lstset{
      language=C++,
      basicstyle=\ttfamily\footnotesize,
      keywordstyle=\color{blue},
      commentstyle=\color{OliveGreen},
      breaklines=true,
      breakatwhitespace=false,
      showspaces=false,
      showtabs=false,
      numbers=left,
      frame=single,
      captionpos=t,
  }

\lstnewenvironment{code}[1][]%
{
   \noindent
   \minipage{\linewidth} 
   \vspace{0.5\baselineskip}
   \lstset{
      #1
  }
  }
   %\lstset{basicstyle=\ttfamily\footnotesize,frame=single,#1}}
{\endminipage}

%\DeclareCaptionFormat{mylst}{\hrule#1#2#3}
%\captionsetup[code]{format=mylst,labelfont=bf,singlelinecheck=off,labelsep=space}

% 
\title{A multi-criteria priority queue for the Pheet task scheduling framework\\
\normalsize Technical report for \\``Project in Software Engineering \& Internet Computing''
}
\author{Martin Kalany, 0825673\\
Vienna University of Technology}

\begin{document}
\maketitle

%scale factor for graphs
\setkeys{Gin}{width=0.5\textwidth}

%R functions for plot generation
<<echo=FALSE, include=FALSE>>=
\SweaveInput{functions.Rnw}
@

%\pagebreak	

\begin{abstract}
\cite{Wimmer14} \todo{abstract}
\end{abstract}

\pagebreak
\tableofcontents
\pagebreak

\section{Introduction} \label{sec:intro}
% explain the idea of this work in a couple of sentences. This is for the informed reader.

% Introduce pheet
Pheet\footnote{\url{www.pheet.org}} is an open-source task scheduling framework for shared memory systems   
and is based on the task-parallel programming model, which allows a programmer to explicitly expose the parallelism of an application.
% The build-up for task priorities
Runtime systems based on the task-parallel model typically impose a non-adaptive, application independent execution order on the tasks where the scheduler is unaware of the preferred execution order of the tasks (The tasks are processed in, e.g., LIFO-order).
% The problem with Dijkstra's algorithm
While it has proven to be an efficient strategy for parallelization \cite{AroraBP01}, this approach has not been particularly useful for the parallelization of algorithms relying on priority queues (such as Dijkstra's famous algorithm for the single-source shortest path problem (SSSP) \cite{Dijkstra59}), since two work pools have to be maintained: 
One is used by the scheduling framework to maintain tasks that are yet to be executed; the other is required by the algorithm itself to determine the execution order of the tasks.

% Priority task scheduling
\emph{Priority task scheduling}, introduced by Wimmer et al.~\cite{WVTCT13}, addresses this problem by making the preferred execution order of tasks known to the scheduler. 
To achieve this, the task scheduler utilizes a concurrent priority queue which fulfills the ordering requirements imposed by the algorithm by assigning comparison-based priority values to the tasks.
Wimmer \cite{Wimmer14} further states that a scheduler based on the task priority scheduling model allows for an efficient parallel implementation of any algorithm relying on priority queues is possible.
They back up their claim by providing a parallel implementation of Dijkstra's algorithm on top of Pheet, which utilizes a scheduler based on the priority task scheduling model.

% Pareto priorities
In their dissertation \cite{Wimmer14}, they suggest to generalize the idea of priority task scheduling to multi-dimensional (or \emph{Pareto-}) priorities, which requires an efficient Pareto-priority queue implementation to be added to Pheet. 
A Pareto-priority queue ensures that the next task to be executed is a Pareto optimum, i.e., a task for which the partial solution is not dominated by the partial solution of any other task.
Such a Pareto-priority queue would allow for an efficient parallel implementation of e.g., algorithms solving the multi-criteria shortest path problem (MSP)\footnote{Sometimes also called multi-objective shortest path problem, multi-objective optimization or multi-objective search.} as discussed in \cite{Martins84}.
They outline a different advantage of using multi-dimensional priorities for task scheduling, which is independent of the algorithm that utilizes the scheduler:
Multi-dimensional priorities establish only a partial ordering on the tasks, which gives the scheduler more flexibility in terms of which task to execute next. 

% Our work
In this work, we investigate a potential implementation of such a multi-dimensional priority queue for the Pheet task scheduling framework, which is then used in turn for a parallel algorithm solving the MSP. 
% MSP is not our focus
We note here that the goal of our work is not to provide a better algorithm for the MSP; we investigate the potential of a multi-dimensional priority-aware task scheduler and use the MSP only as an example application.

% Performance evaluation
We make a comparison based performance evaluation by comparing the execution time of the MSP-algorithm employing different scheduler implementations: 

\todo{lsm/klsm/strategy: linear combination; vs.\ pareto-queue}


\subsubsection*{Outline} \label{sec:intro:outline}
In Section \ref{sec:pheet}, we provide a high-level introduction of Pheet-related concepts relevant for this work. 
Section \ref{sec:pareto_optima} gives a formal definition of Pareto-optimality, followed by a detailed presentation and analysis of our multi-criteria priority queue (Section \ref{sec:mcpq}).
We give a formal definition of the multi-criteria shortest path problem, which is used as an example application, in Section \ref{sec:shortest_path}, where we also discuss the hardness of the problem.
A detailed description of our Pheet-based algorithm solving the MSP will be given in Section \ref{sec:msp_algo}. 
This implementation is then used to compare the performance of our multi-criteria priority queue to other priority queue implementations already available in Pheet (Section \ref{sec:evaluation}).

\todo{Related work, Future work, conclusion,...}

\section{Pareto optima} \label{sec:pareto_optima}
% Informally describe the problem
Given a set of vectors, the \emph{maximal vector problem} is to find the subset of vectors s.t.\ each vector of the subset is not dominated by any vector of the set. 
One vector dominates another if each of its components is equal to or smaller than (w.r.t.\ some partial ordering of the vectors) the corresponding component of the other vector, and strictly smaller in at least one component.
Such a maximal element is called \emph{Pareto-optimal} and the set of maxima is called the \emph{Pareto-set}. 

% Example
Figure \ref{fig:pareto_optima} illustrates the concept of Pareto-optimality. 
A formal definition is provided in the following.

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[xlabel=$w_1$, ylabel=$w_2$, enlargelimits=0.2, nodes near coords]
\addplot[mark=*, point meta=explicit symbolic, color=blue]
  table {
x	y
1	7.8
1.5	7.2
1.6	7
2.2	6.5
2.2	6
2.9	5.5
3.8	4.5
4	4.4
4.7	4.3
5	4
};

\addplot[mark=*, only marks, point meta=explicit symbolic, color=black]
  table {
x	y
1.3	7.8
2.4	7
2.8	6
3.2	7.1
4	5
4	7.7
4.3	6
4.6	7.4
5	5


};

\addplot[mark=*, point meta=explicit symbolic, color=blue] 
coordinates {
	(2.9, 5.5) [A]
};
\draw[blue,dashed] (axis cs:2.9,5.5) -- (axis cs:0,5.5);
\draw[blue,dashed] (axis cs:2.9,5.5) -- (axis cs:2.9,0);

\addplot[mark=*, point meta=explicit symbolic, color=blue] 
coordinates {
	(3.8, 4.5) [B]
};
\draw[blue,dashed] (axis cs:3.8, 4.5) -- (axis cs:0,4.5);
\draw[blue,dashed] (axis cs:3.8, 4.5) -- (axis cs:3.8,0);

\addplot[mark=*, point meta=explicit symbolic, color=black] 
coordinates {
	(3.2, 6.2) [C]
};
\draw[black,dashed] (axis cs:3.2, 6.2) -- (axis cs:0,6.2);
\draw[black,dashed] (axis cs:3.2, 6.2) -- (axis cs:3.2,0);

\addplot[mark=*, point meta=explicit symbolic, color=black] 
coordinates {
	(3.5, 5.2) [D]
};
\draw[black,dashed] (axis cs:3.4, 5.2) -- (axis cs:0,5.2);
\draw[black,dashed] (axis cs:3.4, 5.2) -- (axis cs:3.4,0);

\end{axis}
\end{tikzpicture}
\end{center}
\caption{Example of a Pareto-set in two-dimensional space: The set of Pareto-optimal vectors are in blue, others in black. Note that point C is dominated by A in both dimensions ($w_1(A) < w_1(C)$ and $w_2(A) < w_2(C)$). D is dominated by A in dimension 1 and by B in dimension 2; either condition on its own suffices for D not being Pareto-optimal.}
\label{fig:pareto_optima}
\end{figure}

\subsubsection*{Formal definition}
% Formal problem definition
Let $U_1, U_2,\dots , U_d$ be totally ordered sets, each with the binary relations $\less_i$  and $=_i$, (e.g., $\mathbb{N}$ and the ``smaller than'' and ``equals'' relations) and let $V$ be a set of $d$-dimensional vectors in $U_1 \times U_2 \times \dots \times U_d$.
For any $\vect{v} \in V$, we donate the $i$-th component of $\vect{v}$ by $w_i(\vect{v})$.

% Domination relation
Let $\vect{x}, \vect{y} \in V$. 
We say that $\vect{x}$ \emph{dominates} $\vect{y}$ ($\vect{x} \dom \vect{y}$) -- or equivalently, $\vect{y}$ \emph{is dominated by} $\vect{x}$ -- if and only if
\begin{align}
&\forall i \in \{1,\dots, d\}: w_i(x) \lesseq_i w_i(y) \quad \text{and}\\
&\exists j \in \{1,\dots, d\}: w_j(x) \less_i w_j(y) \quad .
\end{align}
Note that $\vect{x} \dom \vect{y} $ implies $\vect{x} \neq \vect{y}$ and that the binary relation $\moreeq_i$ can easily be defined by the other two relations.
For $d\geq 2$, the dominance relation ($\dom$) defines a partial order on $V$, but not a total order.
Thus, there exist vectors $\vect{x}, \vect{y} \in V$, $\vect{x} \neq \vect{y}$ for which neither $\vect{x} \dom \vect{y}$ nor $\vect{y} \dom \vect{x}$ holds.

% Pareto optima
A vector $\vect{x} \in V$ is a \emph{Pareto-optimum} if and only if
\begin{align}
\nexists \vect{v} \in V: \vect{v} \dom \vect{x} \quad ,
\end{align}
i.e., there is no vector $\vect{v}$ that dominates $\vect{x}$.
%Pareto set
A \emph{Pareto-set} $P \subseteq V$ is the set of all Pareto-optima of $V$.

% Cite some work 
The Pareto-set can be computed in linear expexted-time, as shown in \cite{BentleyCL93}.

\section{Pheet} \label{sec:pheet}
% what we will do in this section
In this section, we provide a brief introduction to Pheet-specific concepts that are relevant for this technical report. 
% reference Martin's dissertation
A detailed introduction to Pheet, the concepts it is built on and the research based on it can be found in the dissertation of Martin Wimmer \cite{Wimmer14}. 

\subsubsection*{Philosophy} \label{sec:pheet:philosophy}
% How is pheet implemented, what are its goals?
Pheet is a task-parallel programming library with the goal of providing a simple-to-use framework for the quick parallelization of algorithms.
Its flexible plug-in architecture based on C++ template meta-programming allows for any component in the task scheduling system to be replaced by an alternative implementation.
Additionally, Pheet provides a set of micro-benchmarks (that aim to evaluate specific aspects of the scheduling framework) and fine grained performance counters (that provide detailed insight into the scheduler and supporting data structures), which makes the framework a suitable platform for the implementation and testing of new components such as schedulers or priority queues. 

%\subsection{Concepts} \label{sec:pheet:concepts}

\subsubsection*{Places} \label{sec:pheet:places}
A \emph{place} denotes a single worker thread in the Pheet scheduling system that is pinned to specific processor, that is, a place will not migrate to another processing unit during execution.
Each processor utilized by Pheet is assigned exactly one place, implying that a processor is uniquely identified by it, which may be very useful for the implementation of parallel algorithms.
Furthermore, a place allows to implement processor-local data structures, e.g., parts of a distributed priority queue or application specific data structures and data accessible by the tasks that are executed on the associated processor.

\subsubsection*{Task parallelism} \label{sec:pheet:task_parallel}
% work pool
Pheet is based on the \emph{task-parallel model}, where a task is a small portion of work that is to be executed sequentially.
As soon as a processor is ready to execute work, it retrieves a task from the \emph{work pool}, which contains all the tasks that have yet to be executed.
A task may itself create additional tasks that are then stored in the work pool.

A famous algorithm based on the work pool pattern is Dijkstra's algorithm \cite{Dijkstra59} for the shortest path problem:
The algorithm requires a priority queue (that may be implemented as e.g., a simple binary search tree or a Fibonacci heap). This priority queue is a specialized work pool.

Note that in a task parallel program, the work pool is not necessarily a centralized data structure; each place may maintain its own. 

\subsubsection*{Task priorities and strategy scheduling}  \label{sec:pheet:task_priorites}
% What does the scheduler do?
The work pool exposes available work to the \emph{scheduler}, which is responsible for devising a schedule, i.e., a mapping of tasks to the processors.
% Global scheduling strategy
In standard task-parallel programming models, the scheduler typically employs a global policy that treats all tasks equally and thus independently of any task-specific properties.

% Task priorities 
Wimmer introduced the concept of \emph{task priorities} to make the scheduler aware of the preferred (relative) execution order of tasks. 
% Discrete priorities
One approach to handling priorities is to assign a discrete priority value (typically taken form a small set) to each task. 
% Comparison based prioritization
Another is a comparison based approach, where the programmer has to provide a comparator to the scheduler, that, given two tasks, decides which to execute first.

% Scheduling strategy
In Pheet, this comparator has to be implemented within the \emph{scheduling strategy} of a task.
A scheduling strategy is associated with a single task and may specify behavior that is depending on task specific criteria.
In contrast to a global scheduling policy, a scheduling strategy is associated with a single task and thus allows the programmer to influence the scheduler's behavior for a specific task.
% Dead tasks
An important application of this concept is the mechanism of \emph{dead tasks}, which allows to mark a spawned but not yet executed (or currently executing) task as obsolete. 
This mechanism is useful for speculative execution: A task may be spawned when it is likely that the work will of the task will have to be performed sometime in the future; when further calculations determine that the task need not be executed after all, the task can be marked dead via its associated strategy, thus instructing the scheduler not to execute the task.
Pheet handles dead tasks lazily, i.e., a dead task will not be dropped immediately but at a time it is convenient for the scheduler to do so, which allows for a more efficient implementation of the scheduler's data structures.


% Main part of the work
\section{Multi-criteria priority queue} \label{sec:mcpq}
% What do we do in this section
In this section, we present our multi-criteria priority queue (MCPQ) for Pheet. 

% General description
% What is the MCPQ
The goal of the MCPQ is to provide a concurrent priority queue that can then be used by Pheet's strategy scheduler (see Section \ref{sec:pheet:task_priorites}) to determine the execution order of tasks with multi-dimensional priorities.
The multi-dimensional priority of a task as a \emph{priority vector}, which may in general be a vector of the form described in Section \ref{sec:pareto_optima}.
We simply speak of a non-dominated or Pareto-optimal task when referring to task with a non-dominated priority vector.

% Outline
The first subsections will provide a high-level, somewhat abstract discussion of the MCPQ, in particular of the requirements and major design choices (Section \ref{sec:mcpq:definition}), the integration into Pheet (Section \ref{sec:mcpq:pheet_integration}) and a white-box description via its interfaces (Section \ref{sec:mcpq:interface}). 
\todo{Outline next subsections}

\subsection{Definition and requirements} \label{sec:mcpq:definition}
% Abstract datastructure description
On an abstract level, the MCPQ is an \emph{ordered container} (as defined by Wimmer \cite{Wimmer14}), i.e., a bag-like data structure offering the following operations:
\begin{itemize}
\item \verb|push| adds an item\footnote{The ``item'' will be an atomic reference to a task.} to the container.
\item \verb|pop| returns an item that was previously added and removes it from the container. The order in which items are returned is determined by their priority.
\end{itemize}

% MCPQ used to store tasks; 
The MCPQ is used to store a set of tasks that are ready to execute.
% Priorities and partial solutions
The priority of a task reflects the potential quality of the (partial) solutions it will produce upon execution.
By using multi-dimensional priority vectors, the quality of a solution may be measured by several, possibly conflicting metrics\footnote{We refer to our algorithm for the multi-criteria shortest path problem (definition see Section \ref{sec:shortest_path:defs:msp}; algorithm see Section \ref{sec:msp_algo}) for a concrete application of this rather abstract concept.}. 
% Priority of task == desirability of execution
Obviously, it is desirable to execute the tasks that will produce (partial) solutions with the highest potential quality (i.e., tasks with highest priority) before other tasks.
% Pareto
Since each task is associated with a multi-dimensional priority vector, the tasks with highest priorities are the ones with a Pareto-optimal\footnote{A formal definition and detailed explanation of Pareto-optimality can be found in Section \ref{sec:pareto_optima}.} priority vector (w.r.t.\ the set of priority vectors of tasks \emph{currently} managed by the queue).
We can restate the semantics of the \verb|pop|-operation as follows: \verb|pop| returns a ready-task with a Pareto-optimal priority vector.

% concurrent DS
A priority queue for Pheet needs to be able to handle \emph{concurrent} accesses by multiple threads since each thread will fetch tasks to execute from the queue and insert newly generated tasks into the queue.

\subsubsection*{Ordering requirements} \label{sec:mcpq:ordering}
% ordering semantics  
The semantic definition of the \verb|pop| operation as given above requires that \emph{all} threads agree on the set of tasks with Pareto-optimal priority.
This implies that all the \verb|push| and \verb|pop| operations executed by different threads need to appear to take affect in the same order for each thread, i.e., they need to be linearizable w.r.t.\ all the threads.
While these \emph{global ordering} semantics match the intuitive expectation, they also pose a (potentially big) performance bottleneck. 
Accordingly, our priority queue implementation does not adhere to global ordering semantics; instead, \emph{purely local ordering} semantics are employed: Each thread maintains its own priority queue on which all the \verb|push| and \verb|pop| operations are executed.
Thus, a task returned by \verb|pop| has a Pareto-optimal priority only w.r.t.\ the tasks in the same local priority queue, while the priority queue of another thread might contain a task whose priority vector dominates the one of the task returned by \verb|pop|.
Due to the local ordering semantics, a thread might execute unnecessary additional work (since the solutions created by the task will be of much lower quality than the ones created by task with a dominating priority vector). 
However, this negative effect is countered by increased parallelism and less overhead due to synchronization.

\subsubsection*{Spying} \label{sec:mcpq:spying}
% Spying
When a thread does not have any more ready-tasks in its local priority queue, it will try to \emph{spy} some from the priority queue of another thread. 
A thread spies ready-tasks from another priority queue by copying references to the tasks to its own, thread-local priority queue.
Note that the tasks is then shared by multiple threads.
Once a thread pops a task it informs all the other threads that the task is already being processed and thus should not be executed by other threads anymore; this is done by marking a task as \emph{taken}.
The operations of the spying thread are linearized with the ones of all the other threads on the same priority queue.

\subsection{Description and analysis} \label{sec:mcpq:description}
% Purely local ordering semantics
Our MCPQ observes only local ordering semantics; thus, each thread may maintain its own priority queue, which however has to allow other threads to spy on it.

%Parts
% Concurrent and local part
The data structure is split into a part that may be accessed by several threads concurrently and a second part -- built on top of the first -- that may be accessed by the owning thread only.
% Virtual array
For the concurrent part , we assume a (potentially) infinite, thread-safe array-like data structure with linear (w.r.t.\ the capacity $n$ of data structure) access time for a whole sequence of $m\leq n$ items, henceforth called the \emph{Virtual Array}.
That is, sequentially accessing $m$ items that are stored in a consecutive sequence takes time $O(n)$.
(This will be proved formally in Section \ref{sec:virtual_array}.)
The virtual array is used to hold all the tasks currently managed by the thread-local priority queue and allows other threads to scan it for tasks awaiting execution.
% Thread-local part
The thread-local part, which we call the \emph{Pareto task storage}, is based on the ideas of \emph{log-structured merge-lists\footnote{This idea is based on log-structured merge trees (\cite{Wimmer14}, Section 5.7).}} and \emph{partitioning}, explained in the following and depicted in Fig.\ \todo{A nice figure to illustrate those things}.

% Concepts for local part
\subsubsection{Log-structured merge-lists} \label{sec:mcpq:description:lsm}
%structure
The Virtual Array of size $n$ is subdivided into an ordered sequence of a logarithmic number of blocks, where the first block $b_0$ is of some constant size $s_0$ and any other block $b_i$ is $2^j$ ($j \in \mathbb{N}, j > 0$) times larger than its predecessor block.
Each block maintains a constant-size set of tasks whose priority is not dominated by any other task in that block (see Section \ref{sec:mcpq:description:partitioning} for a detailed description of how this is done).
A highest-priority item (i.e., a non-dominated item w.r.t.\ to all items currently in the queue) can then be found by scanning the set of non-dominated items of all the blocks and thus in logarithmic time\footnote{This will be shown formally in the remainder of this section.}.
More formally, we have $n_b = \lceil log \left(\frac{n}{s_0}\right)\rceil$ blocks s.t.\ 
\begin{align}
n \leq \sum_{i=0}^{n_b} s_i < 2n \quad .
\end{align}
Let $a$ be the binary representation\footnote{W.l.o.g., we assume that $s_0$ divides $n$ evenly.} of $\frac{n}{s_0}$. Then, the size of the $i$-th block is given as
\begin{align}
s_i &= 2^j s_0 \quad ,
\end{align}
where $j$ is the index of the $i$-th non-zero bit in $a$, i.e., $j$ satisfies the equation
\begin{align}
i = \sum_{k=0}^{j} a[k] \quad .
\end{align}
We will refer to a block of size $2^j s_0$ as a level $j$ block.

%adding and removing items: general
The number of items $n$ that have to be managed by the priority queue changes over time as new ones are added while the highest-priority items are removed from the queue.
%merging
To maintain the logarithmic number of blocks, we add an additional level $0$ block $b_\text{insert}$, which will receive all newly added items.
Once this block is full, it is \emph{merged} with block $b_0$ (which is also a level $0$ block) to create a new block of level $1$.
This merging operation is then continued recursively: If block $b_{i+1}$ has the same level as block $b_i$, merge them to a block of the next higher level.
Fig.\ \ref{fig:merging} illustrates an example merging operation.
\begin{figure}
\begin{center}
\label{fig:merging}
\caption{Merging}
\begin{tikzpicture}
\end{tikzpicture}
\end{center}
\end{figure}

% Overview of pop
The \verb|pop| operation performs a \verb|peek| on each block to obtain a set of tasks that are not dominated w.r.t.\ to the tasks in their respective blocks.
This set contains $O(\log n)$ tasks (one task from each block) and is scanned linearly to find a tasks that is not dominated by any other currently in the queue. 

%removed, dead and taken items
It remains to discuss how items that have been removed from the queue are handled. 
This requires some understanding of how a single block actually maintains its set of non-dominated tasks, which is why we defer this point until after the next subsection.

\subsubsection{Partitioning} \label{sec:mcpq:description:partitioning}
%Requirement
As outlined in the previous section, each block manages a set of tasks and is required to find a task with Pareto-optimal priority (w.r.t.\ the tasks in the block) in constant time.
This operation is called \verb|peek|; its semantics are similar to the ones of \verb|pop|, except that the returned task is only guaranteed not to be dominated by any task covered by the block. 
Furthermore, \verb|peek| does not remove 

% all operations on the Virtual array
We note here that a block always covers a range of tasks stored in the Virtual Array and may thus be viewed as a sub-array of it. 
Any operation concerning a task is thus directly executed on the Virtual Array data structure.

%Idea
A block partitions the tasks it holds by a pivot value, which is found by randomly\footnote{All random selections in this work are meant to be made \emph{uniformly and independently.}} selecting a task with priority vector $p$ within the range to be partitioned and selecting a random value $k$ in the range $[1,\dots,d]$, where $d$ is the cardinality of the priority vector. 
The pivot element $x$ is then taken to be $x = w_k(p)$ and our procedure (Algorithm \ref{algo:partition})  for in-place partitioning\footnote{In principle, one could take the whole priority vector for the pivot element. However, this might make the partitioning infeasible if the set of Pareto-optima is large. Furthermore, comparing two vectors is $d$ times more expensive than comparing two scalar values.} tasks with multi-dimensional priorities is similar to the well-known partitioning algorithm for scalar values.
% partitioning
\begin{algorithm}
\caption{Pseudo-code for partitioning a block}
\label{algo:partition}
\begin{algorithmic}[1]
\Require{Block $b$; $left,right \leq$ size of $b$; $left\leq right$}
\Require{$s_p$: desired maximum size of right-most partition}
\Function{Partition}{Block $b$, $left$, $right$}
\State Randomly select task $t$ in the range $b[left],\dots, b[right]$
\State Let $t.p$ be the priority vector of dimension $d$ associated with $t$
\State $k \gets$ random number in $[1,\dots,d]$
\State $x \gets w_k(t.p)$ \Comment{Value of $t.p$ at dimension $k$}
\While{$left < right$}
  \While{$left < right$ and (task $t = b[left]$, $w_k(t.p) \less_k x$)}
    \State $left = left + 1$
  \EndWhile
  \While{$left < right$ and (task $t = b[right]$, $w_k(t.p) \moreeq_k x$)}
    \State $right = right -1 $
  \EndWhile
  \If{$left < right$}
    \State Swap the tasks $b[left]$ and $b[right]$
  \EndIf
\EndWhile
\Statex \Comment{Check if task at $left$ belongs to left partition}
\If{$t = b[left]$, $w_k(t.p) \less_k x$} 
  $ left = left +1$
\EndIf
\Statex \Comment{Recursively partition the right part until it falls below the cut-off value}
\If{$left < right$ and $(right-left) > s_p$}
  \State \Call{Partition}{$b$,$left$,$right$}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}
Tasks with (at dimension $k$) less priority than the pivot element get sorted to the left, while tasks with higher or equal priority are sorted to the right.
We will call these two parts the left and the right partition and use a subscript to indicate which partitioning step created them (the number of partitioning steps is equal to the recursion depths of the procedure).
Note that tasks that get sorted into the right partition are guaranteed to not be dominated by the ones in the left (since they have a higher priority value in at least one dimension).
Nevertheless, the left partition might contain Pareto-optimal tasks, but this behavior is acceptable since the MCPQ is required to find but one Pareto-optimal task and not the whole set of Pareto-optimal tasks.
The right partition is then partitioned recursively until it is smaller than a given constant cut-off value $s_p$, the maximum size that we are aiming for the right partition.
Note that the left partition of each partitioning step is not processed further and that the right partition contains tasks that are not dominated by any task in any of the left partitions.
Figure \todo{show partitioning} illustrates this partitioning process.

% problems we have here
In the algorithm outlined above, several important details still need to be addressed:

\noindent
\textbf{Pivot generation}. As with other algorithms, e.g., Quicksort, it is crucial to select ``good'' pivot elements to avoid worst-case behavior. 
To get a reasonable pivot value (at least on expectation) we simply sample multiple times and then choose the median as our pivot element.  

\noindent
\textbf{Ensuring termination}. The algorithm as given above does not necessarily terminate.
Assume that all the tasks managed by a block have the same priority. 
Partitioning would sort all tasks into the right partition irregardless of which task is chosen as the pivot, since all of them have the same priority.
We call such an event a ``failed partitioning step''. 
Note that it may also occur if we choose the same pivot element as was used by the last partitioning step.
While in this case we can try to generate a new, different pivot element, the former case does not allow for further partitioning. 
We ensure termination by aborting the recursive procedure after a constant number of successive failed partitioning steps.
In this case, the size of the right-most partition will be larger than the size we are aiming for; nevertheless, it is guaranteed that the tasks it contains are not dominated by tasks that have been sorted to the left.

\noindent
\textbf{Handling dead, taken and removed tasks}. Once a tasks is popped, it has to be removed from the queue. 
Furthermore, tasks that have been marked dead or taken also need to be removed from the MCPQ. 
% active, non-active tasks
We will subsume all those tasks as ``non-active'', whereas at task still awaiting executing is called an active task.
% How a block handles dead tasks
Speaking informally, each block moves its non-active tasks to the end of the block and keeps them there.
Once the block gets merged with another block, the non-active tasks of both blocks are moved to the end of the block generated by the merge.
Thus, the non-active tasks gradually migrate to the end of the sequence of blocks, i.e., to the end of the virtual array.
Once they reach the very last block, the non-active tasks may be dropped from the priority queue. 

% Only cover blocks here
In the following, we will describe in detail how a block handles non-active tasks. 
All aspects concerning the sequence of blocks -- like merging blocks and the dropping of the non-active tasks -- are discussed in Section \todo{well, which?}.

% structure of a block
\begin{figure}
\begin{center}
\label{fig:block_structure}
\caption{Structure of a block after partitioning. From the left to the right, we have 1) several left partitions, one created by each partitioning step; 2) the right partition, containing tasks not dominated by any task in the left partitions; and 3) the section containing the non-active tasks of this block.
}
\begin{tikzpicture}
\end{tikzpicture}
\end{center}
\end{figure}

% Extended algorithm, structure of a blocks
Fig.\ \ref{fig:block_structure} shows the structure of a block, after it was partitioned.

% extended algorithm
The partitioning procedure (Algorithm \ref{algo:partition}) is adapted as follows to handle non-active tasks:
\begin{itemize}
\item Only active tasks are sampled for the pivot element.
\item An additional index, called $non\_active$, is used to point to the start of the section containing the non-active elements.
Initially, we have $non\_active = right$.
\item If a non-active task is encountered during the scan for the next task to swap, it is swapped out to the section holding the non-active tasks. 
\end{itemize}

The adopted procedure is shown in Algorithm \ref{algo:partition_with_nonactive}.
An example illustrating the procedure is shown in Fig. \ref{fig:partition_with_nonactive}.

\begin{algorithm}
\caption{Pseudo-code for partitioning with non-active tasks}
\label{algo:partition_with_nonactive}
\begin{algorithmic}[1]
\Function{Partition}{Block $b$, $left$, $right$}
\Statex \Comment{Generate the pivot element as above, but without sampling non-active tasks.}
\State $non\_active \gets right$ 
\While{$left < right$}
  \While{$left < right$}
    \If{$left$ is an active task}
      \If{$left \less_k x$} \Comment{If the task at $left$ has...}
	\State $left = left + 1$  \Comment{...less priority than the pivot, it stays,...}
      \Else \State break; \Comment{...otherwise, it is moved to the right partition.}
      \EndIf
    \Else \Comment{$left$ is a non-active task}
      \State $non\_active = non\_active - 1$; \Comment{Now points to an active task}
      \State swap the tasks $b[left]$ and $b[non\_active]$
      \If{$non_active == right$} $ right = right - 1 $;
      \EndIf
    \EndIf
  \EndWhile
  \While{$left < right$}
    \If{$right$ is an active task}
      \If{$right \moreeq_k x$} \Comment{If the task at $right$ has more or equal...}
	\State $right = right - 1$  \Comment{...priority as the pivot, it stays,...}
      \Else \State break; \Comment{...otherwise, it is moved to the left partition.}
      \EndIf
    \Else \Comment{$right$ is a non-active task}
      \State $non\_active = non\_active - 1$
      \If {$right == non\_active$} $right = right - 1$
      \Else
	\State Swap the tasks $b[right]$ and $b[dead]$;
      \EndIf
    \EndIf
  \EndWhile
  \If{$left < right$}
    \State Swap the tasks $b[left]$ and $b[right]$;
  \EndIf
\EndWhile
\Statex \Comment{Check if tasks at $left$ belongs to left partition (as above).}
\Statex \Comment{Partition recursively, if necessary (as above).}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{figure}
\begin{center}
\label{fig:partition_with_nonactive}
\caption{Example of a partition run with non-active tasks}
\begin{tikzpicture}
\end{tikzpicture}
\end{center}
\end{figure}



\subsubsection*{Analysis} \label{sec:mcpq:description:partition:analysis}

\subsubsection{Merging blocks} \label{sec:mcpq:description:merging}

\subsubsection{Dropping non-active tasks} \label{sec:mcpq:description:dropping}

\subsubsection{Spying} \label{sec:mcpq:description:spying}

\subsection{Integration in Pheet} \label{sec:mcpq:pheet_integration}
% Strategy scheduler 2
Pheet's \verb|StrategyScheduler2|, a \emph{two-level concurrent ordered container} (described in detail in \cite{Wimmer14}, Section 5.5) allows for a quick integration of our MCPQ into the scheduling framework.
% Base classes provided; pheet terminology
It offers base classes for alternative \emph{task storage}\footnote{In Pheet terminology, a priority queue managing tasks is called a task storage.} implementations.
% The classes we us.
In particular, to integrate the MCPQ into the Pheet scheduling framework, we will be extending the following classes:
\begin{inparaenum}
\item \verb|Strategy2BaseTaskStorageBaseItem|,
\item \verb|Strategy2BaseTaskStoragePlace| and
\item \verb|Strategy2BaseTaskStorage|.
\end{inparaenum}
We will give an intuitive understanding of their functionality required by our implementation together with a detailed description of the classes extending those base classes in the following. 
For a detailed description of the base classes and correctness proofs, we refer the reader to \cite{Wimmer14}.


\subsection{Implementation} \label{sec:mcpq:impl_analysis}
\todo{say that we proceed in a bottom-up manner}
\subsubsection*{Other stuff}
\todo{This does not really fit here}
Furthermore, such a \emph{distributed} implementation of the MCPQ avoids performance bottlenecks inherent to centralized data structures.

% lock- and wait-free
 To provide good progress guarantees, the MCPQ is implemented as a lock- and wait-free data structure. 
\todo{More here? or just refer to diss. p. 49? This last sentence doesn't fit anyway}



\subsubsection{Interfaces, overview}
% PriorityVector
We map the mathematical concept to an object as shown in Listing \ref{lst:priority_vector}, with \verb|PriorityValue| representing a value from a domain $U_i$ for $1 \leq i \leq d$.
We use this representation of a priority vector when referencing it in pseudo-code. 
\begin{code}[label=lst:priority_vector, caption=PriorityVector]
/* To be implemented by the application. */
class PriorityVector {

  /* Get the priority value at dimension d */
  PriorityValue priority_at(int d);
  
  /* Return true if this priority vector at dimension d has a priority value smaller than v */
  bool less_priority(int d, PriorityValue v);
  
  /* Return true if this priority vector at dimension d has a priority value greater than v */
  bool greater_priority(int d; PriorityValue v);
}
\end{code}

% A general, high level overview and interfaces
The class \verb|ParetoItem| (Listing \ref{lst:PTSItem}) is used to represent tasks managed by scheduler. It references an object of type \verb|SchedulerTask| (Listing \ref{lst:SchedulerTask})(which contains the code to be executed, i.e., the work the task is to perform) and an object implementing the \verb|ParetoStrategy|-interface (Listing \ref{lst:ParetoStrategy}) (which handles the priority of the task).

The \verb|ParetoItem| class has to guarantee that its \verb|take| operation returns the referenced task exactly once, even if multiple threads execute it concurrently. 
Although an item is at first only visible to the thread creating it (also called the \emph{owning} place), this may change after another thread performs a \verb|steal| operation on the priority queue of the owning place.
The base class \verb|BaseItem| provides some functionality for memory re-usage and serves as an interface for \verb|StrategyScheduler2|, which is oblivious of the task storage implementation.	

% ParetoItem
\begin{code}[label=lst:PTSItem, caption=Interface of ParetoItem]
class ParetoItem : BaseItem {

  /* The task to execute. */
  SchedulerTask task;
  
  /* The strategy associated with the task. */
  ParetoStrategy strategy;
  
  /* If false, no thread is processing this item yet. */
  atomic<bool> taken;
  
  /* Get the task referenced by this item if it is not taken. A task may be taken only once. */
  SchedulerTask take();
}
\end{code}

The \verb|SchedulerTask| and \verb|ParetoStrategy| classes have to be implemented by the application employing the scheduler, i.e., the application programmer.
While the former contains the code to be executed upon execution of the task, the latter provides access to the priority vector associated with a task.
Furthermore, the function \verb|priorizite|, given another \verb|ParetoStrategy| object, decides which of the two has higher priority. 
The \verb|dead_task| function is used by the scheduler to query whether the associated task may be dropped without being executed\footnote{This feature, called ``speculative execution'', is described in Section \ref{sec:pheet:task_priorites}}.
Both functions will be used by our task storage implementation to order tasks w.r.t.\ their associated priorities.

% Task
\begin{code}[label=lst:SchedulerTask, caption=Interface of SchedulerTask]
/* To be implemented by the application. The code in execute() will be run when the task is executed. */
class SchedulerTask {

  void execute();
}
\end{code}

% Strategy
\begin{code}[label=lst:ParetoStrategy, caption=Interface of ParetoStrategy]
/* To be implemented by the application. */
class  {

  void initialize(PriorityVector v);
  
  PriorityVector priority_vector();
  
  /* Return true if the priority vector of this strategy is not dominated by the priority vector of other. */
  bool prioritize(ParetoStrategy other);
  
  /* Return true if the associated task is marked as dead; i.e., if it needn't be executed anymore. */
  bool dead_task();
}
\end{code}
The central -- or global -- part of the MCQP is implemented in \verb|ParetoTaskStorage| (Listing \ref{lst:ParetoTaskStorage}), which is mostly used for rerouting \verb|pop| and \verb|steal| operations to the place-local parts of the MCPQ, implemented in \verb|ParetoTaskStoragePlace| (Listing \ref{lst:ParetoTaskStoragePlace}). 
% boundary
Notice that the \verb|pop| and \verb|steal| operations of both classes have an additional parameter called the boundary item. 
The \verb|StrategyScheduler2| maintains its own LIFO priority queue\footnote{This is necessary for some advanced features the base data structures provide. We do not require them, they are not introduced in this report.} and the boundary item would be the next item selected by \verb|pop|/\verb|steal| according to this order.
However, this default behavior may be overridden within certain limits: 
\begin{itemize}
\item The \verb|pop|/\verb|steal| operation may select other items with a \emph{higher} priority than the boundary item, thereby violating the LIFO order. Since we are using multi-dimensional priority vector, this means that the priority associated with the selected item may not be dominated by the priority of the task referenced by the boundary item.
\item The \verb|pop| operation must not return a \verb|SchedulerTask| if the boundary item was already taken by another thread in the meantime.
\end{itemize}

% TaskStoragePlace
\begin{code}[label=lst:ParetoTaskStoragePlace, caption=Interface of ParetoTaskStoragePlace]
/* The place-local part of the MCPQ. */
class ParetoTaskStoragePlace : Strategy2BaseTaskStoragePlace {

  /* Add the given item to the task storage. */
  void push(ParetoItem item);
  
  /* Get an item with locally non-dominated priority and remove it from the task storage. */
  SchedulerTask pop(ParetoItem boundary);
  
  /* Get an item (with locally non-dominated priority) by stealing it from another place. */
  SchedulerTask steal(ParetoItem boundary, ParetoTastStoragePlace other_place);
}
\end{code}

% TaskStorage
\begin{code}[label=lst:ParetoTaskStorage, caption=Implementation of ParetoTaskStorage]
/* The global part of the MCPQ. Calls to pop and steal are simply forwarded to the responsible places. */
class ParetoTaskStorage : Strategy2BaseTaskStorage {

  ParetoTaskStoragePlace[] places;
  
  SchedulerTask pop(ParetoItem boundary, int place_id) {
    return places[place_id].pop(boundary);
  }
  
  SchedulerTask steal(ParetoItem boundary, int place_id) {
    return places[place_id].steal(boundary);
  }
}
\end{code}


% Virtual Array
\subsubsection{Virtual array} \label{sec:virtual_array}
% Motivation
The \verb|VirtualArray| class (Listing \ref{lst:VirtualArray}) provides a thread-safe array of potentially unlimited size. 
It is used to relief other classes from certain aspects of memory management which can be tricky for shared data.
An instance of \verb|ParetoTaskStoragePlace| will store all the items it manages in the place-local \verb|VirtualArray|, which allows other places (i.e., threads) to scan the \verb|VirtualArray| and steal some of the items it holds. 
Thus, the \verb|VirtualArray| serves as a clear separation of the data structure into a part that is accessed by several threads and a part accessed only by the owning thread.

% Interface
\begin{code}[label=lst:VirtualArray, caption=Virtual array]
class VirtualArray {
public:
  \todo{should we show the VirtualArrayIterator here?}
  /* Access the item at position idx. */
  ParetoItem operator[](int idx);
  /* Increase the capacity by value. */
  void increase_capacity(int value);
  /* Decrease the capacity by value. */
  void decrease_capacity(int value);
}
\end{code}

% Implementation
It is implemented via a doubly-linked list of blocks (class \verb|VirtualArrayBlock|) of constant size $s_\text{v}$. 
This doubly-link list may be traversed in forward direction by all threads; but only the owning thread may traverse it backwards or alter its structure. 
To ensure thread-safety, references to the items in the array are stored as atomic pointers.
To provide for constant time access -- at least when traversing a consecutive range of elements -- we provide an iterator which allows constant time access to the item it currently references as well as to the next and previous items in the array.

%\subsubsection*{Capacity management}
% Memory management & usage
The capacity of the virtual array may be increased and decreased dynamically.
Let $c$ be the current capacity of the virtual array.
After a call to \verb|increase_capacity| with parameter \verb|value| returns, the virtual array may be accessed in the range $[0, c + \verb|value|]$. 
Since we provide access to the elements of the virtual array via iterators, it might occur that one thread wants to access an element via an iterator, while another thread already decreased the capacity of the virtual array s.t.\ the element is not in the valid range anymore. \todo{there should be a footnote to the section that makes it clear why this can happen, i.e., ParetoTaskStoragePlace}. 
To avoid accessing an invalid memory location in this situation, the operation \verb|decrease_capacity| merely marks an amount of items equal to \verb|value| as reusable.
That is, given an initial capacity $c$, the range $r_d = [c - \verb|value|, c]$ may still be accessed after the operation returns. 
However, that range will be reused for storing items once the capacity of the virtual array is increased again.
Note that a subsequent increase of the capacity might lead to items in the range $r_d$ to be overwritten. 
Thus, care must be taken to ensure that, at the time \verb|decrease_capacity| is called, no item in the range $r_d$ has to be accessed by any thread via the array anymore.
We will describe in Section \todo{which?} how this is ensured.
\todo{the capacity is only ever decreased if there are no more active items in the reduced range, i.e., all items in that range are either null, dead or taken. Another thread still scanning that range thus would not steal such items; a subsequent increase and refill would thus overwrite only non-active items!}


\begin{comment}
\section{Bits and pieces}
%Node
\todo{do we really need this? or can we assume that it is known/intuitively understandable?}

\begin{minipage}[c]{\linewidth}
\begin{lstlisting}[label=lst:Node, caption=Basic structure and operations of Node.]
class Node {
  /* All nodes adjacent to an outgoing arc of this Node. */
  Set<Node> neighbors();
}
\end{lstlisting}
\end{algorithm}

% Path
\begin{minipage}[c]{\linewidth}
\begin{lstlisting}[label=lst:Path, caption=Basic structure and operations of Path.]
class Path {
  Path step(Arc a);
  Node tail();
  Node head();
  Weight_Vector weight();
  bool dominated;
}
\end{lstlisting}
\end{algorithm}

% pareto priority queue idea
%\begin{comment}
 \cite{Wimmer14}

% What does the scheduler do?
The task parallel model is independent of any machine specifics such as the number of processors and the memory hierarchy.
It merely exposes the available work to the \emph{scheduler}, which is responsible for devising a schedule , i.e., a mapping of tasks to the processors, thereby determining which processor executes a task as well as the execution order of the tasks. 
The scheduler 

% Online/offline scheduling; what is used in Pheet and why
The schedulers developed for Pheet employ \emph{online scheduling}, where -- in contrast to \emph{offline scheduling} -- the schedule cannot be precomputed statically, since not all the required information is available before the execution.
This is due to e.g., new tasks being created during the execution and/or the nondeterministic execution time of tasks.

% Explain that we don't explain scheduling
Task scheduling itself is a vast and active area of research with many different variations in the problem setting and models.
Since we do not deal with any complex scheduling in this work, we refrain from giving a more elaborate introduction and refer the reader to the Wimmer's dissertation\cite{Wimmer14}, which contains several pointers to the literature.
% task parallelism
Additionally, the programmer may define dependencies between the tasks which ensure that a task is not executed before all required prerequisites are met; if, however, two tasks do not depend on each other, they may be executed in parallel (or sequentially in arbitrary order).
\todo{the above is quite general and does not really relate to the rest of the work or pheet.}

Furthermore, once a place starts executing a task, the task will stay at this place until its execution is finished. 

%This is not relevant
By uniquely identifying processors, places simplify the implementation of parellel algorithms and data structures.

Furthermore, places are employed to deal with aspects related to the memory hierarchy as well as locality. 
However, as those concepts are not considered in our contribution, we refer the interested reader to the work of Wimmer\cite{Wimmer14}.
\end{comment}

\section{Shortest path problems} \label{sec:shortest_path}
% Informally introduce shortest path
The problem of finding a shortest path between two nodes in a graph is a classical problem in computer science with numerous practical applications, such as finding the quickest route from location A to location B.
% Introduce the problem of msp
While the \emph{shortest-path problem} minimizes for a single criteria, e.g., the travel time, in practical applications one often wants to optimize multiple -- and possibly conflicting -- objectives:
When searching for a route from A to B, we may want to minimize the travel time as well as toll costs.
In such a setting, we usually cannot give a single best solution anymore. 
Instead, we can give several reasonable solutions: 
The fastest route will usually use highways, thus increasing the toll costs, while a slightly slower route may reduce the toll costs by avoiding highways. 
Problems like this can be modeled as \emph{multi-criteria shortest path} (MSP) problems, where we are interested in all optimal alternatives, i.e., in routes that are \emph{Pareto-optimal}. 
A route from A to B is Pareto-optimal if there is no other route with less or equal cost for each of the $d$ objectives.

%subsection{Definitions and notation} \label{sec:shortest_path:defs}
In the following, we provide definitions for variations of the shortest path problems considered in this report. 
The classic text book by Cormen et al.~\cite{CLRS01} provides a more detailed introduction to basic shortest path problems and algorithms.

% Define single-source shortest path
\subsection{The classical shortest path problem} \label{sec:shortest_path:defs:sssp}
We are given a weighted directed graph $G=(V,A)$, where $V= \{v_1, v_2, \dots, v_n\}$ is a finite set of nodes, $A \subseteq V \times V$ is a finite set of arcs (directed edges) and $c: A \rightarrow \mathbb{N}$ is the cost or weight function. 
We define the set of neighbors of a node $v$ as the nodes adjacent to an outgoing arc of $v$, i.e., $\{w \in V \mid \exists a\in A, a = (v,w) \}$.
W.l.o.g., we assume that $G$ is a connected graph.

Let $p = \langle v_0, a_1, v_1, a_2, \dots,a_k, v_k \rangle$ be a path\footnote{We will also write $p = \langle v_0, v_1, \dots,v_k \rangle$ when not dealing with multigraphs, since an arc $a_i$ is well defined by its two adjacent nodes.}from $v_0$ to $v_k$ in $G$.
Furthermore, let $P$ be the set of all paths and $P_{u,v}$ be the set of all paths from node $u$ to node $v$ in $G$.
We define the weight $w(p)$ of a path $p$ as 
\begin{align*}
w: P &\rightarrow \mathbb{N} \\
p &\mapsto w(p) = \sum_{i=1}^{k}c(a_i) \quad , \neqn{sssp_weighted}
\end{align*}
i.e., the sum of costs all the arcs in the path $p$.

% Variations: Single source, all-pairs,... shortest path
In a \emph{single-source shortest path problem (SSSP)}, we want to compute a shortest path w.r.t.~some weight function $w$ from a given source node $s \in V$ to each node $v \in V$.
Different variations of the problem exist, such as the \emph{single-destination}, the \emph{single-pair} or the \emph{all-pairs} shortest path problem. 
Note that an optimal algorithm for the SSSP can easily be turned into an optimal algorithm for the first two variants \cite{CLRS01}.
Thus, we will only deal with the SSSP in this report.

\subsection{Multi-criteria shortest path (MSP)} \label{sec:shortest_path:defs:msp}
% MSP
Multi-criteria shortest path problems \cite{Martins84} generalize shortest path problems w.r.t.~the weight function $c$, which is extended to $d$-dimensional vectors, i.e., $\vect{c}: A \rightarrow \mathbb{N}^d$ and
\begin{align*}
\vect{w}: P &\rightarrow \mathbb{N}^d \\
p &\mapsto \vect{w}(p) = (w_1(p), w_2(p), \dots , w_d(p)) \quad , \neqn{msp_weighted}
\end{align*}
where $w_j(p) = \sum_{i=1}^{k}c_j(a_i)$ $\forall j \in \{1,\dots,d\}$.

% Pareto optima
A path $p$ from node $v_1$ to node $v_2$ is a \emph{Pareto-optimal} or \emph{non-dominated} path if there is no path $q \in P$ from $v_1$ to $v_2$ s.t.~$q \dom p$.
Informally, we also call a Pareto-optimal path $p$ \emph{shortest path}.
Thus, for the MSP, the solution consists of a set of Pareto-optimal paths for each considered pair of nodes.


\subsubsection*{Applications} \label{sec:shortest_path:applications} 
% Applications
Apart from the problem of finding an optimal route in a road map as outlined above, the MSP problem has numerous outer applications, such as routing in multimedia networks \cite{ClimacoCP03}, route guidance \cite{JahnMS00} and curve approximation \cite{MehlhornZ00}.

\subsubsection*{Hardness and practicality} \label{sec:shortest_path:hardness}
% Hardness of the problem, reason for parallelization
% Note that the graph instances resulting from modeling practical problems are potentially very large. 
The crucial parameter for the complexity of the MSP is the total number of Pareto optima for all visited nodes. 
Since this number is exponential in $n$ in the worst case \cite{Hansen80}, the MSP is in general NP-hard.
Even for $d=2$, the decision problem whether there exists a path between two nodes whose length is below a given threshold is NP-hard \cite{GareyJ79}.
However, it was observed that the problem is efficiently tractable from a practical viewpoint for many practically relevant instances. 
The input data of practical applications tends to have certain characteristics, which lead to the set of Pareto optima for a vertex to be polynomially bounded \cite{Muller-HannemannW06}. 
It was shown that in applied scenarios this number may even be bounded by a small constant \cite{Muller-HannemannW01}. 
Thus, MSP can be solved efficiently for small $d$ and not too large graphs adhering certain characteristics.

% reason for parallelization
However, due to the potentially very large graph instances resulting from modeling e.g., road or railway networks, the computational cost is significant even if a small number of criteria is to be optimized.
Guerriero and Musmanno \cite{GuerrieroM01} thus suggest that parallel computing might help to design efficient solution methods. 

\subsubsection*{Approximate solutions} \label{sec:shortest_path:approx}
% heuristics
Due to the hardness of the MSP, heuristic methods are sometimes employed (see, e.g., \cite{BastMS03, Sonnier06, EhrgottG02}).
% weighted sum approach
A common approach is to define a total order relation $\tor$ on $P$ that allows for a more efficient computation of the Pareto-optima.
Such a relation $\tor$ has to satisfy the following properties\footnote{We denote the concatenation of two paths $p_1 \in P_{u,v}$, $p_2 \in P_{v,w}$ as $\cP{p_1}{p_2}$}
\begin{align}
\forall p,q \in P_{u,v}: p \dom q \implies p \tor q  \quad \text{Dominance} \\
\forall p \in P_{u,v}, \forall (v,w) \in A: p \tor \cP{p}{w} \quad \text{Monotonic}
\end{align}
Martins et al.~\cite{MartinsPRS07} show that the following relation satisfies these requirements:
\begin{align}
p \osum q \iff \sum_{i=1}^{d} w_i(p) \leq  \sum_{i=1}^{d} w_i(q) 
\end{align}
% supported/non-supported solutions
However, as Sonnier \cite{Sonnier06} points out, a problem with algorithms based on such a relation is that 
\begin{align}
p \osum q  \notimplies p \dom q \quad , \neqn{osum_problem}
\end{align}
i.e., only solutions that lie on the convex hull of the feasible region are found, which are called the supported solutions. 
In other words, there exist Pareto-optima which may not be found because of the property given in Eq.~\ref{eq:osum_problem}.

% graph example from Sonnier
To see this, consider the following example from Sonnier \cite{Sonnier06}.
Assume we are interested in the Pareto-optimal paths from node 1 to node 5 in the graph depicted in Fig.~\ref{fig:example_graph}.

\begin{figure}
\begin {center}
\begin {tikzpicture}[-latex, auto, node distance = 3 cm, on grid, semithick, state/.style = {circle, top color = white, draw, minimum width = 1 cm}]
\node[state] (n1) {$1$};
\node[state] (n2) [below right of = n1] {$2$};
\node[state] (n3) [above right of = n2] {$3$};
\node[state] (n4) [below right of = n3] {$4$};
\node[state] (n5) [above right of = n4] {$5$};
\path (n1) edge node[above = 0.05 cm] {$(1,4)$} (n3);
\path (n3) edge node[above = 0.05 cm] {$(4,1)$} (n5);
\path (n1) edge [bend right = 30] node[below = 0.2 cm] {$(1,1)$} (n2);
\path (n2) edge [bend right = 30] node[below = 0.2 cm] {$(1,2)$} (n3);
\path (n3) edge [bend right = 30] node[below = 0.2 cm] {$(2,1)$} (n4);
\path (n4) edge [bend right = 30] node[below = 0.2 cm] {$(1,1)$} (n5);
%\path (n1) edge [bend left = 30, color=red] node[above = 0.2 cm, color=red] {$(3.8,6.8)$} (n5);
\end{tikzpicture}
\end{center}
\caption{A weighted ($d=2$) directed graph.}
\label{fig:example_graph}
\end{figure}

The solution set is shown in Table \ref{table:pareto_paths}; note that all the paths are supported solutions, since we have 
\begin{align}
\forall j,k \in \{1,\dots,4\}: \sum_{i=1}^{d} w_i(p_j) =  \sum_{i=1}^{d} w_i(p_k) \quad .
\end{align}

\begin{table*}
\begin{center}
    \begin{tabular}{| l | c |}
    \hline
    \textbf{Path} & \textbf{Weight vector} \\ \hline \hline
    $p_1 = \langle 1,2,3,4,5 \rangle$ & (5,5) \\ \hline
    $p_2 = \langle 1,3,4,5\rangle$  & (4,6) \\ \hline
    $p_3 = \langle 1,2,3,5\rangle$  & (3,7) \\ \hline
    $p_4 = \langle 1,3,5\rangle$ & (2,8) \\ \hline
    %$p_5$: 1-5 & (3.8,6.8) \\ \hline
    \end{tabular}
    \caption{Pareto-optimal paths from node 1 to node 2 in the graph of Fig.~\ref{fig:example_graph}}
    \label{table:pareto_paths}
\end{center}
\end{table*}

We extend the example by adding an arc $(1,5)$ with weight vector $(3.8,6.8)$.
Note that now, in addition to the previous solutions, the path $p_5 = \langle 1,5 \rangle $ is a Pareto-optimal path.
However, we have 
\begin{align}
\forall j \in \{1,\dots,4\}: \sum_{i=1}^{d} w_i(p_j) < \sum_{i=1}^{d} w_i(p_5) \quad ,
\end{align}
i.e., $p_5$ is not on the convex hull an thus called an \emph{unsupported non-dominated solution}, as depicted in Fig. \ref{fig:unsupported_solution}.
% approximate solution set
Thus, an \emph{approximate solution set} provides a ``reasonable'' set of non-dominated paths, but is not necessarily complete.

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[xlabel=$w_1$, ylabel=$w_2$, nodes near coords, enlargelimits=0.2]
	\addplot[color=blue, mark=*, point meta=explicit symbolic] 
	coordinates {
		(5,5) [$p_1$]
		(4,6) [$p_2$]
		(3,7) [$p_3$]
		(2,8) [$p_4$]
	};
	\addplot[color=red, mark=*, point meta=explicit symbolic] 
	coordinates {
		(3.8,6.8) [$p_5$]
	};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{An unsupported non-dominated solution: $p_5$}
\label{fig:unsupported_solution}
\end{figure}

\subsubsection*{Mutligraphs} \label{sec:shortest_path:defs:multigraph}
A multigraph is a graph that may contain parallel edges, that is, multiple edges may connect the same two nodes. 
Formally, a directed multigraph $G=(V,A)$ consists of a finite set of nodes $V = \{v_1, v_2,\dots,v_n \}$ and a finite multiset\footnote{In contrast to a set, a multiset may contain the same element multiple times.} of arcs and a function $f$, 
\begin{align}
f: A &\rightarrow (u,v) \text{ where } u,v \in V \quad .
\end{align}

The definition of weighted multigraphs is analogous to the definition of weighted graphs given in Eq.~\ref{eq:msp_weighted}. 

% our algorithm works on multigraphs too!
\todo{This should go somewhere else - it certainly does not belong to the formal definitions}
We note that in contrast to the algorithms mentioned in Section \ref{sec:related}, our algorithm works on multigraphs as well and provide a comparative performance evaluation in Section \todo{well, where?}.



\section{Multi-criteria shortest path algorithm} \label{sec:msp_algo}
% what we do in this section
In this section, we present our algorithm for the MSP.
% Problem setting
We want to compute an exact solution of the MSP for directed graphs with multi-dimensional weight vectors (as defined in Section \ref{sec:shortest_path:defs:msp}). 
% Intro to the algorithm: generalization of Dijkstra
The algorithm follows the principles of Dijkstra's algorithm (we generate a set of candidates and expand the most promising first), where the problem of finding intermediate paths that are Pareto-optimal is delegated to a priority queue. 
Our algorithm relies on the Pheet task scheduling framework for parallelization and, in particular, on Pheet's concept of \emph{priority task scheduling} (Section \ref{sec:pheet:priority_sched}), which allows to combine the work pool of the scheduler with the priority queue used by the algorithm into one data structure: a priority-aware task scheduler.

% Works with different schedulers/task storages
We designed the algorithm s.t.~it may easily employ different task scheduler implementations without any major changes to the algorithm itself. 
Within this framework, we only have to provide implementations of 
\begin{inparaenum}[(i)]
\item the task and 
\item a scheduling strategy.
\end{inparaenum}

% The algorithm is label-correcting (no strict sequence; furthermore, we have a set of solutions for each node)

% Explain graph class?



% Abstract description of the algorithm
\begin{algorithm}
\caption{Pseudo-code for our MSP algorithm}
\label{algo:abstractMSP}
\begin{algorithmic}[1]
\Require{Graph $G$, start node $s$}
\ForAll{$v \in G$}
  \State $S_p[v] \gets \{\}$ \Comment{Initialize the Pareto-set for each node}
\EndFor
\State $S_p[s] \gets \{ \vect{0} \} $ \Comment{Shortest path from $s$ to $s$ is the null-vector}
\State $U \gets \{\langle s \rangle\}$ \Comment{Set of paths that need to be explored further}
\While{$U$ is not empty}
  \State Take path $p$ from $U$ 
  \State \Call{expand}{$p$}
\EndWhile
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Pseudo-code for expanding a candidate path}
\label{algo:abstractExpand}
\begin{algorithmic}[1]
\Require{$S_p[v]$: Set of Pareto-optimal paths to node $v$ ($\forall v\in V$)}
\Require{$U$: Set of paths that need to be explored further}
\Require{$p=\langle s, \dots, v \rangle$ is a path in Graph $G$ from start node $s$ to $v$}
\Function{expand}{Path $p$}
\ForAll{$w \in p$.head.neighbors} 
  \State $p' \gets \cP{p}{w}$	\Comment{Generate candidate}
  \If{$\nexists x \in S_p[w] \text{ s.t.\ } x \dom p'$} 
  \Statex \Comment{Candidate is not dominated by any path in the Pareto-set}
    \ForAll{$y\in S_p[w]$} \Comment{Remove any path dominated by $p'$}
      \If{$p' \dom y$}
	\State $S_p[w] \gets S_p[w] \setminus y$
      \EndIf
    \EndFor
    \State $S_p[w] \gets S_p[w] \cup p'$ \Comment{Add $p'$ to the Pareto-set for node $w$}
    \State $U \gets U \cup p'$ \Comment{$p'$ needs to be explored further}
  \EndIf
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

% Explain the pareto set
\begin{code}[label=lst:ParetoSet, caption=Pareto set operations.]
class ParetoSet {
  /* Insert p into the Pareto-set. Any path p' that is dominated by p will be marked dominated and removed from the set. If p is dominated by a path p' already in the set, p will be marked domianted and will not be inserted into the set. */
  bool insert(Path p);

  /* Get all the paths stored in this Pareto-set. */
  Set<Path> paths();
}
\end{code}

% Msp task
%\begin{minipage}[c]{\linewidth}
%\begin{algorithm}
%\label{lst:MSPTask}
\begin{code}[caption=MSP Task, label=MSP Task]
class MSPTask : SchedulerTask {
public:

  MSPTask(Path p, ParetoSets Sp) //Constructor. Save parameters.
      : p(p), Sp(Sp) { }
  
  void execute() {
    /* The path this task was spawned for might be obsolete. */
    if(p.dominated) return;	
    Set<Path> candidates;
    /* Generate new candidates */
    for (Arc a : p.head().outgoing_edges()) {
      Path q = p.step(e);
      candidates.insert(q);
    }
    /* Add the candidates to the global pareto sets */
    Set<Path> added;
    added = Sp.insert(candidates);
    for (Path p : added) {
      if(!p.dominated()) {
	/* Spawn a new task for each added path */
	spawn_task(p, Sp);
      }
    }
  }
private:
  Path p;
  ParetoSets Sp;
}
\end{code}
%\end{algorithm}

\begin{comment}
% Msp task
\begin{algorithm}
\caption{MSP Task}
\label{algo:msptask}
\begin{algorithmic}[1]
   \Function{initialize}{Graph $g$, Path $p$, Pareto\_Sets $s$} 
      \State $\sC \gets \{\}$ \Comment{Set of candidates a task generates}
      \State $\dots$  \Comment{Store $g$, $p$ and $s$ for this instance of Task}
   \EndFunction
  \Statex
  \Function{execute}{}
    \If{$p$ is dominated} 
      \State return	\Comment{This task instance is obsolete}
    \EndIf
    \State Node $h \gets p$.head	\Comment{The last node of path $p$}
    \ForAll{$v \in h$.neighbors} \Comment{All neighbors of $h$}
      \State $\sC = \sC \cup \cP{p}{v}$ \Comment{Generate candidates}
    \EndFor
    \State $\sA \gets s$.insert(\sC) \Comment{Inserts the generated candidates into global Pareto set}
    \Statex \Comment{$\sA$ contains candidates not dominated by any path in $s$}
    \ForAll{$p\in \sA$}
      spawn\_task($g$, $p$, $s$) \Comment{Spawn a new task for each candidate}
    \EndFor
  \EndFunction
\end{algorithmic}
\end{algorithm}
\end{comment}

The following implementation variants are provided:
\begin{itemize}
% \item \verb|SequentialMsp|:
% \item \verb|StrategyMsp|:
\item \verb|Strategy2MspLSM|: \todo{we need to introduce the lsm somewhere}
% \item \verb|Strategy2MspKLSM|:
\item \verb|Strategy2MspPareto|:
\end{itemize}

% sequential algorithm: uses a priority queue

% StrategyScheduler: 

% Strategy2Scheduler, variants:



\section{Performance evaluation} \label{sec:evaluation}

% Mars hardware
For performance evaluation, we used a shared memory system nicknamed \emph{Mars}, an Intel Xeon based system with the properties listed in Table \ref{table:mars}.
\begin{table*}
\begin{center}
    \begin{tabular}{| l | l |}
    \hline
    CPU model & Intel Xeon E7-8850 \\ \hline
    Number of cores & 80 (8 nodes with 10 cores each) \\ \hline
    CPU clock & 2.00 GHz \\ \hline
    L1i & 32 KB \\ \hline
    L1d & 32 KB \\ \hline
    L2 & 256 KB \\ \hline
    L3 & 24576 KB \\ \hline
    Main memory & 1 TB \\ \hline    
    \end{tabular}
    \caption{Hardware configuration of Mars}
    \label{table:mars}
\end{center}
\end{table*}
% compiler 
For all experiments, Pheet was compiled using \verb|clang 3.4.2-7| with the the \verb|-O3| flag to allow standard compiler optimizations.

\subsection{Test instances} \label{sec:eval:test_instances}
% test graphs
All input instances were created by \verb|PHEET_HOME/test/msp/lib/Graph/Generator/main.h|, which generates a random connected digraph with the following arguments:
\begin{compactitem}
\item \verb|n|: Number of nodes.
\item \verb|m|: Number of edges.
\item \verb|d|: Degree of the weight vectors.
\item \verb|w|: Upper limit for all dimensions of the weight vectors.
\item \verb|r|: Random seed value.
\end{compactitem}
If the option \verb|-p| is given, a multigraph will be generated. 
Otherwise, the generated graph will contain no parallel edges.
\subsection{Methodology} \label{sec:eval:methodology}
% number of runs
% confidence intervals



\section{Related work} \label{sec:related}
%--------------------------------------------------------------------------------------------
%Should related work be covered near the beginning of the paper or near the end?
%   - Beginning, if it can be short yet detailed enough, or if it's critical to take a strong defensive stance about previous work right away. In this case Related Work can be either a subsection at the end of the Introduction, or its own Section 2.
%   	- End, if it can be summarized quickly early on (in the Introduction or Preliminaries), or if sufficient comparisons require the technical content of the paper. In this case Related Work should appear just before the Conclusions, possibly in a more general section "Discussion and Related Work".
%--------------------------------------------------------------------------------------------

% Ehrgott, Gandibleux: 
For an overview of related work for the MCSP without parallelization, we refer to the annotated bibliography of multiobjective combinatorial optimization \cite{EhrgottG02}, specifically Section ``6.1 Shortest path problems''.

% Bi-criteria

% Sonnier: approximate solutions for d \in {3,4}
Sonnier \cite{Sonnier06} were one of the first to publish parallel algorithms for solving MSP problems with 3 or 4 objectives. 
Their algorithms are based on a weighted sum approach and thus provide only an approximate solution, i.e., the solution set may not contain all Pareto-optimal paths.

% Sanders, Erb: first parallel MSP algo, but mostly for d=2
To the best of our knowledge, Sanders and Mandow \cite{SM13} published the first proposal for a parallel algorithm that provides an exact solution of the MSP.
Their approach extends Dijkstra's classical algorithm \cite{Dijkstra59} and relies on a so called \emph{Pareto queue}, a multi-dimensional generalization of a priority queue. 
While they give a high level description of the algorithm for arbitrary $d \geq 2$ and a detailed description of the bi-criteria case (which was further engineered by Erb \cite{Erb13}), they also state that efficient priority queues for $d\geq3$ are not yet known.

\section{Conclusion} \label{sec:conclusion}
\todo{...}
\section{Acknowledgements} \label{sec:ack}
\todo{...}


%bibliography
\bibliographystyle{acm}
\bibliography{sources} 



%tmp plots
\section{Plots}
\subsection{28.08.}

\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/base/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/1/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/2/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/3/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/4/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Improvements on 28-08}
\end{figure}

\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/strategy.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2lsm.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2klsm.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{s2pareto vs.\ s2lsm vs.\ s2klsm vs.\ strategy}
\end{figure}

\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/strategy.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2lsm.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/KDSet/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/KDSet/strategy.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/KDSet/s2lsm.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Each strategy/task storage with NaiveSet and KDSet}
\end{figure}

\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/lincomb/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{s2pareto with lincomb and pareto strategy}
\end{figure}


\subsection{29.08.}
\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/08_28/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/08_29/1/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Set path dominated in NaiveSet if necessary}
\end{figure}

\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/mars/g_2000_13000_3_10000_42/08_29/s2pareto.dat",
"../benchmarks/mars/g_2000_13000_3_10000_42/08_29/s2lsm.dat",
"../benchmarks/mars/g_2000_13000_3_10000_42/08_29/s2klsm.dat",
"../benchmarks/mars/g_2000_13000_3_10000_42/08_29/strategy.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Different strategies/task storages on mars}
\end{figure}

\subsection{08.09.}
\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/09_08/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_08/before/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Relaxed memory order for all virtual array accesses}
\end{figure}

\subsection{11.09.}
\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/local/g_2000_13000_3_10000_42/09_11/after/s2pareto.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/09_11/before/s2pareto.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{Improvements in PLTSBlock::partition}
\end{figure}


\end{document}
