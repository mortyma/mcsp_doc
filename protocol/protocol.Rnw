\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{comment}
\usepackage{hyperref}
\usepackage{cite}
%page boarders
\usepackage[vmargin=3cm, hmargin=3cm]{geometry}
\usepackage{float}
\floatstyle{plaintop}
\usepackage{paralist} %compactitem
\usepackage{fixltx2e} %some latex fixes
\usepackage{microtype} %Subliminal refinements towards typographical perfection
\setlength{\emergencystretch}{2em}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs} %Support use of the Raph Smithâ€™s Formal Script font in mathematics
\usepackage[sc]{mathpazo} %mathematical fonts
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage[margin=10pt, font=small, labelfont=bf]{caption}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{color}
\floatstyle{plaintop}
\usepackage{float}
\restylefloat{table}
\usepackage {tikz}
\usetikzlibrary{positioning}
\usepackage{pgfplots}

\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}
\newcommand\todo[1]{\textcolor{red}{TODO: #1}}
\newcommand\load{L_{\mathrm{max}}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% within an align* environment, number and label the equation. #1 is the label text to be appended to eqn:
\newcommand\neqn[1]{\numberthis\label{eq:#1}}
% total order relation
\newcommand{\tor}{\ensuremath{\leq_R}}
\newcommand{\osum}{\ensuremath{\leq_{\text{sum}}}}
\newcommand{\olex}{\ensuremath{\leq_{\text{lex}}}}

% theorem, lemma, ...
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}

% vectors
\newcommand{\vect}[1]{\vec{#1}}

\lstset{
    language=C,
    basicstyle=\ttfamily,
    keywordstyle=\color{OliveGreen},
    commentstyle=\color{Gray},
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    showspaces=false,
    showtabs=false,
    numbers=left,
}

\title{A multi-criteria priority queue for the Pheet task scheduling framework\\
\normalsize Technical report for \\``Project in Software Engineering \& Internet Computing''
}
\author{Martin Kalany, 0825673\\
Vienna University of Technology}

\begin{document}

\maketitle

%scale factor for graphs
\setkeys{Gin}{width=0.5\textwidth}

%R functions for plot generation
<<echo=FALSE, include=FALSE>>=
\SweaveInput{functions.Rnw}
@

\begin{abstract}
\cite{Wimmer14} \todo{abstract}
\end{abstract}

\pagebreak

%TODO
%\tableofcontents


\section{Introduction} \label{sec:intro}
% explain the idea of this work in a couple of sentences. This is for the informed reader.

% Introduce pheet
Pheet\footnote{\url{www.pheet.org}} is an open-source task scheduling framework for shared memory systems   
and is based on the task-parallel programming model, which allows a programmer to explicitly expose the parallelism of an application.
% The build-up for task priorities
Runtime systems based on the task-parallel model typically impose a non-adaptive, application independent execution order on the tasks where the scheduler is unaware of the preferred execution order of the tasks (The tasks are processed in, e.g., LIFO-order).
% The problem with Dijkstra's algorithm
While it has proven to be an efficient strategy for parallelization \todo{citation needed}, this approach has not been particularly useful for the parallelization of algorithms relying on priority queues (such as Dijkstra's famous algorithm for the single-source shortest path problem (SSSP) \cite{Dijkstra59}), since two work pools have to be maintained: 
One is used by the scheduling framework to maintain tasks that are yet to be executed; the other is required by the algorithm itself to determine the execution order of the tasks.

% Priority task scheduling
\emph{Priority task scheduling}, introduced by Wimmer et al.~\cite{WVTCT13}, addresses this problem by making the preferred execution order of tasks known to the scheduler. 
To achieve this, the task scheduler utilizes a concurrent priority queue which fulfills the ordering requirements imposed by the algorithm by assigning discrete priority values to the tasks.
Wimmer \cite{Wimmer14} further states that a scheduler based on the task priority scheduling model allows for an efficient parallel implementation of any algorithm relying on priority queues is possible.
They back up their claim by providing a parallel implementation of Dijkstra's algorithm on top of Pheet, which utilizes a scheduler based on the priority task scheduling model.

% Pareto priorities
In their dissertation \cite{Wimmer14}, they suggest to generalize the idea of priority task scheduling to multi-dimensional (or \emph{Pareto-}) priorities, which would allow for an efficient parallel implementation of e.g., algorithms solving the multi-criteria shortest path problem (MSP)\footnote{Sometimes also called multi-objective shortest path problem, multi-objective optimization or multi-objective search.} as discussed in \cite{Martins84}.
They outline a different advantage of using multi-dimensional priorities for task scheduling, which is independent of the algorithm that utilizes the scheduler:
Multi-dimensional priorities establish only a partial ordering on the tasks, which gives the scheduler more flexibility in terms of which task to execute next. 

% Our work
In this work, we investigate a potential implementation of such a multi-dimensional priority queue for the Pheet task scheduling framework, which is then used in turn for a parallel algorithm solving the MSP\footnote{This problem was suggested to us by our supervisor Martin Wimmer.}. 

\todo{explain that we compare our priority queue to a solution based on linear combination+one dimensional priority queue,...}


\subsubsection*{Outline} \label{sec:intro:outline}
We give a formal definition of the multi-criteria shortest path problem in Section \ref{sec:shortest_path}, where we also discusses the hardness of the problem.
In Section \ref{sec:pheet}, we provide a general introduction to the Pheet task scheduling framework and a more detailed presentation of the concepts used in Pheet that are relevant for this work.

\todo{...}

\section{Bits and pieces}

% plug-in architecture
Pheet's flexible plug-in architecture based on C++ templates allows for any component in the task scheduling system to be replaced by a different implementation.
% Pheet benchmark suite & performance counters
Additionally, Pheet provides a set of micro-benchmarks (that aim to evaluate specific aspects of the scheduling framework) and fine grained performance counters (that provide detailed insight into the scheduler and supporting data structures). 
% Framework for evaluation of scheduler components
Overall, Pheet is a suitable framework for the evaluation and comparison of scheduler components.

% reference Martin's dissertation
A detailed introduction to Pheet and the research based on it can be found in the dissertation of Martin Wimmer \cite{Wimmer14}. 

% work pool pattern
The task-parallel model is based on the \emph{work pool pattern}:
Informally, a task is a small amount of work that is to be executed sequentially.
All such tasks are stored in a work pool.
As soon as a processor is ready to execute work, it retrieves a task that is ready to be executed from the work pool.
A task may itself create additional tasks that are then stored in the work pool.
A famous algorithm based on the work pool pattern is Dijkstra's algorithm \cite{Dijkstra59} for the SSSP:


% task-parallel model
Additionally, the programmer defines dependencies between the task which ensure that a task is not executed before all required prerequisites are met; if, however, two tasks do not depend on each other, they may be executed in parallel.

% pareto priority queue
\begin{comment}
A Pareto priority queue ensures that that the next task to be executed is a Pareto optimum, i.e., a task for which the partial solution is not dominated by the partial solution of any other task - from MW diss
\end{comment}


\section{Shortest path problems} \label{sec:shortest_path}
% Informally introduce shortest path
The problem of finding a shortest path between two nodes in a graph is a classical problem in computer science with numerous practical applications, such as finding the quickest route from location A to location B.
% Introduce the problem of msp
While the \emph{shortest-path problem} minimizes for a single criteria, e.g., the travel time, in practical applications one often wants to optimize multiple -- and possibly conflicting -- objectives:
When searching for a route from A to B, we may want to minimize the travel time as well as toll costs.
In such a setting, we usually cannot give a single best solution anymore. 
Instead, we can give several reasonable solutions: 
The fastest route will usually use highways, thus increasing the toll costs, while a slightly slower route may reduce the toll costs by avoiding highways. 
Problems like this can be modeled as \emph{multi-criteria shortest path} (MSP) problems, where we are interested in all optimal alternatives, i.e., in routes that are \emph{Pareto-optimal}. 
A route from A to B is Pareto-optimal if there is no other route with less or equal cost for each of the $d$ objectives.

%subsection{Definitions and notation} \label{sec:shortest_path:defs}
In the following, we provide definitions for variations of the shortest path problems considered in this report. 
The classic text book by Cormen et al.~\cite{CLRS01} provides a more detailed introduction to basic shortest path problems and algorithms.

% Define single-source shortest path
\subsection{The classical shortest path problem} \label{sec:shortest_path:defs:sssp}
We are given a weighted directed graph $G=(V,A)$, where $V= \{v_1, v_2, \dots, v_n\}$ is a finite set of nodes, $A \subseteq V \times V$ is a finite set of arcs (directed edges) and $c: A \rightarrow \mathbb{N}$ is the cost or weight function. 
W.l.o.g., we assume that $G$ is a connected graph.
Let $p = \langle v_0, a_1, v_1, a_2, \dots,a_k, v_k \rangle$ be a path\footnote{We will also write $p = \langle v_0, v_1, \dots,v_k \rangle$ when not dealing with multigraphs, since an arc $a_i$ is well defined by its two adjacent nodes.}from $v_0$ to $v_k$ in $G$.
Furthermore, let $P$ be the set of all paths and $P_{u,v}$ be the set of all paths from node $u$ to node $v$ in $G$.
We define the weight $w(p)$ of a path $p$ as 
\begin{align*}
w: P &\rightarrow \mathbb{N} \\
p &\mapsto w(p) = \sum_{i=1}^{k}c(a_i) \quad , \neqn{sssp_weighted}
\end{align*}
i.e., the sum of costs all the arcs in the path $p$.

% Variations: Single source, all-pairs,... shortest path
In a \emph{single-source shortest path problem (SSSP)}, we want to compute a shortest path w.r.t.~some weight function $w$ from a given source node $s \in V$ to each node $v \in V$.
Different variations of the problem exist, such as the \emph{single-destination}, the \emph{single-pair} or the \emph{all-pairs} shortest path problem. 
Note that an optimal algorithm for the SSSP can easily be turned into an optimal algorithm for the first two variants \cite{CLRS01}.
Thus, we will only deal with the SSSP in this report.

\subsection{Multi-criteria shortest path (MSP)} \label{sec:shortest_path:defs:msp}
%MSP
Multi-criteria shortest path problems \cite{Martins84} generalize shortest path problems w.r.t.~the weight function $c$, which is extended to $d$-dimensional vectors, i.e., $\vect{c}: A \rightarrow \mathbb{N}^d$ and
\begin{align*}
\vect{w}: P &\rightarrow \mathbb{N}^d \\
p &\mapsto \vect{w}(p) = (w_1(p), w_2(p), \dots , w_d(p)) \quad , \neqn{msp_weighted}
\end{align*}
where $w_j(p) = \sum_{i=1}^{k}c_j(a_i)$ $\forall j \in \{1,\dots,d\}$.

%Domination relation
Let $\vect{x}$ and $\vect{y}$ be two vectors of dimension $d$, i.e., $\vect{x}, \vect{y} \in \mathbb{N}^d$ and $\vect{x} \neq \vect{y}$. 
We say that $\vect{x}$ \emph{dominates} $\vect{y}$ ($\vect{x} \prec \vect{y}$) -- or equivalently, $\vect{y}$ \emph{is dominated by} $\vect{x}$ -- if and only if
\begin{align}
\forall j \in \{1,\dots, d\}: w_j(x) \leq w_j(y) \quad .
\end{align}
Note that the dominance relation ($\prec$) does not define a total ordering in $\mathbb{R}^d$ for $d\geq 2$.
Thus, there exist vectors $\vect{x}, \vect{y} \in \mathbb{N}^d$, $\vect{x} \neq \vect{y}$ for which neither $\vect{x} \prec \vect{y}$ nor $\vect{y} \prec \vect{x}$ holds.

% Notes
A path $p$ from node $v_1$ to node $v_2$ is a \emph{Pareto-optimal} or \emph{non-dominated} path if there is no path $q \in P$ from $v_1$ to $v_2$ s.t.~$q \prec p$.
Informally, we also call a Pareto-optimal path $p$ \emph{shortest path}.
Thus, for the MSP, the solution consists of a set of Pareto-optimal paths for each considered pair of nodes.


\subsubsection*{Applications} \label{sec:shortest_path:applications} 
% Applications
Apart from the problem of finding an optimal route in a road map as outlined above, the MSP problem has numerous outer applications, such as routing in multimedia networks \cite{ClimacoCP03}, route guidance \cite{JahnMS00} and curve approximation \cite{MehlhornZ00}.

\subsubsection*{Hardness and practicality} \label{sec:shortest_path:hardness}
% Hardness of the problem, reason for parallelization
% Note that the graph instances resulting from modeling practical problems are potentially very large. 
The crucial parameter for the complexity of the MSP is the total number of Pareto optima for all visited nodes. 
Since this number is exponential in $n$ in the worst case \cite{Hansen80}, the MSP is in general NP-hard.
Even for $d=2$, the decision problem whether there exists a path between two nodes whose length is below a given threshold is NP-hard \cite{GareyJ79}.
However, it was observed that the problem is efficiently tractable from a practical viewpoint for many practically relevant instances. 
The input data of practical applications tends to have certain characteristics, which lead to the set of Pareto optima for a vertex to be polynomially bounded \cite{Muller-HannemannW06}. 
It was shown that in applied scenarios this number may even be bounded by a small constant \cite{Muller-HannemannW01}. 
Thus, MSP can be solved efficiently for small $d$ and not too large graphs adhering certain characteristics.

% reason for parallelization
However, due to the potentially very large graph instances resulting from modeling e.g., road or railway networks, the computational cost is significant even if a small number of criteria is to be optimized.
Guerriero and Musmanno \cite{GuerrieroM01} thus suggest that parallel computing might help to design efficient solution methods. 

\subsubsection*{Approximate solutions} \label{sec:shortest_path:approx}
% heuristics
Due to the hardness of the MSP, heuristic methods are sometimes employed (see, e.g., \cite{BastMS03, Sonnier06, EhrgottG02}).
% weighted sum approach
A common approach is to define a total order relation $\tor$ on $P$ that allows for a more efficient computation of the Pareto-optima.
Such a relation $\tor$ has to satisfy the following properties:
\begin{align}
\forall p,q \in P_{u,v}: p \prec q \implies p \tor q  \quad \text{Dominance} \\
\forall p \in P_{u,v}, \forall (v,w) \in A: p \tor p \cup \langle v,w \rangle \quad \text{Monotonic}
\end{align}
Martins et al.~\cite{MartinsPRS07} show that the following relation satisfies these requirements:
\begin{align}
p \osum q \iff \sum_{i=1}^{d} w_i(p) \leq  \sum_{i=1}^{d} w_i(q) 
\end{align}
% supported/non-supported solutions
However, as Sonnier \cite{Sonnier06} points out, a problem with algorithms based on such a relation is that 
\begin{align}
p \osum q  \notimplies p \prec q \quad , \neqn{osum_problem}
\end{align}
i.e., only solutions that lie on the convex hull of the feasible region are found, which are called the supported solutions. 
In other words, there exist Pareto-optima which may not be found because of the property given in Eq.~\ref{eq:osum_problem}.

% graph example from Sonnier
To see this, consider the following example from Sonnier \cite{Sonnier06}.
Assume we are interested in the Pareto-optimal paths from node 1 to node 5 in the graph depicted in Fig.~\ref{fig:example_graph}.
The solution set is shown in Table \ref{table:pareto_paths}; note that all the paths are supported solutions, since we have 
\begin{align}
\forall j,k \in \{1,\dots,4\}: \sum_{i=1}^{d} w_i(p_j) =  \sum_{i=1}^{d} w_i(p_k) \quad .
\end{align}
We extend the example by adding an arc $(1,5)$ with weight vector $(3.8,6.8)$.
Note that now, in addition to the previous solutions, the path $p_5 = \langle 1,5 \rangle $ is a Pareto-optimal path.
However, we have 
\begin{align}
\forall j \in \{1,\dots,4\}: \sum_{i=1}^{d} w_i(p_j) < \sum_{i=1}^{d} w_i(p_5) \quad ,
\end{align}
i.e., $p_5$ is not on the convex hull an thus called an \emph{unsupported non-dominated solution}, as depicted in Fig. \ref{fig:unsupported_solution}.
% approximate solution set
Thus, an \emph{approximate solution set} provides a ``reasonable'' set of non-dominated paths, but is not necessarily complete.

\begin{figure}
\begin {center}
\begin {tikzpicture}[-latex, auto, node distance = 3 cm, on grid, semithick, state/.style = {circle, top color = white, draw, minimum width = 1 cm}]
\node[state] (n1) {$1$};
\node[state] (n2) [below right of = n1] {$2$};
\node[state] (n3) [above right of = n2] {$3$};
\node[state] (n4) [below right of = n3] {$4$};
\node[state] (n5) [above right of = n4] {$5$};
\path (n1) edge node[above = 0.05 cm] {$(1,4)$} (n3);
\path (n3) edge node[above = 0.05 cm] {$(4,1)$} (n5);
\path (n1) edge [bend right = 30] node[below = 0.2 cm] {$(1,1)$} (n2);
\path (n2) edge [bend right = 30] node[below = 0.2 cm] {$(1,2)$} (n3);
\path (n3) edge [bend right = 30] node[below = 0.2 cm] {$(2,1)$} (n4);
\path (n4) edge [bend right = 30] node[below = 0.2 cm] {$(1,1)$} (n5);
%\path (n1) edge [bend left = 30, color=red] node[above = 0.2 cm, color=red] {$(3.8,6.8)$} (n5);
\end{tikzpicture}
\end{center}
\caption{A weighted ($d=2$) directed graph.}
\label{fig:example_graph}
\end{figure}

\begin{table*}
\begin{center}
    \begin{tabular}{| l | c |}
    \hline
    \textbf{Path} & \textbf{Weight vector} \\ \hline \hline
    $p_1 = \langle 1,2,3,4,5 \rangle$ & (5,5) \\ \hline
    $p_2 = \langle 1,3,4,5\rangle$  & (4,6) \\ \hline
    $p_3 = \langle 1,2,3,5\rangle$  & (3,7) \\ \hline
    $p_4 = \langle 1,3,5\rangle$ & (2,8) \\ \hline
    %$p_5$: 1-5 & (3.8,6.8) \\ \hline
    \end{tabular}
    \caption{Pareto-optimal paths from node 1 to node 2 in the graph of Fig.~\ref{fig:example_graph}}
    \label{table:pareto_paths}
\end{center}
\end{table*}

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[xlabel=$w_1$, ylabel=$w_2$, nodes near coords, enlargelimits=0.2]
	\addplot+[color=blue, mark=*, point meta=explicit symbolic] 
	coordinates {
		(5,5) [$p_1$]
		(4,6) [$p_2$]
		(3,7) [$p_3$]
		(2,8) [$p_4$]
	};
	\addplot+[color=red, mark=*, point meta=explicit symbolic] 
	coordinates {
		(3.8,6.8) [$p_5$]
	};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{An unsupported non-dominated solution: $p_5$}
\label{fig:unsupported_solution}
\end{figure}


\subsubsection*{Mutligraphs} \label{sec:shortest_path:defs:multigraph}
A multigraph is a graph that may contain parallel edges, that is, multiple edges may connect the same two nodes. 
Formally, a directed multigraph $G=(V,A)$ consists of a finite set of nodes $V = \{v_1, v_2,\dots,v_n \}$ and a finite multiset\footnote{In contrast to a set, a multiset may contain the same element multiple times.} of arcs and a function $f$, 
\begin{align}
f: A &\rightarrow (u,v) \text{ where } u,v \in V \quad .
\end{align}

The definition of weighted multigraphs is analogous to the definition of weighted graphs given in Eq.~\ref{eq:msp_weighted}. 
We note that in contrast to the algorithms mentioned in Section \ref{sec:related}, our algorithm works on multigraphs as well and provide a comparative performance evaluation in Section \todo{well, where?}.

\section{Pheet} \label{sec:pheet}
% what we will do in this section

In t


\subsection{Task parallelism} \label{sec:pheet:task_parallelism}

\subsection{Priority scheduling} \label{sec:pheet:priority_sched}


\section{Multi-criteria shortest path} \label{sec:msp_algo}
\todo{Our algorithm for the MSP. The algorithm works independently of the scheduler}

\section{Using the StrategyScheduler/linear combination} \label{sec:strategyScheduler}
\todo{\cite{MartinsPRS07} has a theorem showing that linear combination is a valid choice, but probably better to cite \cite{PaixaoS13}.}

\section{Multi-criteria priority queue} \label{sec:mcpq}
\todo{Description and analysis of: virtual array, log-structured list, put/pop, top, steal,\dots}

\section{Performance evaluation} \label{sec:eval}

% Mars hardware
For performance evaluation, we used a shared memory system nicknamed \emph{Mars}, an Intel Xeon based system with the properties listed in Table \ref{table:mars}.
\begin{table*}
\begin{center}
    \begin{tabular}{| l | l |}
    \hline
    CPU model & Intel Xeon E7-8850 \\ \hline
    Number of cores & 80 (8 nodes with 10 cores each) \\ \hline
    CPU clock & 2.00 GHz \\ \hline
    L1i & 32 KB \\ \hline
    L1d & 32 KB \\ \hline
    L2 & 256 KB \\ \hline
    L3 & 24576 KB \\ \hline
    Main memory & 1 TB \\ \hline    
    \end{tabular}
    \caption{Hardware configuration of Mars}
    \label{table:mars}
\end{center}
\end{table*}
% compiler 
For all experiments, Pheet was compiled using \verb|clang 3.4.2-7| with the the \verb|-O3| flag to allow standard compiler optimizations.

\subsection{Test instances} \label{sec:eval:test_instances}
% test graphs
All input instances were created by \verb|PHEET_HOME/test/msp/lib/Graph/Generator/main.h|, which generates a random connected digraph with the following arguments:
\begin{compactitem}
\item \verb|n|: Number of nodes.
\item \verb|m|: Number of edges.
\item \verb|d|: Degree of the weight vectors.
\item \verb|w|: Upper limit for all dimensions of the weight vectors.
\item \verb|r|: Random seed value.
\end{compactitem}
If the option \verb|-p| is given, a multigraph will be generated. 
Otherwise, the generated graph will contain no parallel edges.
\subsection{Methodology} \label{sec:eval:methodology}
% number of runs
% confidence intervals



\section{Related work} \label{sec:related}
%--------------------------------------------------------------------------------------------
%Should related work be covered near the beginning of the paper or near the end?
%   - Beginning, if it can be short yet detailed enough, or if it's critical to take a strong defensive stance about previous work right away. In this case Related Work can be either a subsection at the end of the Introduction, or its own Section 2.
%   	- End, if it can be summarized quickly early on (in the Introduction or Preliminaries), or if sufficient comparisons require the technical content of the paper. In this case Related Work should appear just before the Conclusions, possibly in a more general section "Discussion and Related Work".
%--------------------------------------------------------------------------------------------

% Ehrgott, Gandibleux: 
For an overview of related work for the MCSP without parallelization, we refer to the annotated bibliography of multiobjective combinatorial optimization \cite{EhrgottG02}, specifically Section ``6.1 Shortest path problems''.

% Bi-criteria

% Sonnier: approximate solutions for d \in {3,4}
Sonnier \cite{Sonnier06} were one of the first to publish parallel algorithms for solving MSP problems with 3 or 4 objectives. 
Their algorithms are based on a weighted sum approach and thus provide only an approximate solution, i.e., the solution set may not contain all Pareto-optimal paths.

% Sanders, Erb: first parallel MSP algo, but mostly for d=2
To the best of our knowledge, Sanders and Mandow \cite{SM13} published the first proposal for a parallel algorithm that provides an exact solution of the MSP.
Their approach extends Dijkstra's classical algorithm \cite{Dijkstra59} and relies on a so called \emph{Pareto queue}, a multi-dimensional generalization of a priority queue. 
While they give a high level description of the algorithm for arbitrary $d \geq 2$ and a detailed description of the bi-criteria case (which was further engineered by Erb \cite{Erb13}), they also state that efficient priority queues for $d\geq3$ are not yet known.

\section{Conclusion} \label{sec:conclusion}
\todo{...}
\section{Acknowledgements} \label{sec:ack}
\todo{...}


\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotPerformanceVariance(
"../benchmarks/mars/g_2000_13000_3_10000_42/with_lincomb/s2pareto.dat", from=1, to=3)
@
\end{center}
\caption{Performance with linear combination in Strategy2TaskStrategy::prioritize (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotPerformanceVariance(
"../benchmarks/mars/g_2000_13000_3_10000_42/with_lincomb/s2pareto.dat", from=4, to=80)
@
\end{center}
\caption{Performance with linear combination in Strategy2TaskStrategy::prioritize (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotPerformanceVariance(
"../benchmarks/mars/g_2000_13000_3_10000_42/without_lincomb/s2pareto.dat", from=1, to=3)
@
\end{center}
\caption{Performance without linear combination in Strategy2TaskStrategy::prioritize (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotPerformanceVariance(
"../benchmarks/mars/g_2000_13000_3_10000_42/without_lincomb/s2pareto.dat", from=4, to=80)
@
\end{center}
\caption{Performance without linear combination in Strategy2TaskStrategy::prioritize (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/mars/g_2000_13000_3_10000_42/with_lincomb/s2pareto.dat","../benchmarks/mars/g_2000_13000_3_10000_42/without_lincomb/s2pareto.dat", yAxis="total_time", splitBy="comment", from=8, to=80)
@
\end{center}
\caption{Influence of linear combination in Strategy2TaskStrategy::prioritize; min over 10 runs (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/mars/g_2000_13000_3_10000_42/with_lincomb/s2pareto.dat","../benchmarks/mars/g_2000_13000_3_10000_42/without_lincomb/s2pareto.dat", yAxis="total_time", splitBy="comment", aggFun=mean)
@
\end{center}
\caption{Influence of linear combination in Strategy2TaskStrategy::prioritize; avg over 10 runs (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotBenchmark(
"../benchmarks/mars/g_2000_13000_3_10000_42/improved_pop/before.dat","../benchmarks/mars/g_2000_13000_3_10000_42/improved_pop/after.dat", xAxis="cpus", yAxis="total_time", splitBy="comment")
@
\end{center}
\caption{Improved PLTSBlock::pop (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotBenchmark(
"../benchmarks/mars/g_2000_13000_3_10000_42/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/s2klsm.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/s2lsm.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/improved_put/s2pareto.dat", xAxis="cpus", yAxis="total_time", splitBy="comment")
@
\end{center}
\caption{Improved PLTSBlock::put (g\_2000\_13000\_3\_10000\_42)}
\end{figure}


\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotBenchmark(
"../benchmarks/mars/g_2000_13000_3_10000_42/strategy.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/s2klsm.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/s2lsm.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/s2pareto.dat", xAxis="cpus", yAxis="total_time", splitBy="algorithm")
@
\end{center}
\caption{Abs.\ performance over nr.\ cpus (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotBenchmark(
"../benchmarks/local/g_2000_13000_3_10000_42/baseline/strategy.dat", "../benchmarks/local/g_2000_13000_3_10000_42/baseline/strategy2.dat", xAxis="cpus", yAxis="total_time", splitBy="algorithm")
@
\end{center}
\caption{Abs.\ performance over nr.\ cpus (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotBenchmark(
"../benchmarks/mars/g_2000_13000_3_10000_42/mo_seq_cst/strategy2.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/mo_aq_rel/strategy2.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/mo_relaxed/strategy2.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/strategy.dat", xAxis="cpus", yAxis="total_time", splitBy="comment")
@
\end{center}
\caption{Influence of memory order (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus(
"../benchmarks/mars/g_2000_13000_3_10000_42/mo_seq_cst/strategy2.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/mo_aq_rel/strategy2.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/mo_relaxed/strategy2.dat", yAxis="total_time", splitBy="comment", from=1, to=4)
@
\end{center}
\caption{Influence of memory order, \#cpus=1-4 (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus("../benchmarks/mars/g_2000_13000_3_10000_42/mo_seq_cst/strategy2.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/mo_aq_rel/strategy2.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/mo_relaxed/strategy2.dat", yAxis="total_time", splitBy="comment", from=4, to=80)
@
\end{center}
\caption{Influence of memory order, \#cpus=4-80  (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotBenchmark(
"../benchmarks/local/g_2000_13000_3_10000_42/max_partition_8/strategy2.dat", "../benchmarks/local/g_2000_13000_3_10000_42/max_partition_16/strategy2.dat", "../benchmarks/local/g_2000_13000_3_10000_42/max_partition_32/strategy2.dat", "../benchmarks/local/g_2000_13000_3_10000_42/max_partition_64/strategy2.dat", "../benchmarks/local/g_2000_13000_3_10000_42/max_partition_128/strategy2.dat", "../benchmarks/local/g_2000_13000_3_10000_42/max_partition_256/strategy2.dat", "../benchmarks/local/g_2000_13000_3_10000_42/max_partition_512/strategy2.dat", "../benchmarks/local/g_2000_13000_3_10000_42/max_partition_1024/strategy2.dat", "../benchmarks/local/g_2000_13000_3_10000_42/max_partition_4096/strategy2.dat",
"../benchmarks/local/g_2000_13000_3_10000_42/baseline/strategy2.dat", xAxis="cpus", yAxis="total_time", splitBy="comment")
@
\end{center}
\caption{Influence of MAX\_PARTITION\_SIZE (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus("../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/16/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/32/s2pareto.dat",
"../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/64/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/128/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/256/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/512/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/1024/s2pareto.dat",
yAxis="total_time", splitBy="comment", from=1, to=3)
@
\end{center}
\caption{Influence of MAX\_PARTITION\_SIZE (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus("../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/16/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/32/s2pareto.dat",
"../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/64/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/128/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/256/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/512/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/1024/s2pareto.dat",
yAxis="total_time", splitBy="comment", from=4, to=7)
@
\end{center}
\caption{Influence of MAX\_PARTITION\_SIZE (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotOverCpus("../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/16/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/32/s2pareto.dat",
"../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/64/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/128/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/256/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/512/s2pareto.dat", "../benchmarks/mars/g_2000_13000_3_10000_42/MAX_PARTITION_SIZE/1024/s2pareto.dat",
yAxis="total_time", splitBy="comment", from=8, to=80)
@
\end{center}
\caption{Influence of MAX\_PARTITION\_SIZE (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

\begin{figure}
\begin{center}
<<fig=TRUE, echo=FALSE>>=
plotBenchmark("../benchmarks/local/g_2000_13000_3_10000_42/max_partition_128/strategy2.dat", "../benchmarks/local/g_2000_13000_3_10000_42/baseline/strategy.dat", xAxis="cpus", yAxis="total_time", splitBy="algorithm")
@
\end{center}
\caption{Strategy2 with MAX\_PARTITION\_SIZE=128 vs.~Strategy (g\_2000\_13000\_3\_10000\_42)}
\end{figure}

%bibliography
\bibliographystyle{acm}
\bibliography{sources} 
\end{document}

  