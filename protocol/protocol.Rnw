\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{cite}
%page boarders
\usepackage[vmargin=3cm, hmargin=3cm]{geometry}
\usepackage{float}
\floatstyle{plaintop}
\usepackage{paralist} %compactitem
\usepackage{fixltx2e} %some latex fixes
\usepackage{microtype} %Subliminal refinements towards typographical perfection
\setlength{\emergencystretch}{2em}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs} %Support use of the Raph Smithâ€™s Formal Script font in mathematics
\usepackage[sc]{mathpazo} %mathematical fonts
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage[margin=10pt, font=small, labelfont=bf]{caption}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{color}
\usepackage{xcolor}
\floatstyle{plaintop}
\usepackage{float}
\restylefloat{table}
\usepackage {tikz}
\usetikzlibrary{calc,shapes.multipart,chains,arrows, positioning,shapes}
\usepackage{pgfplots}
\usepackage{fancyvrb}
\usepackage{subfigure}
\usepackage{etoolbox}\AtBeginEnvironment{algorithmic}{\small}

% a nice not implies
\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}
  
% todos  
\newcommand\todo[1]{\textcolor{red}{TODO: #1}}

% number equations s.t. they can be referenced easily
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% within an align* environment, number and label the equation. #1 is the label text to be appended to eqn:
\newcommand\neqn[1]{\numberthis\label{eq:#1}}

% theorem, lemma, ...
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}

% math stuff
% vectors
\newcommand{\vect}[1]{\vec{#1}}

% dominance relation
\newcommand{\dom}{\ensuremath{\prec}}
% less relation
\newcommand{\less}{\ensuremath{\prec}}
\newcommand{\lesseq}{\ensuremath{\preceq}}
\newcommand{\moreeq}{\ensuremath{\succeq}}

% total order relation
\newcommand{\tor}{\ensuremath{\leq_R}}
\newcommand{\osum}{\ensuremath{\leq_{\text{sum}}}}
\newcommand{\olex}{\ensuremath{\leq_{\text{lex}}}}

% sets
\newcommand{\sC}{\ensuremath{S_{\text{c}}}}
\newcommand{\sA}{\ensuremath{S_{\text{a}}}}
\newcommand{\sR}{\ensuremath{S_{\text{r}}}}

% concatenation of paths
\newcommand{\cP}[2]{\ensuremath{#1;#2}}

% style of listings
\definecolor{Gray}{gray}{0.5}
\definecolor{OliveGreen}{cmyk}{0.64,0,0.95,0.40}

   \lstset{
      language=C++,
      basicstyle=\ttfamily\footnotesize,
      keywordstyle=\color{blue},
      commentstyle=\color{OliveGreen},
      breaklines=true,
      breakatwhitespace=false,
      showspaces=false,
      showtabs=false,
      numbers=left,
      frame=single,
      captionpos=t,
  }

\lstnewenvironment{code}[1][]%
{
   \noindent
   \minipage{\linewidth} 
   \vspace{0.5\baselineskip}
   \lstset{
      #1
  }
  }
   %\lstset{basicstyle=\ttfamily\footnotesize,frame=single,#1}}
{\endminipage}

%\DeclareCaptionFormat{mylst}{\hrule#1#2#3}
%\captionsetup[code]{format=mylst,labelfont=bf,singlelinecheck=off,labelsep=space}

% 
\title{A multi-criteria priority queue for the Pheet task scheduling framework\\
\normalsize Technical report for \\``Project in Software Engineering \& Internet Computing''
}
\author{Martin Kalany, 0825673\\
Vienna University of Technology}

\begin{document}
\maketitle

%scale factor for graphs
\setkeys{Gin}{width=0.5\textwidth}

%R functions for plot generation
<<echo=FALSE, include=FALSE>>=
\SweaveInput{functions.Rnw}
@

\VerbatimFootnotes

%\pagebreak	

\begin{abstract}
\cite{Wimmer14} \todo{abstract}
\end{abstract}

\pagebreak
\tableofcontents
\pagebreak

\section{Introduction} \label{sec:intro}
% explain the idea of this work in a couple of sentences. This is for the informed reader.

% Introduce pheet
Pheet\footnote{\url{www.pheet.org}} is an open-source task scheduling framework for shared memory systems   
and is based on the task-parallel programming model, which allows a programmer to explicitly expose the parallelism of an application.
% The build-up for task priorities
Runtime systems based on the task-parallel model typically impose a non-adaptive, application independent execution order on the tasks where the scheduler is unaware of the preferred execution order of the tasks (The tasks are processed in, e.g., LIFO-order).
% The problem with Dijkstra's algorithm
While it has proven to be an efficient strategy for parallelization \cite{AroraBP01}, this approach has not been particularly useful for the parallelization of algorithms relying on priority queues (such as Dijkstra's famous algorithm for the single-source shortest path problem (SSSP) \cite{Dijkstra59}), since two work pools have to be maintained: 
One is used by the scheduling framework to maintain tasks that are yet to be executed; the other is required by the algorithm itself to determine the execution order of the tasks.

% Priority task scheduling
\emph{Priority task scheduling}, introduced by Wimmer et al.~\cite{WVTCT13}, addresses this problem by making the preferred execution order of tasks known to the scheduler. 
To achieve this, the task scheduler utilizes a concurrent priority queue which fulfills the ordering requirements imposed by the algorithm by assigning comparison-based priority values to the tasks.
Wimmer \cite{Wimmer14} further states that a scheduler based on the task priority scheduling model allows for an efficient parallel implementation of any algorithm relying on priority queues is possible.
They back up their claim by providing a parallel implementation of Dijkstra's algorithm on top of Pheet, which utilizes a scheduler based on the priority task scheduling model.

% Pareto priorities
In their dissertation \cite{Wimmer14}, they suggest to generalize the idea of priority task scheduling to multi-dimensional (or \emph{Pareto-}) priorities (as defined in Section \ref{sec:pareto_optima}), which requires an efficient Pareto-priority queue implementation to be added to Pheet. 
A Pareto-priority queue ensures that the next task to be executed is a Pareto optimum, i.e., a task for which the partial solution is not dominated by the partial solution of any other task.
Such a Pareto-priority queue would allow for an efficient parallel implementation of e.g., algorithms solving the multi-criteria shortest path problem (MSP)\footnote{Sometimes also called multi-objective shortest path problem, multi-objective optimization or multi-objective search.} as discussed in \cite{Martins84}.
They outline a different advantage of using multi-dimensional priorities for task scheduling, which is independent of the algorithm that utilizes the scheduler:
Multi-dimensional priorities establish only a partial ordering on the tasks, which gives the scheduler more flexibility in terms of which task to execute next. 

% Our work
In this work, we investigate a potential implementation of such a multi-dimensional priority queue for the Pheet task scheduling framework, which is then used in turn for a parallel algorithm solving the MSP. 
% MSP is not our focus
\footnote{We note here that the goal of our work is not to provide a better algorithm for the MSP; we investigate the potential of a multi-dimensional priority queue and use the MSP only as an example application.}
% first step: use linear combination
As a first step and to provide a reference point, we will solve the MSP via the SSSP problem by reducing the multi-dimensional priorities to a scalar value via the $L^1$ norm.
By doing so, the problem can be solved with Dijkstra's algorithm using one of Pheet's priority queue implementations. 
% total ordering not always possible
However, due to the general nature of a task's priority (defined in Section \ref{sec:pareto_optima}), to determine the highest-priority task the MCPQ cannot rely on a total ordering relation like the $L^1$ norm and has to make use of different techniques.

\todo{something about results?}

\subsubsection*{Outline} \label{sec:intro:outline}
We start with discussing related work in Section \ref{sec:related}, followed by a formal definition of Pareto-optimality (Section \ref{sec:pareto_optima}),and a high-level introduction of Pheet-related concepts that are relevant for this work (Section \ref{sec:pheet}).
A detailed presentation and analysis of our multi-criteria priority queue is given in Section \ref{sec:mcpq}).
We give a formal definition of the multi-criteria shortest path problem, which is used as an example application, in Section \ref{sec:shortest_path}, where we also discuss the hardness of the problem.
A detailed description of a Pheet-based algorithm solving the MSP will be given in Section \ref{sec:msp_algo}. 
This implementation is then used to compare the performance of our multi-criteria priority queue to other priority queue implementations already available in Pheet (Section \ref{sec:evaluation}).

\todo{Future work, conclusion,...}

\section{Related work} \label{sec:related}
%--------------------------------------------------------------------------------------------
%Should related work be covered near the beginning of the paper or near the end?
%   - Beginning, if it can be short yet detailed enough, or if it's critical to take a strong defensive stance about previous work right away. In this case Related Work can be either a subsection at the end of the Introduction, or its own Section 2.
%   	- End, if it can be summarized quickly early on (in the Introduction or Preliminaries), or if sufficient comparisons require the technical content of the paper. In this case Related Work should appear just before the Conclusions, possibly in a more general section "Discussion and Related Work".
%--------------------------------------------------------------------------------------------

% Ehrgott, Gandibleux: 
For an overview of related work for the MCSP without parallelization, we refer to the annotated bibliography of multiobjective combinatorial optimization \cite{EhrgottG02}, specifically Section ``6.1 Shortest path problems''.

% Bi-criteria

% Sonnier: approximate solutions for d \in {3,4}
Sonnier \cite{Sonnier06} were one of the first to publish parallel algorithms for solving MSP problems with 3 or 4 objectives. 
Their algorithms are based on a weighted sum approach and thus provide only an approximate solution, i.e., the solution set may not contain all Pareto-optimal paths.

% Sanders, Erb: first parallel MSP algo, but mostly for d=2
To the best of our knowledge, Sanders and Mandow \cite{SM13} published the first proposal for a parallel algorithm that provides an exact solution of the MSP.
Their approach extends Dijkstra's classical algorithm \cite{Dijkstra59} and relies on a so called \emph{Pareto queue}, a multi-dimensional generalization of a priority queue. 
While they give a high level description of the algorithm for arbitrary $d \geq 2$ and a detailed description of the bi-criteria case (which was further engineered by Erb \cite{Erb13}), they also state that efficient priority queues for $d\geq3$ are not yet known.

\section{Pareto optima} \label{sec:pareto_optima}
% Informally describe the problem
Given a set of vectors, the \emph{maximal vector problem} is to find the subset of vectors s.t.\ each vector of the subset is not dominated by any vector of the set. 
One vector dominates another if each of its components is equal to or smaller than (w.r.t.\ some partial ordering of the vectors) the corresponding component of the other vector, and strictly smaller in at least one component.
Such a maximal element is called \emph{Pareto-optimal} and the set of maxima is called the \emph{Pareto-set}. 

% Example
Figure \ref{fig:pareto_optima} illustrates the concept of Pareto-optimality. 
A formal definition is provided in the following.

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[xlabel=$w_1$, ylabel=$w_2$, enlargelimits=0.2, nodes near coords]
\addplot[mark=*, point meta=explicit symbolic, color=blue]
  table {
x	y
1	7.8
1.5	7.2
1.6	7
2.2	6.5
2.2	6
2.9	5.5
3.8	4.5
4	4.4
4.7	4.3
5	4
};

\addplot[mark=*, only marks, point meta=explicit symbolic, color=black]
  table {
x	y
1.3	7.8
2.4	7
2.8	6
3.2	7.1
4	5
4	7.7
4.3	6
4.6	7.4
5	5


};

\addplot[mark=*, point meta=explicit symbolic, color=blue] 
coordinates {
	(2.9, 5.5) [A]
};
\draw[blue,dashed] (axis cs:2.9,5.5) -- (axis cs:0,5.5);
\draw[blue,dashed] (axis cs:2.9,5.5) -- (axis cs:2.9,0);

\addplot[mark=*, point meta=explicit symbolic, color=blue] 
coordinates {
	(3.8, 4.5) [B]
};
\draw[blue,dashed] (axis cs:3.8, 4.5) -- (axis cs:0,4.5);
\draw[blue,dashed] (axis cs:3.8, 4.5) -- (axis cs:3.8,0);

\addplot[mark=*, point meta=explicit symbolic, color=black] 
coordinates {
	(3.2, 6.2) [C]
};
\draw[black,dashed] (axis cs:3.2, 6.2) -- (axis cs:0,6.2);
\draw[black,dashed] (axis cs:3.2, 6.2) -- (axis cs:3.2,0);

\addplot[mark=*, point meta=explicit symbolic, color=black] 
coordinates {
	(3.5, 5.2) [D]
};
\draw[black,dashed] (axis cs:3.4, 5.2) -- (axis cs:0,5.2);
\draw[black,dashed] (axis cs:3.4, 5.2) -- (axis cs:3.4,0);

\end{axis}
\end{tikzpicture}
\end{center}
\caption{Example of a Pareto-set in two-dimensional space: The set of Pareto-optimal vectors are in blue, others in black. Note that point C is dominated by A in both dimensions ($w_1(A) < w_1(C)$ and $w_2(A) < w_2(C)$). D is dominated by A in dimension 1 and by B in dimension 2; either condition on its own suffices for D not being Pareto-optimal.}
\label{fig:pareto_optima}
\end{figure}

\subsubsection*{Formal definition}
% Formal problem definition
Let $U_1, U_2,\dots , U_d$ be totally ordered sets, each with the binary relations $\less_i$  and $=_i$, (e.g., $\mathbb{N}$ and the ``smaller than'' and ``equals'' relations) and let $V$ be a set of $d$-dimensional vectors in $U_1 \times U_2 \times \dots \times U_d$.
For any $\vect{v} \in V$, we donate the $i$-th component of $\vect{v}$ by $w_i(\vect{v})$.

% Domination relation
Let $\vect{x}, \vect{y} \in V$. 
We say that $\vect{x}$ \emph{dominates} $\vect{y}$ ($\vect{x} \dom \vect{y}$) -- or equivalently, $\vect{y}$ \emph{is dominated by} $\vect{x}$ -- if and only if
\begin{align}
&\forall i \in \{1,\dots, d\}: w_i(x) \lesseq_i w_i(y) \quad \text{and}\\
&\exists j \in \{1,\dots, d\}: w_j(x) \less_i w_j(y) \quad .
\end{align}
Note that $\vect{x} \dom \vect{y} $ implies $\vect{x} \neq \vect{y}$ and that the binary relation $\moreeq_i$ can easily be defined by the other two relations.
For $d\geq 2$, the dominance relation ($\dom$) defines a partial order on $V$, but not a total order.
Thus, there exist vectors $\vect{x}, \vect{y} \in V$, $\vect{x} \neq \vect{y}$ for which neither $\vect{x} \dom \vect{y}$ nor $\vect{y} \dom \vect{x}$ holds.

% Pareto optima
A vector $\vect{x} \in V$ is a \emph{Pareto-optimum} if and only if
\begin{align}
\nexists \vect{v} \in V: \vect{v} \dom \vect{x} \quad ,
\end{align}
i.e., there is no vector $\vect{v}$ that dominates $\vect{x}$.
%Pareto set
A \emph{Pareto-set} $P \subseteq V$ is the set of all Pareto-optima of $V$.

\section{Pheet} \label{sec:pheet}
% what we will do in this section
In this section, we provide a brief introduction to Pheet-specific concepts that are relevant for this technical report. 
% reference Martin's dissertation
A detailed introduction to Pheet, the concepts it is built on and the research based on it can be found in the dissertation of Martin Wimmer \cite{Wimmer14}. 

\subsubsection*{Philosophy} \label{sec:pheet:philosophy}
% How is pheet implemented, what are its goals?
Pheet is a task-parallel programming library with the goal of providing a simple-to-use framework for the quick parallelization of algorithms.
Its flexible plug-in architecture based on C++ template meta-programming allows for any component in the task scheduling system to be replaced by an alternative implementation.
Additionally, Pheet provides a set of micro-benchmarks (that aim to evaluate specific aspects of the scheduling framework) and fine grained performance counters (that provide detailed insight into the scheduler and supporting data structures), which makes the framework a suitable platform for the implementation and testing of new components such as schedulers or priority queues. 

%\subsection{Concepts} \label{sec:pheet:concepts}

\subsubsection*{Places} \label{sec:pheet:places}
A \emph{place} denotes a single worker thread in the Pheet scheduling system that is pinned to specific processor, that is, a place will not migrate to another processing unit during execution.
Each processor utilized by Pheet is assigned exactly one place, implying that a processor is uniquely identified by it, which may be very useful for the implementation of parallel algorithms.
Furthermore, a place allows to implement processor-local data structures, e.g., parts of a distributed priority queue or application specific data structures and data accessible by the tasks that are executed on the associated processor.

\subsubsection*{Task parallelism} \label{sec:pheet:task_parallel}
% work pool
Pheet is based on the \emph{task-parallel model}, where a task is a small portion of work that is to be executed sequentially.
As soon as a processor is ready to execute work, it retrieves a task from the \emph{work pool}, which contains all the tasks that have yet to be executed.
A task may itself create additional tasks that are then stored in the work pool.

A famous algorithm based on the work pool pattern is Dijkstra's algorithm \cite{Dijkstra59} for the shortest path problem:
The algorithm requires a priority queue (that may be implemented as e.g., a simple binary search tree or a Fibonacci heap). This priority queue is a specialized work pool.

Note that in a task parallel program, the work pool is not necessarily a centralized data structure; each place may maintain its own. 

\subsubsection*{Task priorities and strategy scheduling}  \label{sec:pheet:task_priorites}
% What does the scheduler do?
The work pool exposes available work to the \emph{scheduler}, which is responsible for devising a schedule, i.e., a mapping of tasks to the processors.
% Global scheduling strategy
In standard task-parallel programming models, the scheduler typically employs a global policy that treats all tasks equally and thus independently of any task-specific properties.

% Task priorities 
Wimmer introduced the concept of \emph{task priorities} to make the scheduler aware of the preferred (relative) execution order of tasks. 
% Discrete priorities
One approach to handling priorities is to assign a discrete priority value (typically taken form a small set) to each task. 
% Comparison based prioritization
Another is a comparison based approach, where the programmer has to provide a comparator to the scheduler, that, given two tasks, decides which to execute first.

% Scheduling strategy
In Pheet, this comparator has to be implemented within the \emph{scheduling strategy} of a task.
A scheduling strategy is associated with a single task and may specify behavior that is depending on task specific criteria.
In contrast to a global scheduling policy, a scheduling strategy is associated with a single task and thus allows the programmer to influence the scheduler's behavior for a specific task.
% Dead tasks
An important application of this concept is the mechanism of \emph{dead tasks}, which allows to mark a spawned but not yet executed (or currently executing) task as obsolete. 
This mechanism is useful for speculative execution: A task may be spawned when it is likely that the work will of the task will have to be performed sometime in the future; when further calculations determine that the task need not be executed after all, the task can be marked dead via its associated strategy, thus instructing the scheduler not to execute the task.
Pheet handles dead tasks lazily, i.e., a dead task will not be dropped immediately but at a time it is convenient for the scheduler to do so, which allows for a more efficient implementation of the scheduler's data structures.


% Main part of the work
\section{Multi-criteria priority queue} \label{sec:mcpq}
% What is the MCPQ
The goal of the MCPQ is to provide a concurrent priority queue that can then be used by Pheet's strategy scheduler  to determine the execution order of tasks with multi-dimensional priorities of the form described in Section \ref{sec:pareto_optima}.


% Priorities and partial solutions
The priority of a task reflects the potential quality of the (partial) solutions it will produce upon execution.
By using multi-dimensional priority vectors, the quality of a solution may be measured by several, possibly conflicting metrics\footnote{We refer to our algorithm for the multi-criteria shortest path problem (definition see Section \ref{sec:shortest_path:defs:msp}; algorithm see Section \ref{sec:msp_algo}) for a concrete application of this concept.}. 
% Priority of task == desirability of execution
Obviously, it is desirable to execute the tasks that will produce (partial) solutions with the highest potential quality (i.e., tasks with highest priority) before other tasks.
% Pareto
Since each task is associated with a multi-dimensional priority vector, the tasks with highest priorities are the ones with a Pareto-optimal priority vector (w.r.t.\ the set of priority vectors of tasks \emph{currently} managed by the queue).

% abstract definition
The MCPQ is an \emph{ordered container} (as defined by Wimmer \cite{Wimmer14}), i.e., a bag-like data structure which orders tasks by their priorities.
It offers the following operations:
\begin{itemize}
\item \verb|push| adds a task to the container.
\item \verb|pop| returns a task that was previously added and removes it from the container. The order in which tasks are returned is determined by their priority.
\end{itemize}



% Outline
\begin{comment}
We start with a high-level discussion of the MCPQ, in particular of the requirements and major design choices (Section \ref{sec:mcpq:definition}).
Section \ref{sec:mcpq:description} provides a detailed description and analysis of the data structure, followed by a presentation of its implementation and integration into Pheet in Section \ref{sec:mcpq:pheet_integration}.

\subsection{Definition and requirements} \label{sec:mcpq:definition}
% Abstract datastructure description
\end{comment}

\subsubsection*{Ordering requirements} \label{sec:mcpq:ordering}
% ordering semantics  
The semantic definition of the \verb|pop| operation requires that \emph{all} threads agree on the set of tasks with Pareto-optimal priority, implying that all the \verb|push| and \verb|pop| operations executed by different threads need to appear to take affect in the same order for each thread, i.e., they need to be linearizable w.r.t.\ all the threads.
While these \emph{global ordering} semantics match the intuitive expectation, they also pose a (potentially big) performance bottleneck. 
\emph{Local ordering} semantics typically occur when each thread maintains its own, thread-local priority queue.
A task returned by \verb|pop| is required to have a Pareto-optimal priority only w.r.t.\ the tasks managed by the thread executing the operation; the priority queue of another thread might contain a task with strictly higher priority.
% pro and cons			
With local ordering semantics, a thread might execute unnecessary additional work (since the solutions created by the task will be of much lower quality than the ones created by a task with a dominating priority vector and thus will eventually be abandoned). 
However, this negative effect is countered by increased parallelism and less overhead due to synchronization.

\subsubsection*{Spying} \label{sec:mcpq:spying}
% Spying
When a thread does not have any more ready-tasks in its local priority queue, it will try to \emph{spy} some from the priority queue of another thread. 
A thread spies ready-tasks from another priority queue by copying references to the tasks to its own, thread-local priority queue.
Note that the tasks are then shared by multiple threads.
Once a thread pops a task it informs all the other threads that the task is already being processed and thus should not be executed by other threads anymore; this is done by marking a task as \emph{taken}.
The operations of the spying thread are linearized with the ones of all the other threads on the same priority queue.

\subsection{Description and analysis} \label{sec:mcpq:description}
% Purely local ordering semantics
The MCPQ observes only local ordering semantics and thus each thread maintains its own priority queue. 
% A general idea
% Concurrent and local part
The data structure is split into a part that may be accessed by several threads concurrently and a second part -- built on top of the first -- that may be accessed by the owning thread only.
% Virtual array
For the concurrent part, we assume a (potentially) infinite, thread-safe array-like data structure with linear (w.r.t.\ the capacity $n$ of data structure) access time for a whole sequence of $m\leq n$ items, called the \emph{Virtual Array}. 
The Virtual Array is used to hold all the tasks currently managed by the thread-local priority queue and allows other threads to scan it for tasks awaiting execution.
It is discussed in more detail in Section \ref{sec:virtual_array}.

% Thread-local part
The thread-local part is based on the idea of \emph{log-structured merge trees} (\cite{Wimmer14}, Section 5.7) and \emph{partitioning} to provide the operations \verb|push| and \verb|pop| as defined above.
To do so, the Virtual Array of size $n$ is subdivided into an ordered sequence of a logarithmic number of blocks.
Each block maintains a constant-size set of tasks whose priority is not dominated by any other task in that block
and provides the following two operations:
\begin{itemize}
\item \verb|peek| finds a highest priority task w.r.t.\ the tasks managed by the block\footnote{Note that \verb|peek| on a block is similar to \verb|pop| on the whole priority queue, except that it does not remove the returned task from the data structure.}.
\item \verb|take| removes a given task from the priority queue.
\end{itemize}

% Overview of pop
The \verb|pop| operation performs a \verb|peek| on each block to obtain a set of $O(\log n)$ tasks (one task from each block) that are not dominated w.r.t.\ to the tasks in their respective blocks.
This set is then scanned linearly to find a task that is not dominated by any other currently in the queue. 

% Blocks are just logical; offset
It is important to note that a block is only a logical construct which provides access to and manages a range of tasks stored in the virtual array. 
This range is identified by the offset $o$ of the block and its capacity.
Thus, any operation performed at position $i$ within a block is actually performed at position $o+i$ within the virtual array.

% Memory management: tricky, we use Pheet's capabilities.
Memory management for concurrent data structures can be hard, since a shared object may only be deleted if it can be guaranteed that the object is not accessed by any thread at the time of deletion.
Pheet therefore provides a wait-free memory reuse scheme (see \cite{Wimmer14}, Section 4.3).
We will not concern ourselves with memory management in this work, but make the following assumptions, justified by Lemmas 4.3.2 and 4.3.3 in \cite{Wimmer14}:

\begin{assumption} \label{ass:deleteIsConstant}
Deleting objects savely requires $O(1)$ time.
\end{assumption}

\begin{assumption} \label{ass:allocatedMemory}
When employing a memory manager to manage memory of size $n$, the size of the allocated memory is $O(n)$.  
\end{assumption}

% keys
As usual in the literature, we will denote the items managed by a priority queue as \emph{keys}.
In the context of this work, an item is a priority vector which is associated with a task.
We say that a key is deleted if it was returned by a \verb|pop| operation or its associated task was marked as taken or dead. 

% analysis only for sequential case
For the analysis, we will limit ourselves to the sequential case, i.e., 
\begin{assumption} \label{ass:sequential}
We assume that no keys are spied or marked deleted by another thread.
\end{assumption}

% outline
The concepts of log-structured merge-lists and partitioning are discussed in sections \ref{sec:mcpq:description:lsm} and \ref{sec:mcpq:description:partitioning}, respectively. 
Section \ref{sec:mcpq:description:merging_blocks} shows how the logarithmic number of blocks is maintained.
Operations \verb|peek| (Section \ref{sec:mcpq:description:peek}), \verb|take| (Section \ref{sec:mcpq:description:take}), \verb|push| (Section \ref{sec:mcpq:description:push}) and \verb|pop| (Section \ref{sec:mcpq:description:pop}) as well as \verb|spy| (Section \ref{sec:mcpq:description:spying}) are discussed as soon as all relevant concepts and structures have been introduced.


\newcommand{\itemWidth}{7mm}
% draw a block
% #1	id of each block element will be b\i#1
% #2	nr of block elements
% #3	Items will be numbered k_#3,...,k_#3+#2
% #4	xshift of first element

\newcommand{\block}[4]{
\pgfmathsetmacro\to{#2 - 1}
\foreach \i in {0,...,\to}{
    \pgfmathsetmacro\idx{\i + #3}
    \node[minimum size=\itemWidth, rectangle, draw, xshift=#4+(\i-1)*\itemWidth](b#1i\i) {\tiny $k_{\pgfmathprintnumber{\idx}}$};
}
}

\newcommand{\blockto}[5]{
\pgfmathsetmacro\to{#2 - 1}
\foreach \i in {0,...,\to}{
    \node[minimum size=\itemWidth, rectangle, draw, xshift=#4+(\i-1)*\itemWidth](b#1i\i) { };
}
\pgfmathsetmacro\tok{#5 - 1}
\foreach \i in {0,...,\tok}{
    \pgfmathsetmacro\idx{\i + #3}
    \node[minimum size=\itemWidth, rectangle, xshift=#4+(\i-1)*\itemWidth] {\tiny $k_{\pgfmathprintnumber{\idx}}$};
}
\node[anchor=north west] at (b#1i0.south west) {\tiny Block $b_{#1}$};
}


% #1 	number of keys, 0 <= #1 <= 2
% #2  	Items will be numbered k_#2,...,k_#2+#1
% #3 	xshift of first element
\newcommand{\insertblock}[3]{
\foreach \i in {0,...,1}{
    \node[minimum size=\itemWidth, rectangle, draw, xshift=#3+(\i-1)*\itemWidth](bai\i) {};
}
\ifnum #1 > 0
    \foreach \i in {1,...,#1}{
	\pgfmathsetmacro\idx{\i - 1 + #2}
	\node[minimum size=\itemWidth, rectangle, xshift=#3+(\i-2)*\itemWidth] {\tiny $k_{\pgfmathprintnumber{\idx}}$};
    }
\fi
\node[anchor=north west] at (bai0.south west) {\tiny Block $b_a$};
}

\begin{comment}
\begin{figure}
\begin{center}
\begin{tikzpicture}[>=stealth']
\foreach \i in {0,...,12}{
   \node[minimum size=\itemWidth, rectangle, draw, yshift=2cm, xshift=\i*\itemWidth](va\i) {\tiny $k_{\i}$};
}
\node[anchor=south west] at (va0.north west) {\tiny Virtual Array};

\block{0}{8}{0}{\itemWidth*0};
\block{1}{4}{8}{\itemWidth*9};
\insertblock{1}{12}{\itemWidth*15};

 
\draw[dotted] (b0i0.north west) -- (va0.south west);
\draw[dotted] (b0i7.north east) -- (va7.south east);

\draw[dotted] (b1i0.north west) -- (va8.south west);
\draw[dotted] (b1i3.north east) -- (va11.south east);

\draw[dotted] (bai0.north west) -- (va12.south west);
\draw[dotted] (bai1.north east) -- (va12.south east);

\end{tikzpicture}
\end{center}
\caption{Overview of the MCPQ structure: The Virtual Array provides an array-like data structure of (potentially) infinite capacity and can be accessed concurrently in a lock- and wait-free manner by multiple threads. A log-structured merge-list operating on the keys stored by the Virtual Array is used by the owning thread to find Pareto-optimal keys. }
\label{fig:structure}
\end{figure}
\end{comment}


% Virtual Array
\subsubsection{Virtual array} \label{sec:virtual_array}
% Motivation
The Virtual Array is a thread-safe array-like data structure with iterator-based access.
% concurrency
It may be traversed in forward direction by all threads, but only the owning thread may traverse it backwards or alter its structure. 
To ensure thread-safety, references to the items in the array need to be stored as atomic pointers.
% Operations
It provides the following operations:
\begin{itemize}
\item \verb|begin| obtains an iterator to the first item in Virtual Array.
\item \verb|end| obtains an iterator to the past-the-end\footnote{This is the theoretical item \emph{after} the last item in the array and thus the end-iterator does not point to any element.} item in the Virtual Array.
\item \verb|remove(iterator l, iterator r)| removes the range $]l, r[$ from the Virtual Array and frees the items stored there. 
\end{itemize}
An iterator provides the usual operations:
\begin{itemize}
\item \verb|++| (\verb|--|) moves the iterator to the next (previous) item.
\item \verb|advance(m)| applies the \verb|++| (if $m$ is negative: the \verb|--|) operator $m$ times.
\end{itemize}


% Implementation
The Virtual Array is implemented via a doubly-linked list of arrays of constant size $s_\text{v}$, called \emph{blocks}. 
% Analysis
The operations \verb|begin| and \verb|end| of the Virtual Array as well as the \verb|++| and \verb|--| operators of an iterator trivially require $O(1)$ time, while the \verb|advance| operation has time complexity\footnote{In practice, this operation can be speed-up to require $m/s_v$ time.} $O(m)$.
% Access time
% Deletion
To ensure thread safety, the \verb|delete| operation only removes whole blocks from the Virtual Array. 
Given iterators $l,r$, it removes the range $[l', r']$, where $l' = c_1 s_v$ is the minimal $l'$ s.t.\ $l < l' $ and $r' = c_2 s_v$ is the maximal $r'$ s.t.\ $r' < r $, for constants $c_1, c_2 > 0$.
Note that items in the range $]l, l'[$ and $]r', r[$ are not removed, i.e., after the \verb|remove| operation the range $]l, r[$ is not necessarily empty and may contain up to $2 s_v$ elements. 
It is the user's responsibility to always apply the operation with the largest possible range\footnote{I.e., removing the range $]l,r[$ cannot be done by using the \verb|remove| operation twice with the ranges $]l, k[$, $]k-1, r[$ for some $k$, $l<k<r$.} and including items that have not been removed although they were in the range of a previous \verb|remove| operation\footnote{Section \ref{sec:mcpq:description:partitioning} shows how this is done}.
The \verb|delete| operation requires $O(1)$ time.

\begin{lemma}
To store $n$ items, the Virtual Array allocates $O(n)$ memory.
\end{lemma}
\begin{proof}
Assuming that the \verb|remove| operation is applied as described above, the worst case is 1 non-deleted item per block. If a block does not contain any non-deleted items, the closest preceding non-deleted item is in a preceding block, while the closest succeeding item is in a succeeding block, implying that the block would have been deleted by a \verb|remove| operation. 
Since blocks are of constant size, the Virtual Array is of size at most $s_v n$.
\end{proof}

\subsubsection{Log-structured merge-lists (LSML)} \label{sec:mcpq:description:lsm}
\begin{figure}
\begin{center}
\subfigure [Structure of an LSM-L containing 7 keys.] {
    \begin{tikzpicture}[>=stealth']
    \block{0}{4}{0}{\itemWidth*0};
    \block{1}{2}{4}{\itemWidth*5};
    \insertblock{1}{6}{\itemWidth*8};

    \path[->, bend left=20] (b0i3) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i3);
    \end{tikzpicture}
}

\subfigure [After adding an additional key, block $b_a$ is full.] {
    \begin{tikzpicture}[>=stealth']
    \block{0}{4}{0}{\itemWidth*0};
    \block{1}{2}{4}{\itemWidth*5};
    \insertblock{2}{6}{\itemWidth*8};
    
    \path[->, bend left=20] (b0i3) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i3);
    \end{tikzpicture}
}

\subfigure [After the merge of blocks $b_1$ and $b_a$. A new, empty block $b_a$ is added. ] {
    \begin{tikzpicture}[>=stealth']
    \block{0}{4}{0}{\itemWidth*0};
    \block{1}{4}{4}{\itemWidth*5};
    \insertblock{0}{8}{\itemWidth*10};
    \path[->, bend left=20] (b0i3) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i3);
    \end{tikzpicture}
}

\subfigure [After the merge of blocks $b_0$ and $b_1$.] {
    \begin{tikzpicture}[>=stealth']
    \block{0}{8}{0}{\itemWidth*0};
    \insertblock{0}{8}{\itemWidth*9};
    \end{tikzpicture}
}
\end{center}
\caption{Inserting a key into an LSML.}
\label{fig:lsml_merge}
\end{figure}
% what is it; blocks; size
An LSML is a doubly-linked list of a logarithmic number of sorted arrays, called \emph{blocks}. A block has a capacity of $c 2^l$, where $c$ is the capacity of the smallest block and $l$ is the level of a block.
% nr keys in blocks, insert block b_0
A block of level $l$ contains exactly $c 2^l$ keys and at most one block of level $l$ is allowed.
Given $n$ keys, $n' = c \lfloor \frac{n}{c} \rfloor $ keys are stored in such blocks, while the remaining $n-n'$ keys are stored in an additional block with capacity $c$, called $b_a$, which thus contains 0 to $c$ keys.  
% size of blocks
The levels of the blocks can easily be determined by the binary representation of $n'$:
The level of the $i$-th block is equal to the index of the $i$-th non-zero bit in the binary representation.
% ordering of blocks
The blocks $b_i$ are ordered s.t.\ a block $b_{i}$ is of a strictly greater level than its successor $b_{i+1}$.
Let $b_l$ be the last block in this sequence.
Block $b_a$ is assigned level $0$ and made the successor of $b_l$ only once it is full.
% Merging
In case block $b_l$ is of level $0$, blocks $b_l$ and $b_a$ are \emph{merged} into a single block of level 1.
To maintain the logarithmic number of blocks, this merging operation is applied recursively, that is, two blocks of the same level $l$ are always merged to a single block of level $l+1$.
Note that a new, empty block $b_a$ is created after the old one was merged with another block.
Fig.\ \ref{fig:lsml_merge} illustrates the structure of an LSM-L as well as the merging operation triggered by the insertion of a new key.

\begin{figure}
\begin{center}
\subfigure [Structure of an LSML after keys $k_5$ to $k_7$ have been deleted.] {
    \begin{tikzpicture}[>=stealth']
    \blockto{0}{8}{0}{\itemWidth*0}{5};
    \block{1}{4}{8}{\itemWidth*9};
    \insertblock{1}{12}{\itemWidth*14};

    \path[->, bend left=20] (b0i7) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i7);
    \end{tikzpicture}
}

\subfigure [After removing another key, block $b_0$ is shrunk\dots] {
    \begin{tikzpicture}[>=stealth']
    \blockto{0}{4}{0}{\itemWidth*0}{4};
    \block{1}{4}{8}{\itemWidth*5};
    \insertblock{1}{12}{\itemWidth*10};
    
    \path[->, bend left=20] (b0i3) edge (b1i0);
    \path[->, bend left=20] (b1i0) edge (b0i3);
    \end{tikzpicture}
}

\subfigure [\dots and merged with its successor.] {
    \begin{tikzpicture}[>=stealth']
    \blockto{0}{4}{0}{\itemWidth*0}{4};
    \foreach \i in {0,...,3}{
	\pgfmathsetmacro\idx{\i + 4}
	\node[minimum size=\itemWidth, rectangle, draw, xshift=(\itemWidth*3)+(\i)*\itemWidth] {\tiny $k_{\pgfmathprintnumber{\idx}}$};
    }
    \insertblock{1}{12}{\itemWidth*9};
    \end{tikzpicture}
}

\end{center}
\caption{Deleting keys from an LSML.}
\label{fig:lsml_remove}
\end{figure}

% Removing keys
Deletion of keys is handled lazily: A key is at first merely marked as deleted. 
Once half of the keys of a block $b_i$ have been removed, it is shrunk to half its original capacity by decrementing its level. If necessary, the block is merged with its successor $b_{i+1}$ (blocks of level 0 will be deleted once empty; the block $b_a$ is never deleted or shrunk).
Note that at most one merge operation is necessary after shrinking a block.
Fig.\ \ref{fig:lsml_remove} illustrates this process.

\begin{corollary}
Merging two blocks requires constant time.
\end{corollary}
\begin{proof}
Since all operations are executed in-place on the Virtual Array, two blocks are merged by doubling the capacity (i.e., the range of keys maintained by the block) of the first and deleting the second block.
\end{proof}

\begin{lemma}
An LSML requires at most $2 + \lfloor \log n \rfloor$ blocks to store $n$ keys.
\end{lemma}
\begin{proof}
If no keys are deleted, this follows directly from the definition of the LSML's structure via the binary representation of $n' = c \lfloor \frac{n}{c} \rfloor$. 

Assume that $\lfloor \log n \rfloor + 1$ blocks of level $l>0$ are required to store $n$ keys.
Furthermore, assume $c=1$.
Once half of the keys of a block are marked as deleted, the block is shrunk to half its original capacity, implying that in a block of level $l>0$, strictly less than half of the keys can be marked for deletion.
Thus, $\lfloor \log n \rfloor$ such blocks contain at least $n/2 + 1$ non-deleted keys.
An additional block of level $l>0$ must be of level at least $\lfloor \log n \rfloor + 1$ (at most one block of any level may exist) and thus contain at least $2^{\lfloor \log n \rfloor + 1}/2 \geq n/2 $ non-deleted keys; a contradiction, since less than $n/2$ keys are left.
In addition to the $\lfloor \log n \rfloor$ blocks of level $l>0$, the additional block $b_a$ is always allocated and one block of level 0 containing just 1 key may be present.
For $c>1$, the number of blocks does not increase since the capacity of each is multiplied by $c$.
\end{proof}

Note that the size of each block is $O(n)$.
So far we have not discussed how a block actually manages the keys it contains (this will be done in Section \ref{sec:mcpq:description:partitioning}). 
We will make the following assumption to be able to provide a preliminary analysis of the LSML:
\begin{assumption}
To manage $n$ keys, a block used $O(n)$ space and the \textit{push}, \textit{peek} and \textit{take} operations on a \emph{single block} can be done in constant time. \todo{maybe just do the partitioning part first?}
\end{assumption}

\begin{corollary}
An LSML uses $O(n)$ space to store $n$ keys.
\end{corollary}
\begin{proof}
This follows directly from the previous lemma.
\end{proof}

\begin{lemma}
The \textit{push} operation requires $O(\log n)$ amortized time.
\end{lemma}
\begin{proof}
\end{proof}

\begin{lemma}
The \textit{pop} operation requires $O(\log n)$ amortized time.
\end{lemma}
\begin{proof}
\end{proof}


\subsubsection{LSML-Blocks} \label{sec:mcpq:description:partitioning}
%Idea
A block partitions the keys it holds by a pivot value, which is found by randomly\footnote{All random selections in this work are meant to be made \emph{uniformly and independently.}} selecting a key within the range to be partitioned and a random value $i \in [1,\dots,d]$ (where $d$ is the cardinality of the keys). 
The pivot element $x$ is then taken to be $x = k[i]$ (Algorithm \ref{algo:partition}). Our procedure (Algorithm \ref{algo:partition}) for in-place partitioning multi-dimensional keys is similar to the well-known partitioning algorithm for scalar values\footnote{In principle, one could take the whole key as the pivot element. However, this might make the partitioning infeasible if the set of Pareto-optima is large.}.

%pivot generation
\begin{algorithm}
\caption{Pseudo-code for the generation of a pivot element}
\label{algo:partition}
\begin{algorithmic}[1]
\Function{Pivot}{Block $b$, $left$, $right$}
\State Randomly select a key $k$ in the range $b[left],\dots, b[right]$
\State $i \gets$ random number in $[1,\dots,d]$
\State $x \gets k[i]$ 
\State \Return{$(x,i)$}
\EndFunction
\end{algorithmic}
\end{algorithm}


% partitioning
\begin{algorithm}
\caption{Pseudo-code for partitioning a block}
\label{algo:partition}
\begin{algorithmic}[1]
\Require{Block $b$; $left,right \leq$ size of $b$; $left\leq right$}
\Require{$s_p$: desired maximum size of right partition $p_r$}
\Function{Partition}{Block $b$, $left$, $right$}
\State $(x,i) \gets$ \Call{pivot}{$b$, $l$, $r$}
\While{$left < right$}
  \While{$left < right$ and (key $k = b[left]$,  $k \less_i x$)}
    \State $left = left + 1$
  \EndWhile
  \While{$left < right$ and (key $k = b[right]$, $k \moreeq_i x$)}
    \State $right = right -1 $
  \EndWhile
  \If{$left < right$}
    \State Swap the keys $b[left]$ and $b[right]$
    \State $left = left + 1$
    \State $right = right - 1$
  \EndIf
\EndWhile
\Statex \Comment{Check if key at $left$ belongs to left partition}
\If{$k = b[left]$, $k \less_i x$}
  $ left = left + 1$
\EndIf
\Statex \Comment{Recursively partition the right part until it falls below the cut-off value}
\If{$left < right$ and $(right-left) > s_p$}
  \State \Call{Partition}{$b$,$left$,$right$}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}
Keys with less priority (at dimension $i$) than the pivot element get sorted to the left, while keys with higher or equal priority are sorted to the right.
We will call these two parts the left and the right partition and use a subscript to indicate which partitioning step created them: The two partitions created by the first partitioning step are thus partitions dubbed $p_{l1}$ and $p_{r1}$.

%Pareto optimality
Note that keys that get sorted into the right partition are guaranteed to not be dominated by the ones in the left (since they have a higher priority value in at least one dimension).
Nevertheless, the left partition might contain Pareto-optimal keys; this behavior is acceptable since the MCPQ is required to find but one Pareto-optimal key and not the whole set of Pareto-optima.
The right partition $p_{r1}$ is partitioned recursively until it is smaller than a given constant cut-off value $s_p$, the maximum size that we are aiming for the right partition.
Note that the left partition $l_i$ of each partitioning step is not processed further and that the last right partition (for simplicity referred to as $p_r$) contains keys that are not dominated by any task in any of the left partitions, 
% the problem with the right partition
but it may itself contain two keys where one dominates the other. 
The \verb|peek| operation (Section \ref{sec:mcpq:description:peek} addresses this problem by linearly scanning the right partition $p_r$ to ensure that a non-dominated key is returned.
A different approach would be to flat out compute the Pareto-optima within $r_p$ by a linear expected-time algorithm as discussed in \cite{BentleyCL93, GodfreySG07}.

% figs and refs
Fig.\ \ref{fig:block_structure} the structure of a block after it was partitioned while Fig.\ \ref{fig:partition_with_nonactive}. provides an example for the partitioning process (both include some details that will be covered in the remainder of this section).

% problems we have here
In the algorithm outlined above, several important details still need to be addressed:

\medskip
\noindent
\textbf{Pivot generation}. As with other randomized algorithms, e.g., Quicksort, it is crucial to select ``good'' pivot elements to avoid worst-case behavior. 
To get reasonable pivot values (at least on expectation) we sample multiple times and then choose the median as our pivot element.  
\begin{corollary}
Generating a pivot element takes $O(n)$ time. 
\end{corollary}
\begin{proof}
Accessing a random item requires $O(n)$ times (due to the underlying Virtual Array). Accessing a constant number of items and selecting the median of those does not increase the asymptotic complexity. 
\end{proof}

\medskip
\noindent
\textbf{Ensuring termination}. The algorithm as given above does not necessarily terminate.
Assuming that all the keys managed by a block are equal, partitioning would sort all the keys into the right partition -- irregardless of which key is chosen as the pivot -- and then try to partition the very same elements again.
We call such an event a ``failed partitioning step''
This event may also occur if we choose the same pivot element as was used by the last partitioning step.
While in this case we can try to generate a new, different pivot element, the former case does not allow for further partitioning and we ensure termination by aborting the recursive procedure after a constant number of successive failed partitioning steps.
In this case, the size of the right-most partition will be larger than the size we are aiming for; nevertheless, it is guaranteed that the keys it contains are not dominated by keys that have been sorted to the left.
In the following, we will make the following assumption:
\begin{assumption} \label{ass:partition_succeeds}
After partitioning a block, its right partition $p_r$ is smaller than or equal to the constant cut-off value $s_p$. 
\end{assumption} 

\newcommand{\partition}[4]{
  \node[minimum width=#4, minimum height=8mm, rectangle, draw, anchor=west] at (#2.east) (#1) {};
  \node[anchor=north west] at (#1.south west) {\tiny #3}
}

% structure of a block
\begin{figure}
\begin{center}
\begin{tikzpicture}
\node (n0) {};
\partition{n1}{n0}{$p_{l_1}$}{30mm};
\partition{n2}{n1}{$p_{l_2}$}{20mm};
\node[anchor=west,yshift=2.5mm] at (n2.east) (d1) {\tiny\dots};
\node[anchor=west, minimum width=6mm] at (n2.east) (d0) {};
\node[anchor=west,yshift=-2.5mm] at (n2.east) (d1) {\tiny\dots};
\partition{n3}{d0}{$p_{l_i}$}{15mm};
\partition{n4}{n3}{$p_{r}$}{15mm};
\partition{n5}{n4}{$p_{d}$}{25mm};
\end{tikzpicture}
\caption{Structure of a block after partitioning. From the left to the right, we have 1) several left partitions $p_{li}$, one created by each partitioning step; 2) the right partition $p_r$, containing keys not dominated by any key in the left partitions; and 3) the section $p_{d}$ containing the keys marked for deletion. 
}
\label{fig:block_structure}
\end{center}
\end{figure}


\medskip
\noindent
\textbf{Handling dead, taken and removed tasks}. Once a key was returned by \verb|pop|, it has to be removed from the queue. 
Furthermore, keys associated with tasks that have been marked dead or taken also need to be removed. 
% active, non-active tasks
We will subsume all those tasks as ``non-active'' and its associated keys as deleted, whereas a task still awaiting execution is called an active task,
% How a block handles dead tasks
Speaking informally, each block moves its deleted keys to the end of the block and keeps them there.
Once the block gets merged with another block, the deleted keys of both blocks are moved to the end of the block generated by the merge and thus the deleted keys gradually accumulate in consecutive ranges and, once the number of deleted keys is large enough, part of them can be removed via the \verb|remove| operation of the Virtual Array. Smaller ranges will move to the end of the LSML and accumulate there.

\begin{algorithm}
\caption{Pseudo-code for partitioning with deleted keys}
\label{algo:partition_with_nonactive}
\begin{algorithmic}[1]
\While{$left < right$}
\State key $k \gets b[left]$
\If{$k$ is a non-deleted key}
    \If{$k \less_i x$}
    \State $left = left + 1$  \Comment{If $k$ has less priority than the pivot, it stays}
    \Else
    { break;} \Comment{Otherwise, it is moved to the right partition}
    \EndIf
\Else \Comment{$k$ is a deleted key}
    \State $delete = delete - 1$; 
    \State swap keys $b[left]$ and $b[delete]$
    \If {$right == delete$} $right = right - 1$
    \EndIf
\EndIf
\EndWhile
\While{$left < right$}
\State key $k \gets b[right]$
\If{$k$ is a non-deleted key}
    \If{$k \moreeq_i x$}
    \State $right = right - 1$   \Comment{If the pivot has less priority than $k$, $k$ stays}
    \Else { break;} \Comment{Otherwise, $k$ is moved to the left partition.}
    \EndIf
\Else \Comment{$k$ is a deleted key}
    \State $delete = delete - 1$
    \If {$right == delete$} $right = right - 1$
    \Else
    \State Swap the keys $b[right]$ and $b[delete]$;
    \EndIf
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

% extended algorithm
The partitioning procedure is adapted as follows to handle deleted keys (Algorithm \ref{algo:partition_with_nonactive} replaces lines 4-9 in Algorithm \ref{algo:partition}):
\begin{itemize}
\item An additional partition, called $p_{d}$, is used for storing the deleted keys. The index $delete$ is used to keep track of it and is initially set to $right$, i.e., the end of the range to be partitioned.
\item If a deleted key is encountered during the scan for the next key to swap, it is swapped out to the section $p_d$ holding the deleted keys. 
\end{itemize}

\begin{lemma}
The partition operation has worst-case complexity $O(n^2)$.
\end{lemma}
\begin{proof}
The complexity of Algorithm \ref{algo:partition} is dominated by the outer while-loop (lines 3-15) and the recursive call (generating a pivot element requires $O(n)$, the checks in lines 16 and 18 require constant time). 
The loop is executed at most $n$ times, since either $left$ is incremented or $right$ is decremented at least 1 in each execution. Each execution of one of the inner loops (lines 4-6 and 7-9, respectively) reduces the number of executions of the outer loop by 1. 

The same argument holds for the modification (given in Algorithm \ref{algo:partition_with_nonactive}, with one exception: When, while scanning from left (right), a deleted key is encountered, it is swapped to section $p_d$. 
If $delete$ and $right$ point to the same location, $right$ can be decremented since the key it points to is a deleted one and therefore does not have to be moved anymore. 
If that is not the case however, neither $left$ nor $right$ may be advanced, since the items at their respective locations have not been checked yet. 
This, however, can happen at most $n$ times for the whole block, since $right <= delete$ always holds and thus the worst-case complexity does not change.

Due to Assumption \ref{ass:partition_succeeds}, we know that each successive call reduces the size of the range that remains to be partitioned by at least 1 item, in a constant number of partitioning attempts. 
Thus, the recursion depth is bounded by $O(n)$ and partitioning requires $O(n^2)$ time in the worst case.
\end{proof}

\begin{conjecture}
Partition has expected time complexity $O(n \log n)$.
\end{conjecture}
The conjecture is immediate if one assumes that each pivot element is taken from a key close to the median (when ordered keys by their values at dimension $i$, which the pivot is taken from) to be partitioned, since each partitioning step would half the size of the range that requires further partitioning.
To prove the conjecture formally, assumptions about the distribution of the keys would have to be made. 

The deleted keys are only touched once to move them to the section of deleted items and that the number of non-deleted items in a block never increases. This further indicates reasonable performance in practice.


\subsubsection*{Removing deleted keys}
After partitioning the whole block, its level is reduced as far as possible (Algorithm \ref{algo:reducellvl}) and, in case the section containing deleted keys is large enough, part of them are removed from the LSML via the \verb|remove| operation of the Virtual Array.
The level $l$ of a block $b$ can be reduced if
\begin{itemize}
\item $l > 0$ (the minimum level of a block is $0$), and 
\item the level of the block succeeding $b$ is is smaller than $l$ (if such a block exists), and
\item all non-deleted keys are covered after reducing the level, i.e., the $delete$ index must remain inside the range of keys covered by the block.
\end{itemize}

\begin{algorithm}
\caption{Pseudo-code for shrinking a block}
\label{algo:reducellvl}
\begin{algorithmic}[1]
\Function{Shrink}{Block $b$}
\State $l \gets $ level of $b$ 
\State $l_s \gets $ level of the successor of $b$
\While{$l > 0$ and $l_s < l$ and $delete < s_p  2^{l-1}$}
\State $l = l-1$
\EndWhile
\State \Call{remove}{$delete$, end of block $b$} \Comment{Step 3: Remove deleted keys}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{corollary}
The \textsc{Shrink} operation has time complexity $O(\log n)$.
\end{corollary}
\begin{proof}
The loop is executed at most $l$ times. Since $n = 2^{\log l}$ (by definition of a block), the number of executions is bounded by $O(\log n)$ times while removing a range of items from the Virtual Array requires $O(1)$ time (see Section \ref{sec:virtual_array}).
\end{proof}

\begin{lemma} \label{lemma:partitionandshrink}
After partitioning and shrinking a block, it uses $O(n)$ memory to store $n$ keys. 
\end{lemma}
\begin{proof}
\textsc{Partition} moves all deleted keys to the section of deleted keys, and \textsc{Shrink} extends this section as far as possible. It follows directly from the call of the Virtual Array's \textsc{Remove} operation at the end of the \textsc{Shrink} procedure at most a constant number of keys are stored by the block in addition to the $n$ keys.
\end{proof}

\subsubsection{Peek} \label{sec:mcpq:description:peek}
\begin{algorithm}
\caption{Pseudo-code for the peek operation}
\label{algo:peek}
\begin{algorithmic}[1]
\Require{Block $b$ is partitioned into partitions $p_{l1},\dots , p_{li}, p_r, p_{d}$} 
\Function{peek}{Block $b$}
\State key $k = null$ 
\ForAll{Keys $x$ in $p_r$} \Comment{Scan the right partition}
  \If{$k == null$ or $x \less k$} $k = x$
  \EndIf
\EndFor
\If{$k == null$} \Comment{No non-deleted keys in the right partition}
  \State Extend $p_{d}$ to include $p_r$ \Comment{$p_d$ now covers $p_r$}
  \If{$p_{l1} < p_{d}$} \Comment{Check if some non-deleted keys exist in the block}
    \State Set $p_r \gets p_{li}$
    \State \Call{Partition}{$b$, $p_r$, $p_{d}$} \Comment{Further partition $p_r$}
    \State \Call{Shrink}{$b$} \Comment{Shrink the block as far as possible}
  \EndIf
  \State $k \gets $ \Call{peek}{$b$}	\Comment{Call peek again on the new right partition}
\EndIf
\State \Return{$k$}
\EndFunction
\end{algorithmic}
\end{algorithm}
% peek
Once a block is partitioned, the \verb|peek| operation (Algorithm \ref{algo:peek} is relatively easy:
We simply need to scan all the non-deleted keys in the right partition. The first encountered key is kept as a candidate and compared to all remaining keys in the partition to ensure that it is not dominated by any of them; if it is, the dominating key is taken as the new candidate.
% falling back
In case no such key is found in the right partition, we fall back to the previous partition: since the right partition does not contain any key, it is merged with the section holding the deleted keys; the left partition with highest index is then taken as the new right partition.
This partition might have to be partitioned further, since its size is bounded from above only by $O(n)$ and thus may exceed the size $s_p$ that we aim for the right partition. 
Once the top partition falls below $s_p$, we can again scan for a non-dominated key.
% dead block
In case no such key can be found (i.e., after falling back to the last existing partition) the whole block is marked as deleted, implying that no more peek operations have to be executed on it.
Note that when falling back to another partition, \verb|peek| shrinks the block to ensure that it uses $O(n)$ memory (see Lemma \ref{lemma:partitionandshrink}) but does not merge blocks in case the successor of block $b$ has the same level. 
It will be shown in the following how the \verb|push| operation ensures the logarithmic number of blocks after using \verb|peek| on a block.

\begin{lemma}
The \verb|peek| operation has amortized time complexity $O(\log n)$.
\end{lemma}
\begin{proof}
The for-loop (lines 3-6) has constant complexity because the size of the right partition $p_r$ is bounded by the constant $s_p$, due to Ass.\ \ref{ass:partition_succeeds}.

We amortize the cost of the recursive calls to $\verb|peek|$ as well as to \textsc{Partition} over the number of keys in a block:

All calls to \textsc{Partition} are made with non-overlapping ranges of keys and thus for $n$ \verb|peek| operations the whole block is partitioned only once, i.e., the partitioning operations caused by $n$ calls to \verb|peek| require $O(n\log n)$ time in total.

A block is partitioned into at most $n$ partitions, implying that $n$ \verb|peek| operations cause at most $n$ fall backs. 
\verb|peek| is called once for each fall back and thus $n$ \verb|peek| operations cause at most $n$ recursive calls in total.

The complexity of \verb|peek| is dominated by \text{Partition} and thus has an amortized complexity of $O(\log n)$.
\end{proof}

\subsubsection{Take} \label{sec:mcpq:description:take}
While the \verb|peek| operation finds a Pareto-optimal key without removing it from the queue, the \verb|take| operation removes a given key from the MCPQ by marking it as deleted.
Subsequent \verb|push| and \verb|pop| operations will eventually remove the deleted key from the priority queue.
The \verb|take| operation thus trivially has complexity $O(1)$.

\subsubsection{Merging blocks} \label{sec:mcpq:description:merging_blocks}
\begin{algorithm}
\caption{Pseudo-code for merging several blocks}
\label{algo:merge_from}
\begin{algorithmic}[1]
\Function{MergeFrom}{Block $b$}
\State let $b_p$ be the block preceding $b$
\State $merged \gets false$
\While{$b_p$ exists and is either of same level as $b$ }
\State combine $b_p$ and $b$
\State $b \gets b_p$
\State $b_p \gets$ predecessor of $b$
\State $merged \gets true$
\EndWhile
\State \Return{$merged $}
\EndFunction
\end{algorithmic}
\end{algorithm}

As outlined above, all items that are added to the MCPQ are first stored in a special block of level 0, called $b_a$. 
Once this block is full, a new block of level 0 is created and all keys from the block $b_a$ are moved there.
If another block of level 0 already exists, the two blocks are merged to create a block of level 1. 
This operation is continued as long as there are two blocks of same level, as shown in Algorithm \ref{algo:merge_from} (note that due to the structure of the LSML, two blocks of same level are always neighbors).
Note that the resulting block $b$ will have to be partitioned before the next \verb|peek| operation. 

\begin{corollary}
The \textsc{MergeFrom} operation has worst-case time complexity $O(\log n)$.
\end{corollary}
\begin{proof}
Since there are $O(\log n)$ blocks in an LSML, the while loop (lines 4-9) requires at most $O(\log n)$ time (in case all the blocks are combined) and the resulting block is of size at most $n$ 
\end{proof}

\subsubsection{Push} \label{sec:mcpq:description:push}
Adding keys to the MCPQ via the \verb|push| operation is quite simple, as shown in Algorithm \ref{algo:push}. 
In case the block $b_a$ is full, a new block of level 0 is created and all keys from block $b_a$ are moved there. Blocks are then merged, if necessary.

\begin{algorithm}
\caption{Pseudo-code for the push operation}
\label{algo:push}
\begin{algorithmic}[1]
\Require{Insert-block $b_\text{insert}$}
\Function{Push}{Key $k$}
\If{$b_a$ is full}
  \State Create a new block $b$ of level 0 at the end of the linked list
  \State Move all elements from $b_a$ to $b$
  \If{\Call{MergeFrom}{$b$}}
    \State \Call{Partition}{$b$}
  \EndIf
\EndIf
\State Insert $k$ in $b_\text{insert}$
\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{table*}
\begin{center}
    \begin{tabular}{| r | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c |}
    \hline
    $i$: &  1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16\\
    \hline
    Processed keys:  & 0 & 2 & 0 & 4 & 0 & 2 & 0 & 8 & 0 & 2 & 0 & 4 & 0 & 2 & 0 & 16\\ 
    \hline
    \end{tabular}
\end{center}
\caption{Number of keys processed in a merge operation after inserting keys $i s_p$ keys.}
\label{table:nrMerges}
\end{table*}

\begin{lemma} \label{lemma:push}
The \verb|push| operation has amortized time complexity $O(\log^2 n)$.
\end{lemma}
\begin{proof}
In case the block $b_a$ is not full, \verb|push| trivially requires $O(1)$ time. 
Otherwise, creating a new block and moving all keys from block $b_a$ to the new block can be done in constant time (the number of keys in $b_a$ is equal to the constant $s_p$).
It follows that \textsc{Partition} dominates the complexity of the operation. 

We amortize its cost over $n$ \verb|push| operations: 
The first time block $b_a$ is full, no merge is required since no other block exists. 
The second time however the new block needs to be merged with the other block of level 0, prompting the partitioning of $2s_p$ keys.
Once $4 s_p$ keys have been inserted, one additional merge of $4 s_p$ keys is required.
However, no additional merge of $2 s_p$ keys is done. 
This can be seen as follows: 
Inserting $3 s_p $ keys creates one block of level 1 and one block of level 0. 
After adding $s_p$ more keys, a new level 0 block is created and, since now two blocks of same level exist, \textsc{MergeFrom} will combine them to a level 1 block. 
Now that two blocks of level 1 exists, they are combined into a level 2 block, which is then partitioned.
Table \ref{table:nrMerges} list the number of keys processed in a merge and partition operation after a given number of keys have been added.

One merge operation is required every $2 s_p$ keys, i.e., $n/2s_p$ merge operations are required for $n$ keys. 
This is clearly less than one merge of $2 s_p$ keys every $2 s_p$ keys plus one merge of $4 s_p$ keys every $4 s_p$ keys and so on. 
The number of keys processed by the partition operation after a merge operations is thus at most  $m = n \log n$ and pushing $n$ keys requires at most 
$$
m \log m = n \log n \log (n \log n) = n \log^2 n + n \log n \log \log n
$$
time, implying \verb|push| has amortized complexity $O(\log^2 n)$.
\end{proof}


\subsubsection{Pop} \label{sec:mcpq:description:pop}
To find a non-dominated key, \verb|peek| is used to generate a set of $O(\log n)$ keys (one per block), where each key is Pareto-optimal w.r.t.\ the keys stored in the block it was obtained from. 
Thus, this set needs to be scanned to ensure that a key that is not dominated by any other in the set is returned. \todo{We have a problem here: the ``not dominated'' relation is not transitive; thus, the key we eventually return might be dominated by one of the keys in another block}.
In case the number of non-deleted keys in a block after a \verb|peek| operation drops below half the block's capacity, the block is shrunk. 
To maintain the logarithmic number of blocks, a merge of $b$ with its predecessor (in case the block was shrunk to the same level as $b$ in the previous iteration of the while loop), or with its successor (in case $b$ got shrunk to the same level as its successor and the successor is not shrunk in the next iteration) might be required. 
However, at most one of these situations 

In case the predecessor of block $b$ was shrunk to 
Block $b$ is merged with its predecessor $b_p$ in case $b_p$ was shrunk in the previous iteration of the while loop or with its successor $b_s$ in case $b$ gets shrunk to the same level of $b_s$. 
Thus, at most one of those merge operations can occur.

\begin{algorithm}
\caption{Pseudo-code for the pop operation}
\label{algo:pop}
\begin{algorithmic}[1]
\Require{Block $b_a$}
\Function{pop}{}
\State Key $k_{best} \gets$ \Call{peek}{$b_a$}
\State Block $b_{best} \gets b_a$
\State Block $b \gets b_a $
\While{$b.next$ exists}
  \State $b \gets b.next$
  \State Key $k \gets$ \Call{peek}{$b$}
  \If{$k \neq null$}
    \If{$k_{best} == null$ or $k \less k_{best}$}
      \State $k_{best} \gets k$
      \State  $b_{best} \gets b$ 
    \EndIf
  \EndIf
  \If{\Call{MergeFrom}{$b$}}
    \State mark block $b$ for partitioning
  \EndIf
\EndWhile
\ForAll{blocks $x$ marked for partitioning}
    \State \Call{Partition}{$x$}
\EndFor
\State \Return{$k$}
\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{lemma}
The \verb|pop| operation has amortized time complexity $O(\log^2 n)$.
\end{lemma}
\begin{proof}
The number of executions of the while loop is bounded by the number of blocks and is thus $O(\log n)$. 
\verb|peek| and \textsc{MergeFrom} each require $O(\log n)$ time and thus the while loop has complexity $O(\log n)$.

The cost of \textsc{Partition} can be amortized over $n$ \verb|pop| operations to $O(\log^2 n)$ with similar arguments as used for the analysis of the \verb|push| operation (Lemma \ref{lemma:push}).
\end{proof}

So far, for the analysis of \verb|push| (\verb|pop|) we assumed that a sequence of $n$ operations was not interrupted by any \verb|pop| (\verb|push|) operations. 
Regarding general sequences of \verb|push| and \verb|pop|, we make the following claim (without proof): 
\begin{conjecture}
The amortized asymptotic complexity of $n$ \verb|push| (\verb|pop|) operations does not change if some keys are popped (pushed) in the meantime.
\end{conjecture}

\subsubsection{Spying} \label{sec:mcpq:description:spying}
In case thread does not have any ready-tasks in its own local priority queue, it may \verb|spy| some from another thread. 
A thread spies keys from another thread, called the victim, by copying them into its own priority queue.
The key itself is not altered or deleted from the victims priority queue and it has to be ensured by appropriate synchronization mechanism that two \verb|pop| operations executed by different threads do not return the same key.
This can be achieved with the help of an atomic flag, associated with each key: before \verb|pop| returns a key, it has to check that the flag is not set before setting the flag itself, all in one atomic operation. 
This variable is called the \verb|taken| flag and taken keys are dealt with in the same way as deleted keys.

\begin{algorithm}
\caption{Pseudo-code for the spy operation}
\label{algo:spy}
\begin{algorithmic}[1]
\Function{spy}{}
\State Place $other \gets item.owner$
\For{$i \gets other.begin$; $i < other.end$; $i = i+1 $}
  \State Key $k \gets other[i]$
  \If{$k$ is a non-deleted key}
    \Call{Put}{$k$}
  \EndIf
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

A \verb|spy| operation is performed by scanned the keys of another thread's priority queue (via the Virtual Array) and adding each non-deleted key to the thread-local priority queue via \verb|push|.

\begin{corollary}
The \verb|spy| operation has complexity $O(n \log^2 n)$.
\end{corollary}
\begin{proof}
This follows immediately from \verb|push| having complexity $O(\log^2 n)$.
\end{proof}


\subsection{Integration in Pheet} \label{sec:mcpq:pheet_integration}
% Strategy scheduler 2
Pheet's \verb|StrategyScheduler2|, a \emph{two-level concurrent ordered container} (described in detail in \cite{Wimmer14}, Section 5.5) allows for a quick integration of our MCPQ into the scheduling framework.
% Base classes provided; pheet terminology
It offers base classes for alternative \emph{task storage}\footnote{In Pheet terminology, a priority queue managing tasks is called a task storage.} implementations.
% The classes we us.
In particular, to integrate the MCPQ into the Pheet scheduling framework, we will be extending the following classes:
\begin{inparaenum}
\item \verb|Strategy2BaseTaskStorageBaseItem|,
\item \verb|Strategy2BaseTaskStoragePlace| and
\item \verb|Strategy2BaseTaskStorage|.
\end{inparaenum}
We will give an intuitive understanding of their functionality required by our implementation together with a detailed description of the classes extending those base classes in the following. 
For a detailed description of the base classes and correctness proofs, we refer the reader to \cite{Wimmer14}.


\subsection{Implementation} \label{sec:mcpq:impl_analysis}
% A general, high level overview and interfaces
The class \verb|ParetoItem| (Listing \ref{lst:PTSItem}) is used to represent tasks managed by scheduler. It references an object of type \verb|SchedulerTask| (Listing \ref{lst:SchedulerTask}), which contains the code to be executed, i.e., the work the task is to perform, and an object implementing the \verb|ParetoStrategy| interface (Listing \ref{lst:ParetoStrategy}), which handles the priority of the task and allows to mark it as dead.

The \verb|ParetoItem| class has to guarantee that its \verb|take| operation returns the referenced task exactly once, even if multiple threads execute it concurrently. 
Although an item is at first only visible to the thread creating it (also called the \emph{owning} place), this may change after another thread performs a \verb|steal| operation on the priority queue of the owning place.
The base class \verb|BaseItem| provides some functionality for memory re-usage and serves as an interface for \verb|StrategyScheduler2|, which is oblivious of the task storage implementation.	

% ParetoItem
\begin{code}[label=lst:PTSItem, caption=Interface of ParetoItem]
class ParetoItem : BaseItem {

  /* The task to execute. */
  SchedulerTask task;
  
  /* The strategy associated with the task. */
  ParetoStrategy strategy;
  
  /* If false, no thread is processing this item yet. */
  atomic<bool> taken;
  
  /* Get the task referenced by this item if it is not taken. A task may be taken only once. */
  SchedulerTask take();
}
\end{code}

The \verb|SchedulerTask| and \verb|ParetoStrategy| classes have to be implemented by the application employing the scheduler, i.e., the application programmer.
While the former contains the code to be executed upon execution of the task, the latter provides access to the priority vector associated with a task.
Furthermore, the function \verb|priorizite|, given another \verb|ParetoStrategy| object, decides which of the two has higher priority. 
The \verb|dead_task| function is used by the scheduler to query whether the associated task may be dropped without being executed\footnote{This feature, called ``speculative execution'', is described in Section \ref{sec:pheet:task_priorites}}.
Both functions will be used by our task storage implementation to order tasks w.r.t.\ their associated priorities.

% Task
\begin{code}[label=lst:SchedulerTask, caption=Interface of SchedulerTask]
/* To be implemented by the application. The code in execute() will be run when the task is executed. */
class SchedulerTask {

  void execute();
}
\end{code}

The central -- or global -- part of the MCQP is implemented in \verb|ParetoTaskStorage| (Listing \ref{lst:ParetoTaskStorage}), which is mostly used for rerouting \verb|pop| and \verb|steal| operations to the place-local parts of the MCPQ, implemented in \verb|ParetoTaskStoragePlace| (Listing \ref{lst:ParetoTaskStoragePlace}). 
% boundary
Notice that the \verb|pop| and \verb|steal| operations of both classes have an additional parameter called the boundary item. 
The \verb|StrategyScheduler2| maintains its own LIFO priority queue\footnote{This is necessary for some advanced features the base data structures provide. We do not require them, they are not introduced in this report.} and the boundary item would be the next item selected by \verb|pop|/\verb|steal| according to this order.
However, this default behavior may be overridden within certain limits: 
\begin{itemize}
\item The \verb|pop|/\verb|steal| operation may select other items with a \emph{higher} priority than the boundary item, thereby violating the LIFO order. Since we are using multi-dimensional priority vector, this means that the priority associated with the selected item may not be dominated by the priority of the task referenced by the boundary item.
\item The \verb|pop| operation must not return a \verb|SchedulerTask| if the boundary item was already taken by another thread in the meantime.
\end{itemize}

% TaskStoragePlace
\begin{code}[label=lst:ParetoTaskStoragePlace, caption=Interface of ParetoTaskStoragePlace]
/* The place-local part of the MCPQ. */
class ParetoTaskStoragePlace : Strategy2BaseTaskStoragePlace {

  /* Add the given item to the task storage. */
  void push(ParetoItem item);
  
  /* Get an item with locally non-dominated priority and remove it from the task storage. */
  SchedulerTask pop(ParetoItem boundary);
  
  /* Get an item (with locally non-dominated priority) by stealing it from another place. */
  SchedulerTask steal(ParetoItem boundary, ParetoTastStoragePlace other_place);
}
\end{code}

% TaskStorage
\begin{code}[label=lst:ParetoTaskStorage, caption=Implementation of ParetoTaskStorage]
/* The global part of the MCPQ. Calls to pop and steal are simply forwarded to the responsible places. */
class ParetoTaskStorage : Strategy2BaseTaskStorage {

  ParetoTaskStoragePlace[] places;
  
  SchedulerTask pop(ParetoItem boundary, int place_id) {
    return places[place_id].pop(boundary);
  }
  
  SchedulerTask steal(ParetoItem boundary, int place_id) {
    return places[place_id].steal(boundary);
  }
}
\end{code}


\section{Shortest path problems} \label{sec:shortest_path}
% Informally introduce shortest path
The problem of finding a shortest path between two nodes in a graph is a classical problem in computer science with numerous practical applications, such as finding the quickest route from location A to location B.
% Introduce the problem of msp
While the \emph{shortest-path problem} minimizes for a single criteria, e.g., the travel time, in practical applications one often wants to optimize multiple -- and possibly conflicting -- objectives:
When searching for a route from A to B, we may want to minimize the travel time as well as toll costs.
In such a setting, we usually cannot give a single best solution anymore. 
Instead, we can give several reasonable solutions: 
The fastest route will usually use highways, thus increasing the toll costs, while a slightly slower route may reduce the toll costs by avoiding highways. 
Problems like this can be modeled as \emph{multi-criteria shortest path} (MSP) problems, where we are interested in all optimal alternatives, i.e., in routes that are \emph{Pareto-optimal}. 
A route from A to B is Pareto-optimal if there is no other route with less or equal cost for each of the $d$ objectives.

%subsection{Definitions and notation} \label{sec:shortest_path:defs}
In the following, we provide definitions for variations of the shortest path problems considered in this report. 
The classic text book by Cormen et al.~\cite{CLRS01} provides a more detailed introduction to basic shortest path problems and algorithms.

% Define single-source shortest path
\subsection{The classical shortest path problem} \label{sec:shortest_path:defs:sssp}
We are given a weighted directed graph $G=(V,A)$, where $V= \{v_1, v_2, \dots, v_n\}$ is a finite set of nodes, $A \subseteq V \times V$ is a finite set of arcs (directed edges) and $c: A \rightarrow \mathbb{N}$ is the cost or weight function. 
We define the set of neighbors of a node $v$ as the nodes adjacent to an outgoing arc of $v$, i.e., $\{w \in V \mid \exists a\in A, a = (v,w) \}$.
W.l.o.g., we assume that $G$ is a connected graph.

Let $p = \langle v_0, a_1, v_1, a_2, \dots,a_k, v_k \rangle$ be a path\footnote{We will also write $p = \langle v_0, v_1, \dots,v_k \rangle$, since an arc $a_i$ is well defined by its two adjacent nodes.} from $v_0$ to $v_k$ in $G$.
Furthermore, let $P$ be the set of all paths and $P_{u,v}$ be the set of all paths from node $u$ to node $v$ in $G$.
We define the weight $w(p)$ of a path $p$ as 
\begin{align*}
w: P &\rightarrow \mathbb{N} \\
p &\mapsto w(p) = \sum_{i=1}^{k}c(a_i) \quad , \neqn{sssp_weighted}
\end{align*}
i.e., the sum of costs all the arcs in the path $p$.

% Variations: Single source, all-pairs,... shortest path
In a \emph{single-source shortest path problem (SSSP)}, we want to compute a shortest path w.r.t.~some weight function $w$ from a given source node $s \in V$ to each node $v \in V$.
Different variations of the problem exist, such as the \emph{single-destination}, the \emph{single-pair} or the \emph{all-pairs} shortest path problem. 
Note that an optimal algorithm for the SSSP can easily be turned into an optimal algorithm for the first two variants \cite{CLRS01}.
Thus, we will only deal with the SSSP in this report.

\subsection{Multi-criteria shortest path (MSP)} \label{sec:shortest_path:defs:msp}
% MSP
Multi-criteria shortest path problems \cite{Martins84} generalize shortest path problems w.r.t.~the weight function $c$, which is extended to $d$-dimensional vectors, i.e., $\vect{c}: A \rightarrow \mathbb{N}^d$ and
\begin{align*}
\vect{w}: P &\rightarrow \mathbb{N}^d \\
p &\mapsto \vect{w}(p) = (w_1(p), w_2(p), \dots , w_d(p)) \quad , \neqn{msp_weighted}
\end{align*}
where $w_j(p) = \sum_{i=1}^{k}c_j(a_i)$ $\forall j \in \{1,\dots,d\}$.

% Pareto optima
A path $p$ from node $v_1$ to node $v_2$ is a \emph{Pareto-optimal} or \emph{non-dominated} path if there is no path $q \in P$ from $v_1$ to $v_2$ s.t.~$q \dom p$.
Informally, we also call a Pareto-optimal path $p$ \emph{shortest path}.
Thus, for the MSP, the solution consists of a set of Pareto-optimal paths for each considered pair of nodes.


\subsubsection*{Applications} \label{sec:shortest_path:applications} 
% Applications
Apart from the problem of finding an optimal route in a road map as outlined above, the MSP problem has numerous outer applications, such as routing in multimedia networks \cite{ClimacoCP03}, route guidance \cite{JahnMS00} and curve approximation \cite{MehlhornZ00}.

\subsubsection*{Hardness and practicality} \label{sec:shortest_path:hardness}
% Hardness of the problem, reason for parallelization
% Note that the graph instances resulting from modeling practical problems are potentially very large. 
The crucial parameter for the complexity of the MSP is the total number of Pareto optima for all visited nodes. 
Since this number is exponential in $n$ in the worst case \cite{Hansen80}, the MSP is in general NP-hard.
Even for $d=2$, the decision problem whether there exists a path between two nodes whose length is below a given threshold is NP-hard \cite{GareyJ79}.
However, it was observed that the problem is efficiently tractable from a practical viewpoint for many practically relevant instances. 
The input data of practical applications tends to have certain characteristics, which lead to the set of Pareto optima for a vertex to be polynomially bounded \cite{Muller-HannemannW06}. 
It was shown that in applied scenarios this number may even be bounded by a small constant \cite{Muller-HannemannW01}. 
Thus, MSP can be solved efficiently for small $d$ and not too large graphs adhering certain characteristics.

% reason for parallelization
However, due to the potentially very large graph instances resulting from modeling e.g., road or railway networks, the computational cost is significant even if a small number of criteria is to be optimized.
Guerriero and Musmanno \cite{GuerrieroM01} thus suggest that parallel computing might help to design efficient solution methods. 

\subsubsection*{Approximate solutions} \label{sec:shortest_path:approx}
% heuristics
Due to the hardness of the MSP, heuristic methods are sometimes employed (see, e.g., \cite{BastMS03, Sonnier06, EhrgottG02}).
% weighted sum approach
A common approach is to define a total order relation $\tor$ on $P$ that allows for a more efficient computation of the Pareto-optima.
Such a relation $\tor$ has to satisfy the following properties\footnote{We denote the concatenation of two paths $p_1 \in P_{u,v}$, $p_2 \in P_{v,w}$ as $\cP{p_1}{p_2}$}
\begin{align}
\forall p,q \in P_{u,v}: p \dom q \implies p \tor q  \quad \text{Dominance} \\
\forall p \in P_{u,v}, \forall (v,w) \in A: p \tor \cP{p}{w} \quad \text{Monotonic}
\end{align}
Martins et al.~\cite{MartinsPRS07} show that the following relation satisfies these requirements:
\begin{align}
p \osum q \iff \sum_{i=1}^{d} w_i(p) \leq  \sum_{i=1}^{d} w_i(q) 
\end{align}
% supported/non-supported solutions
However, as Sonnier \cite{Sonnier06} points out, a problem with algorithms based on such a relation is that 
\begin{align}
p \osum q  \notimplies p \dom q \quad , \neqn{osum_problem}
\end{align}
i.e., only solutions that lie on the convex hull of the feasible region are found, which are called the supported solutions. 
In other words, there exist Pareto-optima which may not be found because of the property given in Eq.~\ref{eq:osum_problem}.

% graph example from Sonnier
To see this, consider the following example from Sonnier \cite{Sonnier06}.
Assume we are interested in the Pareto-optimal paths from node 1 to node 5 in the graph depicted in Fig.~\ref{fig:example_graph}.

\begin{figure}
\begin {center}
\begin {tikzpicture}[-latex, auto, node distance = 3 cm, on grid, semithick, state/.style = {circle, top color = white, draw, minimum width = 1 cm}]
\node[state] (n1) {$1$};
\node[state] (n2) [below right of = n1] {$2$};
\node[state] (n3) [above right of = n2] {$3$};
\node[state] (n4) [below right of = n3] {$4$};
\node[state] (n5) [above right of = n4] {$5$};
\path (n1) edge node[above = 0.05 cm] {$(1,4)$} (n3);
\path (n3) edge node[above = 0.05 cm] {$(4,1)$} (n5);
\path (n1) edge [bend right = 30] node[below = 0.2 cm] {$(1,1)$} (n2);
\path (n2) edge [bend right = 30] node[below = 0.2 cm] {$(1,2)$} (n3);
\path (n3) edge [bend right = 30] node[below = 0.2 cm] {$(2,1)$} (n4);
\path (n4) edge [bend right = 30] node[below = 0.2 cm] {$(1,1)$} (n5);
%\path (n1) edge [bend left = 30, color=red] node[above = 0.2 cm, color=red] {$(3.8,6.8)$} (n5);
\end{tikzpicture}
\end{center}
\caption{A weighted ($d=2$) directed graph.}
\label{fig:example_graph}
\end{figure}

The solution set is shown in Table \ref{table:pareto_paths}; note that all the paths are supported solutions, since we have 
\begin{align}
\forall j,k \in \{1,\dots,4\}: \sum_{i=1}^{d} w_i(p_j) =  \sum_{i=1}^{d} w_i(p_k) \quad .
\end{align}

\begin{table*}
\begin{center}
    \begin{tabular}{| l | c |}
    \hline
    \textbf{Path} & \textbf{Weight vector} \\ \hline \hline
    $p_1 = \langle 1,2,3,4,5 \rangle$ & (5,5) \\ \hline
    $p_2 = \langle 1,3,4,5\rangle$  & (4,6) \\ \hline
    $p_3 = \langle 1,2,3,5\rangle$  & (3,7) \\ \hline
    $p_4 = \langle 1,3,5\rangle$ & (2,8) \\ \hline
    %$p_5$: 1-5 & (3.8,6.8) \\ \hline
    \end{tabular}
\end{center}
\caption{Pareto-optimal paths from node 1 to node 2 in the graph of Fig.~\ref{fig:example_graph}}
\label{table:pareto_paths}
\end{table*}

We extend the example by adding an arc $(1,5)$ with weight vector $(3.8,6.8)$.
Note that now, in addition to the previous solutions, the path $p_5 = \langle 1,5 \rangle $ is a Pareto-optimal path.
However, we have 
\begin{align}
\forall j \in \{1,\dots,4\}: \sum_{i=1}^{d} w_i(p_j) < \sum_{i=1}^{d} w_i(p_5) \quad ,
\end{align}
i.e., $p_5$ is not on the convex hull an thus called an \emph{unsupported non-dominated solution}, as depicted in Fig. \ref{fig:unsupported_solution}.
% approximate solution set
Thus, an \emph{approximate solution set} provides a ``reasonable'' set of non-dominated paths, but is not necessarily complete.

\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[xlabel=$w_1$, ylabel=$w_2$, nodes near coords, enlargelimits=0.2]
	\addplot[color=blue, mark=*, point meta=explicit symbolic] 
	coordinates {
		(5,5) [$p_1$]
		(4,6) [$p_2$]
		(3,7) [$p_3$]
		(2,8) [$p_4$]
	};
	\addplot[color=red, mark=*, point meta=explicit symbolic] 
	coordinates {
		(3.8,6.8) [$p_5$]
	};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{An unsupported non-dominated solution: $p_5$}
\label{fig:unsupported_solution}
\end{figure}

\begin{comment}
\subsubsection*{Mutligraphs} \label{sec:shortest_path:defs:multigraph}
A multigraph is a graph that may contain parallel edges, that is, multiple edges may connect the same two nodes. 
Formally, a directed multigraph $G=(V,A)$ consists of a finite set of nodes $V = \{v_1, v_2,\dots,v_n \}$ and a finite multiset\footnote{In contrast to a set, a multiset may contain the same element multiple times.} of arcs and a function $f$, 
\begin{align}
f: A &\rightarrow (u,v) \text{ where } u,v \in V \quad .
\end{align}

The definition of weighted multigraphs is analogous to the definition of weighted graphs given in Eq.~\ref{eq:msp_weighted}. 

% our algorithm works on multigraphs too!
\todo{This should go somewhere else - it certainly does not belong to the formal definitions}
We note that in contrast to the algorithms mentioned in Section \ref{sec:related}, our algorithm works on multigraphs as well and provide a comparative performance evaluation in Section.
\end{comment}


\section{A Pheet-based MSP algorithm} \label{sec:msp_algo}
% what we do in this section
In this section, we present our algorithm for the MSP.
% Problem setting
We want to compute an exact solution of the MSP for directed graphs with multi-dimensional weight vectors (as defined in Section \ref{sec:shortest_path:defs:msp}). 
% Intro to the algorithm: generalization of Dijkstra
The algorithm follows the principles of Dijkstra's algorithm: we generate a set of candidates and expand the most promising first. The problem of finding intermediate paths that are Pareto-optimal is delegated to a priority queue. 
% we have a set of solutions for each node
Note that due to the multi-dimensional weight vectors, we have a set of optima for each node (see Section \ref{sec:pareto_optima}).
% The algorithm is label-correcting (no strict sequence)
In contrast to Dijkstra's algorithm, ours is a \emph{label correcting} algorithm.
The weight vectors are only partially ordered and thus there can be multiple Pareto-optimal candidates that may be expanded in arbitrary order. 
However, even though non of these candidates dominates any of the others, the partial solutions created by expanding them might do so.

% Pareto set
Each node $v$ maintains a \emph{Pareto set} to store the Pareto-optimal paths to the node $v$ that have been found so far. 
We denote the set of all Pareto sets by $S_p$ and a specific Pareto set attached to a node $v$ by $S_p[v]$.

% Pseudo code
A high level description of the algorithm is shown in Algorithm \ref{algo:abstractMSP}; note its similarity to Dijkstra's algorithm.
% weight vector <-> priority vector
As with Dijkstra's algorithm, paths with minimal weight should be expanded first, i.e., paths with minimal weight have the highest priority. 
The procedure for expanding a candidate path is described in detail in Algorithm \ref{algo:abstractExpand}. 

% Abstract description of the algorithm
\begin{algorithm}
\caption{Pseudo-code for our MSP algorithm}
\label{algo:abstractMSP}
\begin{algorithmic}[1]
\Require{Graph $G$, start node $s$}
\ForAll{$v \in G$}
  \State $S_p[v] \gets \{\}$ \Comment{Initialize the Pareto-set for each node}
\EndFor
\State $S_p[s] \gets \{ \vect{0} \} $ \Comment{Shortest path from $s$ to $s$ is the null-vector}
\State $U \gets \{\langle s \rangle\}$ \Comment{$U$ is a priority queue containing paths that need to be expanded further}
\While{$U$ is not empty}
  \State Take a highest-priority path $p$ from $U$ 
  \State \Call{expand}{$p$}
\EndWhile
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Pseudo-code for expanding a candidate path}
\label{algo:abstractExpand}
\begin{algorithmic}[1]
\Require{$S_p[v]$: Set of Pareto-optimal paths to node $v$ ($\forall v\in V$)}
\Require{$U$: Set of paths that need to be explored further}
\Require{$p=\langle s, \dots, v \rangle$ is a path in Graph $G$ from start node $s$ to $v$}
\Function{expand}{Path $p$}
\ForAll{$w \in p$.head.neighbors} 
  \State $p' \gets \cP{p}{w}$	\Comment{Generate candidate}
  \If{$\nexists x \in S_p[w] \text{ s.t.\ } x \dom p'$} 
  \Statex \Comment{Candidate is not dominated by any path in the Pareto-set}
    \ForAll{$y\in S_p[w]$} \Comment{Remove any path dominated by $p'$}
      \If{$p' \dom y$}
	\State $S_p[w] \gets S_p[w] \setminus y$
      \EndIf
    \EndFor
    \State $S_p[w] \gets S_p[w] \cup p'$ \Comment{Add $p'$ to the Pareto-set for node $w$}
    \State $U \gets U \cup p'$ \Comment{$p'$ needs to be explored further}
  \EndIf
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Implementation} \label{sec:msp_algo:implementation}
% Connection to Pheet
The implementation relies on the Pheet task scheduling framework for parallelization and, in particular, on Pheet's concept of \emph{priority task scheduling} (Section \ref{sec:pheet:task_priorites}), which allows to combine the work pool of the scheduler with the priority queue used by the algorithm into one data structure: a priority-aware task scheduler.
% Each path to expand is a task
The algorithm can be parallelized in an intuitive way by wrapping each candidate path into a task. 
Note that expanding several paths can be done independently, except for updates to the Pareto-set of a given node.

% Works with different schedulers/task storages
\begin{code}[label=lst:ParetoSet, caption=Pareto set operations., mathescape]
class ParetoSet {
  /* Insert $\color{OliveGreen} p = \langle s,\dots, v\rangle$ into the Pareto set at node $\color{OliveGreen} v$. Any path $\color{OliveGreen} p'$ in that set dominated by $\color{OliveGreen} p$ is marked dominated and removed from the set. If $\color{OliveGreen} p$ is dominated by a path $\color{OliveGreen} p'$ already in the set, $\color{OliveGreen} p$ is marked dominated and is not inserted into the set. Returns true if $\color{OliveGreen} p$ was inserted into the Pareto set. */ 
  bool insert(Path p);
}
\end{code}

We designed the implementation s.t.~it may easily employ different task scheduler implementations without any changes to the algorithm itself. 
Within this framework, we only have to provide implementations of 
\begin{inparaenum}[(i)]
\item the task and 
\item a scheduling strategy.
\end{inparaenum}
% Pareto set
For the implementation, we extend the Pareto-set (Lst.\ \ref{lst:ParetoSet}) by the \verb|insert| operation to provide some of the functionality required in the procedure for expanding a path (Algorithm \ref{algo:abstractExpand}).
In particular, \verb|insert|, given a path $p = \langle s,\dots,v \rangle$, removes all paths at node $v$ which are dominated by $p$; if however, $p$ is dominated by some path $p'$ in the Pareto-set for node $v$, $p$ is not added to the Pareto-set but is instead marked as dominated itself. 
The operation returns true if and only if $p$ was added to the Pareto-set.
Note that for any path $p$, only one of the following two alternatives may occur:
\begin{itemize}
\item $p$ is added to the Pareto-set and some paths $p'$ that are dominated by $p$ are marked dominated and removed from the set.
\item $p$ itself is dominated by some path $p'$ from the Pareto-set and is thus marked dominated and not added to the set.
\end{itemize} 
In other words, it cannot be the case that $p$ is marked dominated, but another path $p'$ already in the Pareto-set was removed from it because it is dominated by $p$. 
To see this, assume that the Pareto-set contains two paths $p_1$ and $p_2$, where $p_1$ is dominated by $p$ and $p_2$ dominates $p$. 
The \verb|insert| operation would first remove $p_1$ since $p \less p_1$ and then mark $p$ dominated since $p_2 \less p$. 
However, due to transitivity of the dominance relation, this implies $p_2 \less p_1$. 

% Pareto set implementation
A naive implementation only needs to scan the Pareto-set either until it finds a path $p'$ that dominates $p$ or remove all paths dominated by $p$.

\begin{code}[caption=MSP Task, label=lst:msptask, mathescape]
class MSPTask : SchedulerTask {
public:

  MSPTask(Path p, ParetoSets Sp) //Constructor. Save parameters.
    : p(p), Sp(Sp) { }
  
  void execute() {
    /* The path this task was spawned for might be obsolete. */
    if(p.dominated) return;  
    /* Generate new candidates and add them to the global Pareto set */
    for (Arc a : p.head().outgoing_edges()) {
      Path candidate = p.step(e);
      if(Sp.insert(candidate)) {
        /* Spawn a new task for each candidate path that was added to the Pareto set */
        spawn_task(p, Sp, strategy);
      }
    }
  }
private:
  Path p;
  ParetoSets Sp;
}
\end{code}

An implementation of the Pheet-based MSP algorithm is given in Lst.\ \ref{lst:msptask}. 
The \verb|spawn_task| function (line 15) is generic and allows tasks to be spawned with different strategies.
The scheduler uses the strategy -- that is to be defined by the application -- to determine the order of tasks.
Furthermore, tasks may be marked dead via their associated strategy in case they were spawned speculatively and need not be executed anymore.
We provide two strategies that are based the generic \verb|Strategy| class (Lst.\ \ref{lst:strategy})
\begin{itemize}
\item The \verb|LinearCombinationStrategy| (Lst.\ \ref{lst:LinearCombinationStrategy} assumes that a total order relation can be defined on the set of priority vectors. As Section \ref{sec:shortest_path:approx} shows, the $\leq_{sum}$ relation based on the $LÂ¹$ norm constitutes such a relation for integer vectors (which, however, is not applicable in general).
\item The \verb|ParetoStrategy| (Lst.\ \ref{lst:ParetoStrategy}) allows for general (i.e., non-integer) priority vectors and does not rely on a total order relation.
Given two priority vectors, it offers function to check whether one dominates the other as well to compare them w.r.t.\ a given dimension.
\end{itemize}


\begin{code}[label=lst:strategy, caption=Interface of the Strategy class, mathescape]
class ParetoStrategy {

  void initialize(PriorityVector pv);
    
  /* Return true if the priority vector of this strategy has higher or equal priority than other. */
  bool prioritize(PriorityVector other);
    
  /* Return true if the associated task is marked as dead; i.e., if it need not be executed anymore. */
  bool dead_task();
}
\end{code}

\begin{code}[label=lst:LinearCombinationStrategy, caption=LinearCombinationStrategy, mathescape]
class ParetoStrategy {
    
  /* Return true if the priority vector of this strategy has higher or equal priority than other w.r.t. to the total order relation $\leq_{sum}$, i.e., the $L^1$ norm. */
  bool prioritize(PriorityVector other) {
    return sum(pv) <= sum(other);
  }
}
\end{code}


\begin{code}[label=lst:ParetoStrategy, caption=Interface of ParetoStrategy, mathescape]
class ParetoStrategy {
  /* Return true if the priority vector of this strategy has higher or equal priority than other w.r.t. the dominance relation, i.e., if other does not dominate the priority vector of this strategy. */
  bool prioritize(PriorityVector other);
  
  /* Return true if the priority vector of this strategy at dimension d has less priority than v. */
  bool less_priority(Dimension d, Value v);
  
 /* Return true if the priority vector of this strategy at dimension d has higher priority than v. */
  bool higher_priority(Dimension d, Value v);
}
\end{code}

When a new task is spawned with the \verb|ParetoStrategy|, the \verb|ParetoTaskStorage|, an implementation of our MCPQ (Section \ref{sec:mcpq}), is used by the scheduler. 
Pheet's \verb|LSMLocalityTaskStorage| (see \cite{Wimmer14}) is used for tasks spawned with the \verb|LinearCombinationStrategy| strategy.
We note here that Pheet provides several more priority queue implementations (or task storages in Pheet terminology) that the scheduler can employ. 
However, the \verb|LSMLocalityTaskStorage| priority queue is the most suitable for a comparison with our MCPQ implementation since it too observes purely local ordering guarantees and is also based on log-structured merge-lists.

\section{Performance evaluation} \label{sec:evaluation}
We compare the performance of our MCPQ to Pheet'S LSM priority queue via the MSP algorithm.
% Mars hardware
All experiments were run on a shared memory system nicknamed \emph{Mars}, an Intel Xeon based system with the properties listed in Table \ref{table:mars}.
\begin{table*}
\begin{center}
    \begin{tabular}{| l | l |}
    \hline
    CPU model & Intel Xeon E7-8850 \\ \hline
    Number of cores & 80 (8 nodes with 10 cores each) \\ \hline
    CPU clock & 2.00 GHz \\ \hline
    L1i & 32 KB \\ \hline
    L1d & 32 KB \\ \hline
    L2 & 256 KB \\ \hline
    L3 & 24576 KB \\ \hline
    Main memory & 1 TB \\ \hline    
    \end{tabular}
    \caption{Hardware configuration of Mars}
    \label{table:mars}
\end{center}
\end{table*}
% compiler 
Pheet was compiled using \verb|gcc 4.9.1| with the the \verb|-O3| flag to allow standard compiler optimizations.
All experiments were repeated 20 times and the graphs show the mean values \todo{and confidence intervals?}.

\subsection{Test instances} \label{sec:eval:test_instances}
% test graphs
Input instances were created by our own graph generator \footnote{\verb|PHEET_HOME/test/msp/lib/Graph/Generator/main.h|}, which generates a random connected digraph with the following arguments:
\begin{compactitem}
\item \verb|n|: Number of nodes.
\item \verb|m|: Number of arcs.
\item \verb|d|: Degree of the weight vectors.
\item \verb|w|: Upper limit for all dimensions of the weight vectors.
\item \verb|r|: Random seed value.
\end{compactitem}
If the option \verb|-p| is given, a multigraph will be generated. 
Otherwise, the generated graph will contain no parallel edges.

The generator first generates a random tree to ensure that the graph is connected and then inserts additional arcs. 
These are selected randomly from the set of $n(n-1)$ arcs of a complete graph minus the $(n-1)$ arcs chosen for the tree. 

\setkeys{Gin}{width=1\textwidth}
\subsection{Results} \label{sec:results}
\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE, width=9, height=4>>=
plotOverCpus(
"../benchmarks/mars/g_5000_30000_3_10000_42/s2pareto.dat",
"../benchmarks/mars/g_5000_30000_3_10000_42/s2lsm.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{g\_5000\_30000\_3\_10000\_42}
\end{figure}


\begin{figure}[H]
\begin{center}
<<fig=TRUE, echo=FALSE, width=9, height=4>>=
plotOverCpus(
"../benchmarks/mars/g_1000_3000_5_10000_42/s2pareto.dat",
"../benchmarks/mars/g_1000_3000_5_10000_42/s2lsm.dat",
yAxis="total_time", from=1, to=80)
@
\end{center}
\caption{g\_1000\_3000\_5\_10000\_42}
\end{figure}


\section{Future work} \label{sec:future}

\section{Conclusion} \label{sec:conclusion}
\todo{...}

\section{Acknowledgements} \label{sec:ack}
I sincerely want to thank my supervisor Martin Wimmer for providing this interesting and challenging topic as well as guidance and support throughout the duration of the project, as well as my co-supervisor Jesper Larsson Tr\"aff for valuable feedback.
Some work, which is mostly covered by Section \ref{sec:msp_algo}, was done jointly with Jakob Gruber and I highly appreciate his contributions.
Furthermore, I am especially grateful to all members of the ``Parallel Computing'' research group at the Vienna University of Technology for providing such a motivating and yet comfortable working environment.

%bibliography
\bibliographystyle{acm}
\bibliography{sources} 

\end{document}
